\chapter{Introduction}
\label{cha:introduction}

This chapter gives an overview over the scope of the thesis and existing approaches in the
literature.  A preliminary version of this work has already been presented in
\cite{gabler2019graph}, which forms the basis of the introduction.

\newthought{Many methods} in the field of machine learning work on computation graphs of programs
that represent mathematical expressions.  One example are forms automatic differentiation (AD),
which derive an \enquote{adjoint} expression from a expression that usually represents a loss
function, to calculate its gradient.  Another one are message passing algorithms, which use the
graph as the basic data structure for the operation they perform: passing values between nodes,
representing random variables that depend on each other (in fact, message passing generalizes
various other methods, including AD).  But also in more or less unrelated fields, such as program
analysis or program transformation (cf. Numba\todo{reference}), the same requirements might occur
through the need to derive abstract graphs of program flow from a given program.\todo{abstract
  interpretation}

There are several options how to provide the computation graph in question to an application, many
of which are already established in the AD community (see \cite{baydin2018automatic} for a survey on
AD methods).  For one, graphs can be required to be written out explicitely by the user, by defining
a custom input format\todo{example} or a library to build graphs \enquote{by hand} through an
API\todo{operator overloading: Torch, TF, ...}.  Such APIs are called \emph{operator overloading} in
AD language, because they extend existing operations to additionally track the computation graph at
runtime on so-called tapes or Wengert lists \cite{bartholomew-biggs2000automatic}.  This kind of
tracking is dynamic, in the sense that a new tape is recorded for every execution.  However, being
implemented on a library level, it usually requires the programmer to use non-native constructs
instead of language primitives, leading to cognitive overhead. Further, there are additional runtime
costs due to the separate interpretation of derivatives stored on the tape.

Alternatively, an implementation can allow the user to write out computations as a \enquote{normal}
program in an existing programming language (or possibly a restricted subsete of it), and use
metaprogramming techniques to extract graphs from the input program.  Such metaprograms, known under
the name \emph{source transformations}, can in turn operate on plain source code (cf. Tapenade
\cite{tapenadedevelopers2019tapenade}), or on another, more abstracted notion used by the
programming language infrastructure, like the abstract syntax tree (AST), or an intermediate
representation (IR).  They operate on the syntactic structure of the whole program, during or before
compilation.  Unlike in operator overloading, it is hence possible to inspect and exploit control
structures directly. This can lead to more efficient results, compared to operator overloading,
since the transformation is done only once per program and eligible for compiler optimisations.  But
in this approach, no records of the actual execution paths are constructed explicitly~-- purely
static information is used only at compile time, and cannot be accessed for further analysis or
transformation during execution.

\newthought{In a variety} of domains, though, the execution path of programs can drastically change
at each run.  Examples of this from machine learning are models with non-uniform data, such as parse
trees (Socher et al., 2011) or molecular graphs (Bianucci et al., 2000), Bayesian nonparametric
models (Hjort et al., 2010), or simply the occurrence of stochastic control flow in any
probabilistic model.  Such models are called dynamic models.  The lack of an explicit graph
structure makes it impossible, or at least diffult, to apply source transformation approaches on
them.  Operator overloading is the more direct way for supporting them, since it automatically
records a new tape for each input. In fact, many state-of-the-art machine learning libraries support
dynamic graphs based on operator overloading, e.g. DyNet (Neubig et al., 2017), Chainer (Tokui et
al., 2015), and PyTorch (Paszke et al., 2017). TensorFlow Fold (Looks et al., 2017) follows a more
elaborate approach and provides \enquote{dynamic batching} operators to define static graphs
emulating dynamic operations.

However, relying on operator overloading makes it impossible to take advantage of the benefits of
source transformations, such as utilizing information about the control flow, optimizations at
compile time, or exploiting the model structure for efficient posterior inference, e.g. through the
detection of conjugacy relationships in models (Hoffman et al., 2018).

The source transformation approach based on intermediate representations has recently gained
popularity in machine learning\todo{XLA, Zygote}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\label{sec:related-work}

% swift4tf, xla/jags, ad stuffs + zygote, concolic execution & abstract interpretation


%%% Local Variables: 
%%% TeX-master: "main"
%%% End: