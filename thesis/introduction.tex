\chapter{Introduction}
\label{cha:introduction}

This chapter gives an overview over the scope of the thesis and existing approaches in the
literature.  A preliminary version of this work has already been presented in
\textcite{gabler2019graph}, which forms the basis of the introduction.

\newthought{Many methods} in the field of machine learning work on computation graphs of programs
that represent mathematical expressions.  One example are forms of automatic differentiation (AD)
which derive an \enquote{adjoint} expression from a expression that usually represents a loss
function, to calculate its gradient \parencite{griewank2008evaluating, gebremedhin2020introduction}.
Another one are message passing algorithms \parencite{minka2005divergence}, which use the graph as
the basic data structure for the operation they perform: passing values between nodes, representing
random variables that depend on each other (in fact, message passing generalizes various other
methods, including AD).  But also in more or less unrelated fields, such as program analysis or
program transformation (cf. XXX\todo{examples}), the same requirements might occur through the need
to derive abstract graphs of program flow from a given program.\todo{abstract interpretation}

There are several options how to provide the computation graph in question to an application, many
of which are already established in the AD community (see \textcite{baydin2018automatic} for a
survey on AD methods).  For one, graphs can be required to be written out explicitely by the user,
by defining a custom input format\todo{example} or a library to build graphs \enquote{by hand}
through an API (e.g., PyTorch \parencite{paszke2017automatic} or TensorFlow
\parencite{abadi2015tensorflow}).  Such APIs are called \emph{operator overloading} in AD language,
because they extend existing operations to additionally track the computation graph at runtime on
so-called tapes or Wengert lists \parencite{bartholomew-biggs2000automatic}.  This kind of tracking
is dynamic, in the sense that a new tape is recorded for every execution.  However, being
implemented on a library level, it usually requires the programmer to use non-native constructs
instead of language primitives, leading to cognitive overhead. Furthermore, there are additional
runtime costs due to the separate interpretation of derivatives stored on the tape.

Alternatively, an implementation can allow the user to write out computations as a \enquote{normal}
program in an existing programming language (or possibly a restricted subset of it), and use
metaprogramming techniques to extract graphs from the input program.  Such metaprograms, known under
the name \emph{source transformations}, can in turn operate on plain source code (cf. Tapenade
\parencite{tapenadedevelopers2019tapenade}), or on another, more abstracted notion used by the
programming language infrastructure, like the abstract syntax tree (AST), or an intermediate
representation (IR).  They operate on the syntactic structure of the whole program, during or before
compilation.  Unlike in operator overloading, it is hence possible to inspect and exploit control
structures directly. This can lead to more efficient results, compared to operator overloading,
since the transformation is done only once per program and eligible for compiler optimisations.
Additionally, the user is not restricted to the domain specific language provided by a libray, and
can use regular language constructs, data structures, and custom functions rather freely.  But in
this approach, no records of the actual execution paths are constructed explicitly~-- purely static
information is used only at compile time, and cannot be accessed for further analysis or
transformation during execution.

\newthought{In a variety} of domains, though, the execution path of programs can drastically change
at each run.  Examples of this from machine learning are models with non-uniform data, such as parse
trees \parencite{socher2011parsing} or molecular graphs \parencite{bianucci2000application},
Bayesian nonparametric models \parencite{hjort2010bayesian}, or simply the occurrence of stochastic
control flow in any probabilistic model.  Such programs we call dynamic models.  The lack of an
explicit, unique graph structure makes it impossible, or at least diffult, to apply source
transformation approaches on them.  Operator overloading is the more direct way for supporting
dynamic models, since it automatically records a new tape for each input. In fact, many
state-of-the-art machine learning libraries are based on dynamic graphs using operator overloading
in some form.

However, relying on operator overloading makes it impossible to take advantage of the benefits of
source transformations, such as utilizing information about the control flow, integrating with
optimizations at compile time, or exploiting the source model structure.  The source transformation
approach based on intermediate representations has recently gained popularity in machine learning.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\label{sec:related-work}

% swift4tf, xla/jags, ad stuffs + zygote, concolic execution & abstract interpretation
% e.g. DyNet (Neubig et al., 2017), Chainer (Tokui et
% al., 2015), and PyTorch (Paszke et al., 2017). TensorFlow Fold (Looks et al., 2017) follows a more
% elaborate approach and provides \enquote{dynamic batching} operators to define static graphs
% emulating dynamic operations.

%%% Local Variables: 
%%% TeX-master: "main"
%%% End: