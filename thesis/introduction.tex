\chapter{Introduction}
\label{cha:introduction}

This chapter gives an overview over the scope of the thesis and existing approaches in the
literature.  A preliminary version of this work has already been presented in
\textcite{gabler2019graph}, which forms the basis of the introduction.

\newthought{Many methods} in the field of machine learning work on computation graphs of programs
that represent mathematical expressions.  One example are forms of automatic differentiation (AD)
which derive an \enquote{adjoint} expression from a expression that usually represents a loss
function, to calculate its gradient \parencite{griewank2008evaluating, gebremedhin2020introduction}.
Another one are message passing algorithms \parencite{minka2005divergence}, which use the graph as
the basic data structure for the operation they perform: passing values between nodes, representing
random variables that depend on each other (in fact, message passing generalizes various other
methods, including AD).  But also in more or less unrelated fields, such as program analysis or
program transformation \parencite[cf.][]{muchnick1997advanced,singer2018static,aho1986compilers},
the same requirements might occur through the need to derive abstract graphs of program flow from a
given program for the purpose of abstract interpretation.

There are several options how to provide the computation graph in question to an application, many
of which are already established in the AD community (see \textcite{baydin2018automatic} for a
survey on AD methods).  For one, graphs can be required to be written out explicitely by the user,
by defining providing a library to build graphs \enquote{by hand}
(e.g. \textcite{chewxy2020gorgonia,jia2014caffe}~-- these interfaces tend to be more low level) or
through a higher-level API (e.g. PyTorch \parencite{paszke2017automatic} or TensorFlow
\parencite{abadi2015tensorflow}).  Such APIs are called \emph{operator overloading} in AD language,
because they extend existing operations to additionally track the computation graph at runtime on
so-called tapes or Wengert lists \parencite{bartholomew-biggs2000automatic}.  This kind of tracking
is dynamic, in the sense that a new tape is recorded for every execution.  However, being
implemented on a library level, it usually requires the programmer to use non-native constructs
instead of language primitives, leading to cognitive overhead.  This notably happens for control
statements, which can rarely be \enquote{overloaded}.  Furthermore, there are additional runtime
costs due to the separate interpretation of derivatives stored on the tape.

Alternatively, an implementation can allow the user to write out computations as a \enquote{normal}
program in an existing programming language (or possibly a restricted subset of it), and use
metaprogramming techniques to extract graphs from the input program.  Such metaprograms, known under
the name \emph{source transformations}, can in turn operate on plain source code (cf. Tapenade
\parencite{tapenadedevelopers2019tapenade}), or on another, more abstracted notion used by the
programming language infrastructure, like the abstract syntax tree (AST), or an intermediate
representation (IR).  They operate on the syntactic structure of the whole program, during or before
compilation.  Unlike in operator overloading, it is hence possible to inspect and exploit control
structures directly. This can lead to more efficient results, compared to operator overloading,
since the transformation is done only once per program and eligible for compiler optimisations.
Additionally, the user is not restricted to the domain specific language provided by a libray, and
can use regular language constructs, data structures, and custom functions rather freely.  But in
this approach, usually, no records of the actual execution paths are constructed explicitly~--
purely static information is used only at compile time, and cannot be accessed for further analysis
or transformation during execution.

\newthought{In a variety} of domains, though, the execution path of programs can drastically change
at each run.  Examples of this from machine learning are models with non-uniform data, such as parse
trees \parencite{socher2011parsing} or molecular graphs \parencite{bianucci2000application},
Bayesian nonparametric models \parencite{hjort2010bayesian}, or simply the occurrence of stochastic
control flow in any probabilistic model.  Such programs we call dynamic models.  The lack of an
explicit, unique graph structure makes it impossible, or at least diffult, to apply source
transformation approaches on them.  Operator overloading is the more direct way for supporting
dynamic models, since it automatically records a new tape for each input. In fact, many of the
already mentioned state-of-the-art machine learning libraries are based on dynamic graphs using
operator overloading in some form.

However, relying on operator overloading makes it impossible to take advantage of the benefits of
source transformations, such as utilizing information about the control flow, integrating with
optimizations at compile time, or exploiting the source model structure.  The source transformation
approach based on intermediate compiler representations has recently gained popularity in machine
learning; see \textcite{bradbury2018jax,lattner2020mlir}.  While the main focus of these efforts has
been optimization of linear algebra/tensor calculations and automatic differentiation, other use
cases start to emerge, for example automatic detection of sparsity patterns
\parencite{gowda2019sparsity}.

In this thesis, I present a novel variant of automatic extraction of computation graphs suitable for
static and dynamic models, using source transformation instead of operator overloading.  Inspired by
recent work on differential programming \parencite{innes2018don}, the approach transforms the
intermediate representation used by the compiler of the Julia programming language.  This system can
be used to dynamically track computation graphs of any Julia program, including machine learning
models and probabilistic programming systems, without having to explicitely declare graph
structures. The transformation is implemented as a custom part of the compilation process.  Its
result is passed on to the compiler, where it can be optimised further. At run time, both data and
control path are tracked alongside the original calculations, in the form of a nested data
structure.  This data structure contains all functions called during execution, enriched by recorded
control flow decisions and meta information that can be used to analyse the execution. Thus, the
system combines advantages of a source transformation with a tape-based runtime approach.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\label{sec:related-work}

The topic of this thesis crosses several disciplines~-- at least automatic differentiation, compiler
and programming language theory, and probabilitstic programming.  Since these have not always worked
together, similar principles may be have been found or (re-)introduced in each of them.

Automatic differentiation has a long history, in which different styles becoming more or less
fashionable depending on the dominating use-case and available languages and infrastructure.
Traditionally, numerical code in Fortran or C was differentiated by whole-source transfomation
systems like Tapenade \parencite{tapenadedevelopers2019tapenade}.  In recent years, after phase of
many library-based (or \enquote{operator overloading}) systems that were driven by the rise of deep
learning \parencite{abadi2015tensorflow,paszke2017automatic,neubig2017dynet,tokui2015chainer},
compiler-based approaches have regained popularity lately.  There are ongoing efforts to add
built-in automatic differentiation to the Swift programming language in Swift for TensorFlow
\parencite{swift2019}, and work in Julia for Zygote \parencite{innes2018don} has started to apply
source transformation to the intermediate representation of the compiler, which enables
differentiating through complex control flow, custom data types, and nested functions.  A similar
approach to Zygote is taken in Python with Tangent \parencite{vanmerrienboer2018tangent}.

Generalizations of the kinds of analyses and transformations found in these systems can be found
under multiple terms in the compiler literature: data- and control-flow analysis, information
propagation, or abstract interpretation \parencite{muchnick1997advanced,singer2018static}.  These
methods, most of which find fixed points in relations defined over a program, can in turn be seen as
a form of message passing, under which not only a variety of learning algorithms can be summarized
\parencite{minka2005divergence}, but also automatic differentiation \parencite{minka2019automatic}
and gradient based optimization \parencite{dauwels2005steepest}.  Other forms of abstract analysis
exist for program optimization, e.g., sparsity detection \parencite{gowda2019sparsity} or the
detection of program parts that need not to be reevaluation after input changes
\parencite{becker2020dynamic}.

Many implementations of these methods do not use the original form of the program, but a
syntactically simplified lowered form.  Such forms can be dependency graphs as used in compiler
theory, or the intermediate languages used by actual compiler implementations.  Recently, special
intermediate language for machine learning applications have been introduced.  One of them is MLIR
(\enquote{machine learning intermediate language}, \textcite{lattner2020mlir}) with the purpose of
forming a reusable mid-layer between programming languages and runtimes, featuring exchangeability
between different machine learning frameworks and and \enquote{accelerators} (pieces of hardware),
while taking advantage of modern compiler technology similar to LLVM
\parencite{llvmproject2019llvm}.  A similar role is played by Swift's intermediate representation in
the Swift for TensorFlow project.  XLA (\enquote{accelerated linear algebra}, \textcite{xla2020})
plays a similar role for expression-graph based machine learning systems, compiling those graphs
directly to optimized code; this system forms the basis of JAX \parencite{bradbury2018jax}, which
allows to JIT-compile numerical Python functions, that would otherwise be slow, to efficient
platform code.\todo{concolic execution?}

As for the trade-off between transformation-based and library-based implementations, several hybrid
graph tracking approaches between source transformation and graph tracking exist.  Among AD systems,
recent TensorFlow versions have introduced
AutoGraph\footnote{\url{https://www.tensorflow.org/api_docs/python/tf/autograph}, visited
  on~2020-10-26}, which rewrites regular Python functions to traced TensorFlow implementations by
replacing control flow statements by TensorFlow combinators which.  Such functions still need to be
re-traced whenever a non-tensor input argument changes.  Its predecessor TensorFlow Fold
\parencite{looks2017deep} follows a similar, but more explicit style and provides many of these
combinators as \enquote{dynamic batching operators} to define static graphs emulating dynamic
operations.  The \enquote{dynamicity} problem can be approached in other ways as well:
\emph{stochastic memoization} is employed in the probabilistic programming languages Church
\parencite{goodman2012church} and Venture \parencite{mansinghka2014venture} to produce what in the
latter is called \enquote{probabilistic execution traces}: multiple different traces are dynamically
stored as alternative paralles paths in the execution trace, with possible interconnections.


%%% Local Variables: 
%%% TeX-master: "main"
%%% End: