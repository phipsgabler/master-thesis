\chapter{Background}
\label{cha:background}

This chapter provides the background for the concepts used later in
chapters~\ref{cha:impl-dynam-graph} and \ref{cha:graph-track-prob}.  Initially, it gives a quick
overview of Baysian inference and probabilistic programming in general, necessary to understand the
requirements and usual approaches of probabilistic programming systems.

Consequently, the machinery and language used to develop the graph tracking system forming the main
part of the work are described.  This consists firstly of a short introduction to graph tracking and
source-to-source automatic differentiation, which contain many ideas and terminology that will be
used later, and often provided inspiration.  Secondly, the basic notions and techniques of the Julia
compilation process as well as the language's metaprogramming capabilities are described, which form
the basis of the implementation.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bayesian Inference and MCMC methods}
\label{sec:bayes-infer}

Generative modelling is an approach for modelling phenomena based on the assumption that
observables can be fully described through some stochastic process.  When we assume this process to
belong to a specified family of processes, the estimation of the \enquote{best} process is a form of
learning: if we have a good description of how obserations are generated, we can make summary
statements about the whole population (descriptive statistics) or predictions about new
observations.  When observations come in pairs of independent and dependent variables, learning the
conditional model of one given the other solves a regression or classification problem.

Within a Baysian statistical framework, we assume that the family of processes used is specified by
random variables related through conditional distributions with densities, which describe how the
observables would be generated: some \emph{unobserved variables} are generated from \emph{prior
  distributions}, and the \emph{observed data} are generated conditionally on the unobserved
variables.  The goal is to learn the \emph{posterior distribution} of the parameters given the
observations, which is a sort of \enquote{inverse} of how the problem is specified.

As an example, consider image classification: if we assume that certain percentages of an image data
set picture cats and dogs, respectively, the distribution of these labels forms the prior.  Given
the information which kind of animal is depicted on it, an image can then be generated as a matrix
of pixels based on a distribution of images conditioned on labels.  The posterior distribution is
then conditional distribution of the label given an image.  When we have this information, we can,
for example, build a Baysian classifier, by returning for a newly observed image that label which
has the highest probability under the posterior.

This kind of learning is called Bayesian inference since, in the form of densities, the form of the
model can be expressed using Bayes' theorem as the conditional distribution with
density\footnote{Note the abuse of notation regarding \(\prob{\cdot}\); see
  page~\pageref{cha:notation} on notation.}
\begin{equation}
  \label{eq:bayes}
  \overbrace{\prob{\theta \given x}}^{\text{posterior}} =
  \frac{\overbrace{\prob{x \given \theta}}^{\text{likelihood}}\;
    \overbrace{\prob{\theta}}^{\text{prior}}}{\prob{x}},
\end{equation}
where \(x\) are the observed data, and \(\theta\) are the unobserved parameters. The posterior
represents the distribution of the unobserved variables as a combination of the prior belief updated
by what has been observed~\parencite{congdon2006bayesian}.  (In practice, not all of the unobserved
variables have to be model parameters we are actually interested in; these can be integrated out).

Going beyond simple applications like the classifier mentioned above, handling the posterior gets
difficult, though.  Simply evaluating the posterior density
\(\theta \mapsto \prob{\theta \given x}\) at single points is not enough in a Baysian setting for
usages such as prediction, parameter estimation, or evaluation of probabilities of continuous
variables.  The problem is that almost all of the relevant quantities depend on some sort of
expectation over the posterior density, an integral of the form
\begin{equation}
  \label{eq:posterior-expectation}
  \Exp{f(\Theta) \given X = x} = \int f(\theta) \prob{\theta \given x} \dif \mu(\theta),
\end{equation}
for some measurable function \(f\) (with the base measure \(\mu\) depending on the type of
\(\Theta\)). This in turn involves calculating the normalizing marginal
\begin{equation}
  \label{eq:normalizing}
  \prob{x} = \int \prob{x, \theta} \dif \mu(\theta).
\end{equation}
in equation~\ref{eq:bayes}, often called the \enquote{evidence}.

When the distributions involved form a sufficiently \enquote{nice} combination, e.g., a conjugate
pair \parencites[see][chapter 2.2.2]{marin2007bayesian}[chapter
9.2.5]{murphy2012machine}, the integration can be performed analytically, since the posterior
density has a closed form for a certain known distribution, or at least is a known integral.  In
general, however, this is not tractable, not even by standard numerical integration methods, and
approximations have to be made.  Even for discrete variables, the applicability of simple summation
is limited by combinatorial explosion.

\newthought{Different techni{q}ues} for posterior approximation are available: among them are
distribution-based approaches for general graphical models, such as variational inference
\parencite[chapter 21 and 22]{murphy2012machine} and other methods generalized under the framework
of message passing \parencite{minka2005divergence}.  The methods described in this thesis, however,
fall into the category of Monte Carlo methods, and are based on sampling \parencites[chapter
23]{murphy2012machine}{vihola2020lectures}.  Their fundamental idea is to derive, for a specified
density of \(\Theta \from \pi\), a sampling procedure with a consistent estimator for expectations:
\begin{equation}
  \label{eq:mc-methods}
  \kth{I}(f) \to \Exp{f(\Theta)} = \int f(\theta) \pi(\theta) \dif\mu(\theta), \quad \text{as} \quad k
  \to \infty
\end{equation}
in some appropriate stochastic convergence (usually convergence in probability is enough).  We leave
out the conditional dependency on \(X\) here for simplicity in notation, and since the data are
usually fixed in inference problems.

Examples of such methods are rejection sampling, importance sampling, and particle filters.  Many
Monte Carlo methods are defined in a form that directly samples a sequence of individual random
variables \(\sequence{\kth{Y}}\), called a \emph{chain}, for which the estimator is given by the
arithmetic mean, such that a law of large numbers (LLN) holds:
\begin{equation}
  \label{eq:mc-lln}
  \kth{I}(f) = \frac{1}{k} \sum_{i=1}^{k} f(\kth[i]{Y}) \to \Exp{f(\Theta)}
\end{equation}
When we can sample \(\kth{Y} \from \pi\) exactly, they are \iid{} and the LLN holds trivially; such
samplers exist, but might also be difficult to derive or not possess good enough convergence
properties (especially in high dimensions).  Another large class of samplers is formed by
\emph{Markov Chain Monte Carlo} (MCMC) methods, which, instead of sampling exactly from the density,
define \(\kth{Y}\) via a (time-homogeneous) Markov chain:
\begin{equation}
  \label{eq:mc-kernel}
  \begin{aligned}
    &\Prob{\kth[k+1]{Y} \in \dif y
      \given \kth{Y} = \kth{y}, \ldots, \kth[1]{Y} = \kth[1]{y}} \\
    &\quad = \Prob{\kth[k+1]{Y} \in \dif y \given \kth{Y} = \kth{y}}  \\
    &\quad = K(dy \given \kth{y}) \\
    % &\quad = k(y \given \kth{y}) \dif\mu(y)
  \end{aligned}
\end{equation}
for all \(k \ge 1\).  By constructing the \emph{transition kernel}, \(K\), in the right way, the
resulting chain is ergodic with the target density \(\pi\) as the unique stationary distribution,
i.e., for all measurable sets \(A\),
\begin{equation}
  \label{eq:stationarity}
  \int \pi(y) K(A \given y) \dif\mu(y) = \int_A \pi(y) \dif\mu(y),
\end{equation}
and the LLN for Markov chains holds.  (For discrete spaces, this relation is more familiarly written
as a left eigenvalue equation on a stochastic matrix: \(\pi K = \pi\).)  The advantage of MCMC
methods is that they apply equally well to many structurally complex models, and treat densities in
a uniform way, without requiring special knowledge about the specific distribution in question.  I
refer to \textcites[chapter 6]{vihola2020lectures}{robert1999monte}[chapters 24 and
following]{murphy2012machine} as introductions to MCMC theory and practice.

\newthought{Fre{q}uently, MCMC methods} are variations of the \emph{Metropolis-Hastings algorithm}
(MH), which splits the general definition of the transition kernel into two parts: a proposal
distribution, given by a conditional density \(q\) that needs to be easy to sample from, and an
acceptance rate \(\alpha\).  Subsequent samples are then produced by proposing values from \(q\)
given the previous element of the chain, and incorporating them into the chain with a probability
given through \(\alpha\) (see algorithm~\ref{alg:mh}).\todo{nicer algorithm formatting} There exist
many MH-based schemes with different properties and requirements: from the classical random-walk
Metropolis algorithm with Gaussian proposals, over Reversible Jump MCMC for varying dimensions
\parencite{green1995reversible}, to gradient-informed methods like Metropolis Adjusted Langevin and
Hamiltonian Monte Carlo (HMC) \parencite{betancourt2018conceptual,girolami2011riemann}.

\begin{algorithm}[t]
  \begin{myalgorithmic}
  \item Start from an arbitrary \(\kth[1]{Y} = \kth[1]{y}\) with \(\pi(\kth{y}) > 0\).
  \item For each \(k \ge 1\):
    \begin{enumerate}
    \item Sample a proposal \(\kth[k]{\hat{Y}} \from q(\kth[k-1]{Y}, \cdot)\).
    \item With probability \(\alpha(\kth[k]{\hat{Y}}, \kth[k-1]{Y})\), set
      \(\kth[k]{Y} = \kth[k]{\hat{Y}}\); else, keep \(\kth[k]{Y} = \kth[k-1]{Y}\).
    \end{enumerate}
  \end{myalgorithmic}
  \caption{General scheme for the Metropolis-Hastings algorithm.\label{alg:mh}}
\end{algorithm}

For multi-component structures, of the form \(\Theta = [\Theta_1, \ldots, \Theta_N]\), a good
proposal distribution can be hard to find, though.  One way to break down the problem is to use a
family of componentwise updates, given by conditional distributions \(q_{i}\) operating on only one
component of \(\Theta\), with the others fixed:
\begin{equation}
  \label{eq:conditional-kernels}
  \begin{aligned}
    \kth[k]{\hat{Y}}_{-i} &= \kth[k-1]{Y}_{-i} \\
    \kth[k]{\hat{Y}}_{i} &\from q_{i}(\kth[k-1]{Y}_{i}, \cdot \given \kth[k-1]{Y}_{-i})
  \end{aligned}
\end{equation}
The components can be scalar or multivariate blocks, and the kernel may itself be any valid
transition kernel \parencite[chapter 6.6]{vihola2020lectures}.  This allows one to freely mix
different MCMC methods suitable for each variable in a problem.

This so-called \enquote{within-Gibbs} sampler bears its name because it is a generalization of the
classical \emph{Gibbs sampling} algorithm \parencite{geman1984stochastic}: often, the simplest
available set of transition kernels is given by the conditional densities
\(\theta_{i} \mapsto p(\theta_{i} \given \theta_{-i}, x)\). They can directly be used as component
proposals for a within-Gibbs sampler, leading to a cancelling acceptance rate of
\(\alpha \equiv 1\).  This approach has the advantage of being very algorithmic, which makes it
rather easy to apply, even by hand, to many models, and simply to express algorithmically.  Hence,
the method is a popular starting point for general probabilistic programming systems, most
prominently BUGS \parencite{lunn2000winbugs,lunn2009bugs} and JAGS \parencite{plummer2003jags}.

In many real-world models, the factorization structure is quite sparse and results in small Markov
blankets.  Algorithms to derive Gibbs samplers exploit this large independency between variables.
In short, they \enquote{trim} the dependency graph of the model to the local Markov blankets of each
target variable, and derive either a full conditional from it, where possible (for discrete or
conjugate variables), or otherwise approximate it through appropriate local sampling (e.g., slice
sampling).

As an example, consider a simple Gaussian mixture model with equal weights, specified as follows:
\begin{equation}
  \label{eq:normal-mixture-1}
  \begin{aligned}
    \mu_{k} &\iidfrom \Normal(m, s) \quad\text{for } 1 \le k \le K, \\
    Z_{n} &\iidfrom \distr{Categorical}(K) \quad\text{for } 1 \le n \le N, \\
    X_{n} &\iidfrom \Normal(\mu_{Z_{n}}, \sigma) \quad\text{for } 1 \le n \le N
  \end{aligned}
\end{equation}
To derive the conditional distribution of \(Z_{n}\) given the remaining variables, we start by
writing down the factorization of the joint density:
\begin{equation}
  p(z_{1:N}, \mu_{1:K}, x_{1:N}) = \prod_{k} p(\mu_{k}) \prod_{n} p(z_{n}) \prod_{n} p(x_{n} | \mu_{z_{n}}).
\end{equation}
From this, we can derive an unnormalized density proportional to the conditional by removing all
factors not including the target variable:
\begin{equation}
    p(z_{n} \given z_{-n}, \mu_{1:K}, x_{1:N}) \propto p(z_{n}) p(x_{n} | \mu_{z_{n}})
\end{equation}
This is equivalent to finding the Markov blanket of \(Z_{n}\): only those conditionals relating the
target variable to its children and parents remain.  Since the clusters are drawn from a categorical
distribution, the support is discrete, and we can find the normalization constant by summation:
\begin{equation}
  \setlength{\jot}{0.8\baselineskip}
  \begin{aligned}
    &p(z_{n} \given z_{-n}, \mu_{1:K}, x_{1:N}) \\
    % &\quad = p(z_{n}) p(x_{n} | \mu_{z_{n}}) / Z \\
    &\quad = \dfrac{\distr{Categorical}(z_{n} \given K) \, \Normal(x_{n} \given \mu_{z_{n}}, \sigma)}{
      \sum_{k \in\, \supp(Z_{n})} \distr{Categorical}(k \given K)\, \Normal(x_{n} \given \mu_{k},
      \sigma)}, \\
    % &\quad = \dfrac{\distr{Categorical}(z_{n} \given K) \, \Normal(x_{n} \given \mu_{z_{n}}, \sigma)}{
    %   \sum_{k = 1}^{K} \distr{Categorical}(k \given K)\, \Normal(x_{n} \given \mu_{k}, \sigma)},
  \end{aligned}
\end{equation}
which can be expressed as a general discrete distribution over \(\supp(Z_{n}) = \{1, \ldots, K\}\),
with the unnormalized weights given by the numerator.  Next, the conditionals of the \(\mu_{k}\)
have the form
\begin{equation}
  \setlength{\jot}{0.8\baselineskip}
  \begin{aligned}
    &p(\mu_{k} \given z_{1:N}, \mu_{-k}, x_{1:N}) \\
    &\quad \propto p(\mu_k) \prod_{n} p(x_{n} | \mu_{k})^{\indicator{z_{n} \, = \, k}} \\
    &\quad = \prod_{n} \left( \Normal(\mu_{k} \given m, s) \,
      \Normal(x_{n} \given \mu_{k}, \sigma) \right)^{\indicator{z_{n} \, = \, k}}
  \end{aligned}
\end{equation}
which we recognize as a product of conjugate pairs of normal distributions.  More examples are
extensively covered in \textcite[chapter 24.2]{murphy2012machine}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Probabilistic Programming}
\label{sec:prob-prog}

Probabilistic programming is a structured way implementing generative models, as described in the
previous section, through the syntax of a programming language.  It is beneficial to consider
probabilistic programs not only as syntactic sugar for denoting the implementation of a joint
probability density over some set of variables, but as organized objects in their own right: they
open up possibilities that \enquote{black box} density functions cannot automatically provide. In
more concise terms from \textcite{vandemeent2018introduction}:
\begin{quote}
  Probabilistic programming is largely about designing languages, interpreters, and compilers that
  translate inference problems denoted in programming language syntax into formal mathematical
  objects that allow and accommodate generic probabilistic inference, particularly Bayesian
  inference and conditioning.
\end{quote}

A probabilistic program differs from a regular program (that may also contain stochastic parts)
through the possibility of being conditioned on: some of the internal variables can be fixed to
observed values, from outside. As such, the program denotes on the one hand a joint distribution,
that can be \emph{forward sampled} from by simply running the program top to bottom and producing
(pseudo-) random values.  But at the same time, it also represents a conditional distribution, in
form on the unnormalized conditional density, which together with an inference algorithm can also be
\emph{backward sampled} from.  (Other terms, such as \enquote{evaluation} and \enquote{querying},
are used as well.)  Consider the model~\eqref{eq:normal-mixture-1} from above: its mathematical
description might be translated into a program in \turingjl{} syntax
\parencite{ge2018turing,tarek2020dynamicppl}\todo{more prominent turing introduction} as written in
listing~\ref{lst:normal}.
% \begin{codelisting}[t]
\begin{lstlisting}[%
  float=t, frame=tb, caption={%
  \turingjl{} implementation of a Gaussian mixture model with prior on the cluster centers,
  equal cluster weights, and all other parameters fixed.},
  label={lst:normal}]
@model function normal_mixture(x, K, m, s, σ)
    N = length(x)

    μ = Vector{Float64}(undef, K)
    for k = 1:K
        μ[k] ~ Normal(m, s)
    end

    z = Vector{Int}(undef, N)
    for n = 1:N
        z[n] ~ Categorical(K)
    end

    for n = 1:N
        x[n] ~ Normal(μ[z[n]], σ)
    end

    return x
end
\end{lstlisting}
\todo{Fix floated listings layout}
We can then sample from the model in several ways:
\begin{lstlisting}
julia> m = normal_mixture(x_observations, K, m, s, σ);
julia> forward = sample(m, Prior(), 10);
julia> chain = sample(m, MH(), 1000);
\end{lstlisting}
The value of \jlinl{forward} will be an dataframe-like object containing 10 values for each variable
sampled from the forward (i.e., joint) distribution, matching the size of \jlinl{x_observations}.
Similarly, \jlinl{chain} will contain a length 1000 sample from a Markov chain targetting the
posterior, conditially on \jlinl{x_observations}, created using the MH algorithm.  If we were to
write out these two functionalities manually, in idiomatic Julia, we would end up with at least the
following two functions:
\begin{lstlisting}
function normal_mixture_sampler(N, K, m, s, σ)
    μ = rand(Normal(m, s), K)
    z = rand(Categorical(K), N)
    x = rand.(Normal.(μ[z], s))
    return μ, z, x
end

function normal_mixture_logpdf(μ, z, x, K, m, s, σ)
    N = length(x)
    ℓ = 0.0
    ℓ += sum(logpdf(Normal(m, s), μ[k]) for k = 1:K)
    ℓ += sum(logpdf(Categorical(K), z[n]) for n = 1:N)
    ℓ += sum(logpdf(Normal(μ[z[n]]), x[n]) for n = 1:N)
    return ℓ
end
\end{lstlisting}
And still, with these, we would lack much of the flexibility that models written in \dppljl{} form:
no general interface for sampling algorithms to automatically detect all latent and observed
variables; no possility for other, nonstandard execution forms as are needed for Variational
Inference or gradient computation for HMC; no automatic dataframe construction for chains.  All
these points highlight the advantages of dedicated probabilistic programming languages (PPLs) over
manual likelihood functions.

\newthought{Many PPLs are implemented} as external domain-specific languages (DSLs), like Stan
\parencite{carpenter2017stan}, JAGS \parencite{plummer2003jags}, and BUGS
\parencite{lunn2000winbugs,lunn2009bugs}.  Others are specified in the \enquote{meta-syntax} of Lisp
S-expressions, as Church \parencite{goodman2012church}, Anglican \parencite{wood2015new}, or Venture
\parencite{mansinghka2014venture}.  A third group is embedded into host programming languages with
sufficient syntactic flexibility, for example Gen \parencite{cusumano-towner2020gen} and Soss
\parencite{scherrer2019soss} in Julia (besides the already named Turing), or Pyro
\parencite{bingham2018pyro} and PyMC3 \parencite{salvatier2016probabilistic} in Python.

The latter approach is advantageous when one wants to enable the use of regular, general-purpose
programming constructs or interact with other functionalities of the host language.  There are also
a variety of further reasons why one would rather describe an inference problem in terms of a
program than in more \enquote{mathematical} form, like as a graph or likelihood function.  In a good
probabilistic programming DSL, models will read as close to textbook model specifications as
possible, while allowing to use the host language to:
\begin{itemize}
  \firmlist
\item define recursive relationships,
\item write models using imperative constructs, such as loops, or mutable intermediate computations
  for efficiency,
\item optimize details of th execution, e.g. for memoization, likelihood scaling, or preliminary
  termination
\item use distributions over complex custom data structures, e.g. trees,
\item perform inference involving complex transformations from other domains, for which
  implementations already exist, e.g. neural networks or differential equation solvers , or
\item integrate calls to very complex external systems, e.g. simulators or renderers.
\end{itemize}
See \textcite{vandemeent2018introduction} for a general introduction into some common implementation
approaches for PPLs. \Textcite{goodman2014design} gives a detailed overview of the internals of one
specific, continuation-based implementation called WebPPL (using a Lisp-based syntax).

\begin{lstlisting}[float=p, caption={Expansion of model~\ref{lst:normal}.}, label={lst:normal-expanded}]
function normal_mixture(x, K, m, s, σ)
    function evaluator((rng, model, varinfo, sampler, context, x, K, m, s, σ)
        N = length(x)
        μ = Vector{Float64}(undef, K)
        for k = 1:K
            dist_mu = Normal(m, s)
            vn_mu = @varname μ[k]
            inds_mu = ((k,),)
            μ[k] = tilde_assume(
                rng, context, sampler, dist_mu, vn_mu, inds_mu, varinfo
            )
        end
        z = Vector{Int}(undef, N)
        for n = 1:N
            dist_z = Categorical(K)
            vn_z = @varname z[n]
            inds_z = ((n,),)
            z[n] = tilde_assume(
                rng, context, sampler, dist_z, vn_z, inds_z, varinfo
            )
        end
        for n = 1:N
            dist_x = Normal(μ[z[n]], σ)
            vn_x = @varname(x[n])
            inds_x = ((n,),)
            if isassumption(model, x, vn_x)
                x[n] = tilde_assume(
                    rng, context, sampler, dist_x, vn_x, inds_x, varinfo
                )
            else
                tilde_observe(
                    context, sampler, dist_x, x[n], vn_x, inds_x, varinfo
                )
            end
        end
        return x
    end
    return Model(
        :normal_mixture, evaluator, 
        (x = x, K = K, m = m, s = s, σ = σ), 
        NamedTuple()
    )
end
\end{lstlisting}

\newthought{Models in \turingjl{}} work by transforming the model code, which is syntactically a
valid Julia function definition, through the macro \jlinl{@model}.\todo{cite DPPL paper} The result
is new function which produces instances of a structure of type \jlinl{Model}, that in turn will
contain the observations, some metadata, and a nested function with the slightly changed original
model code. In the concrete case of the model in listing~\ref{lst:normal}, the resulting code would
be approximately equal to the code in listing~\ref{lst:normal-expanded}.  The purpose of this is the
following: the outer function, the \enquote{generator}, constructs an instance of the model for
given parameters~-- usually done once per inference problem, to fix the observations and
hyperparameters.  Subsequently, the \jlinl{sample} function can be applied to this instance with
different values for the sampling algorithm, which in turn will use the \jlinl{evaluator} function
of the instance to run the model with chosen \jlinl{sampler} and \jlinl{context} arguments, that are
passed to the \enquote{tilde functions}, to which the statements of the form \jlinl{expr ~ D} are
converted.

Inside the \enquote{tilde functions}, the real stochastic work happens.\todo{observe vs assume}
Depending on the \jlinl{sampler} and the \jlinl{context}, values may be generated and stored in the
\jlinl{varinfo} object, as happens for most \enquote{real} samplers.  In this case, one call to the
\jlinl{evaluator} corresponds to one (Gibbs) sampling step.  In other situations, model evaluation
serves the purpose of density evaluation, in which no new values need to be produced; this use case
is needed for probability queries, or density-based inference methods (which might additionally use
automatic differentiation on the density evaluation procedure).  All shared information for external
usage is thereby conventionally stored in the \jlinl{varinfo} object, which resembles a dictionary
from variable names\footnote{These \jlinl{VarName} objects, constructed by the macro
  \jlinl{@varname}, simply represent an indexed variable by a symbol and an index tuple.} to
values (internal sampler state can also be stored in the \jlinl{sampler} object).  Through the
\jlinl{sample} interface, the resulting values are then stored in a \jlinl{Chains} object, a data
frame containing a value for each variable at each sampling step.

A special distinction is made for variables that are contained in the model arguments.  These by
default serve as observed variables, and will only contribute to the likelihood, instead of being
sampled.  But in certain cases, such as in probability evaluation or when using the complete model
in a generative way, this behaviour can be different.  For this purpose, the variables \jlinl{x[i]}
in the example are wrapped in a conditional statement.

From the point of view of a sampling algorithm, all that it sees is a sequence of tilde statements,
consisting of a value, a variable name, and a distribution.  Turing, crucially, does not have a
representation of model structure.  This is sufficient for many kinds of inference algorithms that
it already implements~-- Metropolis-Hastings, several particle methods, HMC and NUTS, and
within-Gibbs combinations of these~-- but does not allow more intelligent usage of the available
information.  For example, to use a true, conditional, Gibbs sampler, the user has to calculate the
conditionals themselves.  Optimizations such as partial reevaluation of a model to save
calculations, automatic conjugacy detection, or model transformations such as Rao-Blackwellization
cannot be performed in this representation.\todo{cite autoconj etc.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Compilation and Metaprogramming in Julia}
\label{sec:comp-metapr-julia}

sldk sldks dlks dlksfj dlkfjs lkdjf slkdjls dkf

\textcite{singer2018static}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Computation Graphs and Automatic Differentiation}
\label{sec:graph-track-autom}

% 1. AD in general
% 2. Zygote principles



%%% Local Variables: 
%%% TeX-master: "main"
%%% End: