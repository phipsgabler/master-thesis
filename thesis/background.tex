\chapter{Background}
\label{cha:background}

This chapter provides the background for the concepts used later in
chapters~\ref{cha:impl-dynam-graph} and \ref{cha:graph-track-prob}.  Initially, it gives a quick
overview of Baysian inference and probabilistic programming in general, necessary to understand the
requirements and usual approaches of probabilistic programming systems.

Consequently, the machinery and language used to develop the graph tracking system forming the main
part of the work are described.  This consists firstly of the basic notions and techniques of the
Julia compilation process as well as the language's metaprogramming capabilities are described,
which form the basis of the implementation.  Secondly, a short introduction to graph tracking and
source-to-source automatic differentiation is given, which contains many ideas and terminology that
will be used later, and often provided inspiration.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bayesian Inference and MCMC methods}
\label{sec:bayes-infer}

Generative modelling is an approach for modelling phenomena based on the assumption that
observables can be fully described through some stochastic process.  When we assume this process to
belong to a specified family of processes, the estimation of the \enquote{best} process is a form of
learning: if we have a good description of how obserations are generated, we can make summary
statements about the whole population (descriptive statistics) or predictions about new
observations.  When observations come in pairs of independent and dependent variables, learning the
conditional model of one given the other solves a regression or classification problem.

Within a Baysian statistical framework, we assume that the family of processes used is specified by
random variables related through conditional distributions with densities, which describe how the
observables would be generated: some \emph{unobserved variables} are generated from \emph{prior
  distributions}, and the \emph{observed data} are generated conditionally on the unobserved
variables.  The goal is to learn the \emph{posterior distribution} of the parameters given the
observations, which is a sort of \enquote{inverse} of how the problem is specified.

As an example, consider image classification: if we assume that certain percentages of an image data
set picture cats and dogs, respectively, the distribution of these labels forms the prior.  Given
the information which kind of animal is depicted on it, an image can then be generated as a matrix
of pixels based on a distribution of images conditioned on labels.  The posterior distribution is
then conditional distribution of the label given an image.  When we have this information, we can,
for example, build a Baysian classifier, by returning for a newly observed image that label which
has the highest probability under the posterior.

This kind of learning is called Bayesian inference since, in the form of densities, the form of the
model can be expressed using Bayes' theorem as the conditional distribution with
density\footnote{Note the abuse of notation regarding \(\prob{\cdot}\); see
  page~\pageref{cha:notation} on notation.}
\begin{equation}
  \label{eq:bayes}
  \overbrace{\prob{\theta \given x}}^{\text{posterior}} =
  \frac{\overbrace{\prob{x \given \theta}}^{\text{likelihood}}\;
    \overbrace{\prob{\theta}}^{\text{prior}}}{\prob{x}},
\end{equation}
where \(x\) are the observed data, and \(\theta\) are the unobserved parameters. The posterior
represents the distribution of the unobserved variables as a combination of the prior belief updated
by what has been observed~\parencite{congdon2006bayesian}.  (In practice, not all of the unobserved
variables have to be model parameters we are actually interested in; these can be integrated out).

Going beyond simple applications like the classifier mentioned above, handling the posterior gets
difficult, though.  Simply evaluating the posterior density
\(\theta \mapsto \prob{\theta \given x}\) at single points is not enough in a Baysian setting for
usages such as prediction, parameter estimation, or evaluation of probabilities of continuous
variables.  The problem is that almost all of the relevant quantities depend on some sort of
expectation over the posterior density, an integral of the form
\begin{equation}
  \label{eq:posterior-expectation}
  \Exp{f(\Theta) \given X = x} = \int f(\theta) \prob{\theta \given x} \dif \mu(\theta),
\end{equation}
for some measurable function \(f\) (with the base measure \(\mu\) depending on the type of
\(\Theta\)). This in turn involves calculating the normalizing marginal
\begin{equation}
  \label{eq:normalizing}
  \prob{x} = \int \prob{x, \theta} \dif \mu(\theta).
\end{equation}
in equation~\ref{eq:bayes}, often called the \enquote{evidence}.

When the distributions involved form a sufficiently \enquote{nice} combination, e.g., a conjugate
pair \parencites[see][chapter 2.2.2]{marin2007bayesian}[chapter
9.2.5]{murphy2012machine}, the integration can be performed analytically, since the posterior
density has a closed form for a certain known distribution, or at least is a known integral.  In
general, however, this is not tractable, not even by standard numerical integration methods, and
approximations have to be made.  Even for discrete variables, the applicability of simple summation
is limited by combinatorial explosion.

\newthought{Different techni{q}ues} for posterior approximation are available: among them are
distribution-based approaches for general graphical models, such as variational inference
\parencite[chapter 21 and 22]{murphy2012machine} and other methods generalized under the framework
of message passing \parencite{minka2005divergence}.  The methods described in this thesis, however,
fall into the category of Monte Carlo methods, and are based on sampling \parencites[chapter
23]{murphy2012machine}{vihola2020lectures}.  Their fundamental idea is to derive, for a specified
density of \(\Theta \from \pi\), a sampling procedure with a consistent estimator for expectations:
\begin{equation}
  \label{eq:mc-methods}
  \kth{I}(f) \to \Exp{f(\Theta)} = \int f(\theta) \pi(\theta) \dif\mu(\theta), \quad \text{as} \quad k
  \to \infty
\end{equation}
in some appropriate stochastic convergence (usually convergence in probability is enough).  We leave
out the conditional dependency on \(X\) in the following for simplicity of notation, and since the
data are usually fixed in inference problems.

Examples of such methods are rejection sampling, importance sampling, and particle filters.  Many
Monte Carlo methods are defined in a form that directly samples a sequence of individual random
variables \(\sequence{\kth{Y}}\), called a \emph{chain}, for which the estimator is given by the
arithmetic mean, such that a law of large numbers (LLN) holds:
\begin{equation}
  \label{eq:mc-lln}
  \kth{I}(f) = \frac{1}{k} \sum_{i=1}^{k} f(\kth[i]{Y}) \to \Exp{f(\Theta)}
\end{equation}
If we can sample \(\kth{Y} \from \pi\) exactly, they are \iid{} and the LLN holds trivially; such
samplers exist, but might also be difficult to derive or not possess good enough convergence
properties (especially in high dimensions).  Another large class of samplers is formed by
\emph{Markov Chain Monte Carlo} (MCMC) methods, which, instead of sampling exactly from the density,
define \(\kth{Y}\) via a (time-homogeneous) Markov chain:
\begin{equation}
  \label{eq:mc-kernel}
  \begin{aligned}
    &\Prob{\kth[k+1]{Y} \in \dif y
      \given \kth{Y} = \kth{y}, \ldots, \kth[1]{Y} = \kth[1]{y}} \\
    &\quad = \Prob{\kth[k+1]{Y} \in \dif y \given \kth{Y} = \kth{y}}  \\
    &\quad = K(dy \given \kth{y}) \\
    % &\quad = k(y \given \kth{y}) \dif\mu(y)
  \end{aligned}
\end{equation}
for all \(k \ge 1\).  By constructing the parametrize measure \(K\), the \emph{transition kernel},
in the right way, the resulting chain is ergodic with the target density \(\pi\) as the unique
stationary distribution, i.e., for all measurable sets \(A\),
\begin{equation}
  \label{eq:stationarity}
  \int \pi(\theta) K(A \given \theta) \dif\mu(\theta) = \int_A \pi(\theta) \dif\mu(\theta) =
  \Prob{\Theta \in A},
\end{equation}
and the LLN for Markov chains holds.  (For discrete spaces, this relation is more familiarly written
as a left eigenvalue equation on a stochastic matrix: \(\pi K = \pi\).)  The advantage of MCMC
methods is that they apply equally well to many structurally complex models, and treat densities in
a uniform way, without requiring special knowledge about the specific distribution in question.  I
refer to \textcites[chapter 6]{vihola2020lectures}{robert1999monte}[chapters 24 and
following]{murphy2012machine} as introductions to MCMC theory and practice.

\newthought{Fre{q}uently, MCMC methods} are variations of the \emph{Metropolis-Hastings algorithm}
(MH), which splits the general definition of the transition kernel into two parts: a proposal
distribution, given by a conditional density \(q\) that needs to be easy to sample from, and an
acceptance rate \(\alpha\).  Subsequent samples are then produced by proposing values from \(q\)
given the previous element of the chain, and incorporating them into the chain with a probability
given through \(\alpha\) (see algorithm~\ref{alg:mh}).  There exist many MH-based schemes with
different properties and requirements: from the classical random-walk Metropolis algorithm with
Gaussian proposals, over Reversible Jump MCMC for varying dimensions
\parencite{green1995reversible}, to gradient-informed methods like Metropolis Adjusted Langevin and
Hamiltonian Monte Carlo (HMC) \parencite{betancourt2018conceptual,girolami2011riemann}.

% \begin{algorithm}[t]
%   \sffamily
%   \hrule
%   \begin{enumerate}
%     \firmlist
%   \item Start from an arbitrary \(\kth[1]{Y} = \kth[1]{y}\) with \(\pi(\kth{y}) > 0\).
%   \item For each \(k \ge 1\):
%     \begin{enumerate}
%       \firmlist
%     \item Sample a proposal \(\kth[k]{\hat{Y}} \from q(\kth[k-1]{Y}, \cdot)\).
%     \item With probability \(\alpha(\kth[k]{\hat{Y}}, \kth[k-1]{Y})\), set
%       \(\kth[k]{Y} = \kth[k]{\hat{Y}}\); else, keep \(\kth[k]{Y} = \kth[k-1]{Y}\).
%     \end{enumerate}
%   \end{enumerate}
%   \hrule
%   \caption{General scheme for the Metropolis-Hastings algorithm.\label{alg:mh}}
% \end{algorithm}

\begin{algorithm}[t]
  \hrule
  \begin{algorithmic}
    \State Start from an arbitrary \(\kth[1]{Y} = \kth[1]{y}\) with \(\pi(\kth{y}) > 0\)
    \For{\(k \ge 1\)}
    \State Sample a proposal \(\kth[k]{\hat{Y}} \from q(\kth[k-1]{Y}, \cdot)\)
    \State With probability \(\alpha(\kth[k]{\hat{Y}}, \kth[k-1]{Y})\), set
    \(\kth[k]{Y} = \kth[k]{\hat{Y}}\); else, keep \(\kth[k]{Y} = \kth[k-1]{Y}\).
    \EndFor
  \end{algorithmic}
  \hrule
  \caption{General scheme for the Metropolis-Hastings algorithm.\label{alg:mh}}
\end{algorithm}

For multi-component structures, of the form \(\Theta = [\Theta_1, \ldots, \Theta_N]\), a good
proposal distribution can be hard to find, though.  One way to break down the problem is to use a
family of componentwise updates, given by conditional distributions \(q_{i}\) operating on only one
component of \(\Theta\), with the others fixed:
\begin{equation}
  \label{eq:conditional-kernels}
  \begin{aligned}
    \kth[k]{\hat{Y}}_{-i} &= \kth[k-1]{Y}_{-i} \\
    \kth[k]{\hat{Y}}_{i} &\from q_{i}(\kth[k-1]{Y}_{i}, \cdot \given \kth[k-1]{Y}_{-i})
  \end{aligned}
\end{equation}
The components can be scalar or multivariate blocks, and the kernel may itself be any valid
transition kernel \parencite[chapter 6.6]{vihola2020lectures}.  This allows one to freely mix
different MCMC methods suitable for each variable in a problem.

This so-called \enquote{within-Gibbs} sampler bears its name because it is a generalization of the
classical \emph{Gibbs sampling} algorithm \parencite{geman1984stochastic}: often, the simplest
available set of transition kernels is given by the conditional densities
\(\theta_{i} \mapsto p(\theta_{i} \given \theta_{-i}, x)\). They can directly be used as component
proposals for a within-Gibbs sampler, leading to a cancelling acceptance rate of
\(\alpha \equiv 1\).  This approach has the advantage of being very algorithmic, which makes it
rather easy to apply, even by hand, to many models, and simply to express algorithmically.  Hence,
the method is a popular starting point for general probabilistic programming systems, most
prominently BUGS \parencite{lunn2000winbugs,lunn2009bugs} and JAGS \parencite{plummer2003jags}.

In many real-world models, the factorization structure is quite sparse and results in small Markov
blankets.  Algorithms to derive Gibbs samplers exploit this large independency between variables.
In short, they \enquote{trim} the dependency graph of the model to the local Markov blankets of each
target variable, and derive either a full conditional from it, where possible (for discrete or
conjugate variables), or otherwise approximate it through appropriate local sampling (e.g., slice
sampling).

As an example, consider a simple Gaussian mixture model with equal weights, specified as follows:
\begin{equation}
  \label{eq:normal-mixture-1}
  \begin{aligned}
    \mu_{k} &\iidfrom \Normal(m, s) \quad\text{for } 1 \le k \le K, \\
    Z_{n} &\iidfrom \distr{Categorical}(K) \quad\text{for } 1 \le n \le N, \\
    X_{n} &\iidfrom \Normal(\mu_{Z_{n}}, \sigma) \quad\text{for } 1 \le n \le N.
  \end{aligned}
\end{equation}
To derive the conditional distribution of \(Z_{n}\) given the remaining variables, we start by
writing down the factorization of the joint density:
\begin{equation}
  p(z_{1:N}, \mu_{1:K}, x_{1:N}) = \prod_{k} p(\mu_{k}) \prod_{n} p(z_{n}) \prod_{n} p(x_{n} | \mu_{z_{n}}).
\end{equation}
From this, we can derive an unnormalized density proportional to the conditional by removing all
factors not including the target variable:
\begin{equation}
    p(z_{n} \given z_{-n}, \mu_{1:K}, x_{1:N}) \propto p(z_{n}) p(x_{n} | \mu_{z_{n}})
\end{equation}
This is equivalent to finding the Markov blanket of \(Z_{n}\): only those conditionals relating the
target variable to its children and parents remain.  Since the clusters are drawn from a categorical
distribution, the support is discrete, and we can find the normalization constant by summation:
\begin{equation}
  \setlength{\jot}{0.8\baselineskip}
  \begin{aligned}
    &p(z_{n} \given z_{-n}, \mu_{1:K}, x_{1:N}) \\
    % &\quad = p(z_{n}) p(x_{n} | \mu_{z_{n}}) / Z \\
    &\quad = \dfrac{\distr{Categorical}(z_{n} \given K) \, \Normal(x_{n} \given \mu_{z_{n}}, \sigma)}{
      \sum_{k \in\, \supp(Z_{n})} \distr{Categorical}(k \given K)\, \Normal(x_{n} \given \mu_{k},
      \sigma)}, \\
    % &\quad = \dfrac{\distr{Categorical}(z_{n} \given K) \, \Normal(x_{n} \given \mu_{z_{n}}, \sigma)}{
    %   \sum_{k = 1}^{K} \distr{Categorical}(k \given K)\, \Normal(x_{n} \given \mu_{k}, \sigma)},
  \end{aligned}
\end{equation}
which can be expressed as a general discrete distribution over \(\supp(Z_{n}) = \{1, \ldots, K\}\),
with the unnormalized weights given by the numerator.  Next, the conditionals of the \(\mu_{k}\)
have the form
\begin{equation}
  \setlength{\jot}{0.8\baselineskip}
  \begin{aligned}
    &p(\mu_{k} \given z_{1:N}, \mu_{-k}, x_{1:N}) \\
    &\quad \propto p(\mu_k) \prod_{n} p(x_{n} | \mu_{k})^{\indicator{z_{n} \, = \, k}} \\
    &\quad = \prod_{n} \left( \Normal(\mu_{k} \given m, s) \,
      \Normal(x_{n} \given \mu_{k}, \sigma) \right)^{\indicator{z_{n} \, = \, k}}
  \end{aligned}
\end{equation}
which we recognize as a product of conjugate pairs of normal distributions.  More examples are
extensively covered in \textcite[chapter 24.2]{murphy2012machine}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Probabilistic Programming}
\label{sec:prob-prog}

\begin{lstfloat}
  \begin{lstlisting}[style=lstfloat]
@model function normal_mixture(x, K, m, s, σ)
    N = length(x)

    μ = Vector{Float64}(undef, K)
    for k = 1:K
        μ[k] ~ Normal(m, s)
    end

    z = Vector{Int}(undef, N)
    for n = 1:N
        z[n] ~ Categorical(K)
    end

    for n = 1:N
        x[n] ~ Normal(μ[z[n]], σ)
    end

    return x
end
\end{lstlisting}
    \caption{\turingjl{} implementation of a Gaussian mixture model with prior on the cluster centers,
    equal cluster weights, and all other parameters fixed.\label{lst:normal}}
\end{lstfloat}

Probabilistic programming is a structured way implementing generative models, as described in the
previous section, through the syntax of a programming language.  It is beneficial to consider
probabilistic programs not only as syntactic sugar for denoting the implementation of a joint
probability density over some set of variables, but as organized objects in their own right: they
open up possibilities that \enquote{black box} density functions cannot automatically provide. In
more concise terms of \textcite{vandemeent2018introduction}:
\begin{quote}
  Probabilistic programming is largely about designing languages, interpreters, and compilers that
  translate inference problems denoted in programming language syntax into formal mathematical
  objects that allow and accommodate generic probabilistic inference, particularly Bayesian
  inference and conditioning.
\end{quote}

A probabilistic program differs from a regular program (that may also contain stochastic parts)
through the possibility of being conditioned on: some of the internal variables can be fixed to
observed values, from outside. As such, the program denotes on the one hand a joint distribution,
that can be \emph{forward sampled} from by simply running the program top to bottom and producing
(pseudo-) random values.  But at the same time, it also represents a conditional distribution, in
form on the unnormalized conditional density, which together with an inference algorithm can also be
\emph{backward sampled} from.  (Other terms, such as \enquote{evaluation} and \enquote{querying},
are used as well.)  Consider the model~\eqref{eq:normal-mixture-1} from above: to perform inference
on it in \turingjl{} \parencite{ge2018turing}, the probabilistic programming language used in this
thesis, its mathematical description might be translated into the Julia program given in
listing~\ref{lst:normal}.

We can then sample from the model in several ways using Julia:
\begin{lstlisting}
julia> m = normal_mixture(x_observations, K, m, s, σ);
julia> forward = sample(m, Prior(), 10);
julia> chain = sample(m, MH(), 1000);
\end{lstlisting}
The value of \jlinl{forward} will be an dataframe-like object containing 10 values for each variable
sampled from the forward (i.e., joint) distribution, matching the size of \jlinl{x_observations}.
Similarly, \jlinl{chain} will contain a length 1000 sample from a Markov chain targetting the
posterior, conditially on \jlinl{x_observations}, created using the MH algorithm.  If we were to
write out code for these two functionalities manually, in idiomatic Julia, we would end up with at
least two separate functions needed for the sampler:
\begin{lstlisting}
function normal_mixture_sampler(N, K, m, s, σ)
    μ = rand(Normal(m, s), K)
    z = rand(Categorical(K), N)
    x = rand.(Normal.(μ[z], s))
    return μ, z, x
end

function normal_mixture_logpdf(μ, z, x, K, m, s, σ)
    N = length(x)
    ℓ = 0.0
    ℓ += sum(logpdf(Normal(m, s), μ[k]) for k = 1:K)
    ℓ += sum(logpdf(Categorical(K), z[n]) for n = 1:N)
    ℓ += sum(logpdf(Normal(μ[z[n]]), x[n]) for n = 1:N)
    return ℓ
end
\end{lstlisting}
And still, with these, we would lack much of the flexibility that models written in \turingjl: no
general interface for sampling algorithms to automatically detect all latent and observed variables;
no possility for other, nonstandard execution forms as are needed for Variational Inference or
gradient computation for HMC; no automatic name extraction and dataframe building for chains.  All
these points highlight the advantages of dedicated probabilistic programming languages (PPLs) over
hand-written model code.  (Additionally, there is of course a benefit of reducing errors introduced
by the sampling function not matching the likelihood function, or errors involving
log-probabilities.)

\newthought{Many PPLs are implemented} as external domain-specific languages (DSLs), like Stan
\parencite{carpenter2017stan}, JAGS \parencite{plummer2003jags}, and BUGS
\parencite{lunn2000winbugs,lunn2009bugs}.  Others are specified in the \enquote{meta-syntax} of Lisp
S-expressions, as Church \parencite{goodman2012church}, Anglican \parencite{wood2015new}, or Venture
\parencite{mansinghka2014venture}.  A third group is embedded into host programming languages with
sufficient syntactic flexibility, for example Gen \parencite{cusumano-towner2020gen} and Soss
\parencite{scherrer2019soss} in Julia (besides the already named \turingjl{}), or Pyro
\parencite{bingham2018pyro} and PyMC3 \parencite{salvatier2016probabilistic} in Python.

The latter approach is advantageous when one wants to enable the use of regular, general-purpose
programming constructs or interact with other functionalities of the host language.  There are also
a variety of further reasons why one would rather describe an inference problem in terms of a
program than in more \enquote{mathematical} form, like as a graph or likelihood function.  In a good
probabilistic programming DSL, models will read as close to textbook model specifications as
possible, while allowing to use the host language to:
\begin{itemize}
  % \firmlist
\item define recursive relationships,
\item write models using imperative constructs, such as loops, or mutable intermediate computations
  for efficiency,
\item optimize details of th execution, e.g. for memoization, likelihood scaling, or preliminary
  termination,
\item use distributions over complex custom data structures, e.g. trees,
\item perform inference involving complex transformations from other domains, for which
  implementations already exist, e.g. neural networks or differential equation solvers , or
\item integrate calls to very complex external systems, e.g. simulators or renderers.
\end{itemize}
Depending on the choice of features should be supported, several possibilities for the
implementation of such a DSL exist.  All are based on some form of abstract interpretation.  A rough
distinction can be made between \emph{compilation-based methods}, which statically translate the
model code to a graph or density function, and \emph{evaluation-based methods}, which dynamically or
implicitely build such a structure at runtime, by allowing an inference algorithm to interleave the
execution.  The latter make it easier to include host-language control constructs.  See
\textcite{vandemeent2018introduction} for a general introduction into some common implementation
approaches for PPLs, and \textcite{goodman2014design} for a detailed overview of the internals of
one specific, continuation-based implementation called WebPPL (using a Lisp-based syntax).

\begin{lstfloat}
  \begin{lstlisting}[style=lstfloat]
function normal_mixture(x, K, m, s, σ)
    function evaluator((rng, model, varinfo, sampler, context, x, K, m, s, σ)
        N = length(x)
        μ = Vector{Float64}(undef, K)
        for k = 1:K
            dist_mu = Normal(m, s)
            vn_mu = @varname μ[k]
            inds_mu = ((k,),)
            μ[k] = tilde_assume(
                rng, context, sampler, dist_mu, vn_mu, inds_mu, varinfo
            )
        end
        z = Vector{Int}(undef, N)
        for n = 1:N
            dist_z = Categorical(K)
            vn_z = @varname z[n]
            inds_z = ((n,),)
            z[n] = tilde_assume(
                rng, context, sampler, dist_z, vn_z, inds_z, varinfo
            )
        end
        for n = 1:N
            dist_x = Normal(μ[z[n]], σ)
            vn_x = @varname(x[n])
            inds_x = ((n,),)
            if isassumption(model, x, vn_x)
                x[n] = tilde_assume(
                    rng, context, sampler, dist_x, vn_x, inds_x, varinfo
                )
            else
                tilde_observe(
                    context, sampler, dist_x, x[n], vn_x, inds_x, varinfo
                )
            end
        end
        return x
    end
    return Model(
        :normal_mixture, evaluator, 
        (x = x, K = K, m = m, s = s, σ = σ), 
        NamedTuple()
    )
end
\end{lstlisting}
  \caption{Slightly simplified macro-expanded code of the model in listing~\ref{lst:normal}.  The
    inner code is put into an \protect\jlinl{evaluator} closure, and very tilde statement is
    replaced by a \protect\jlinl{tilde_} function, to which additional data and state information
    are passed.\label{lst:normal-expanded}}
\end{lstfloat}
\setlength{\parskip}{0pt}

\newthought{Models in \turingjl{}} are written in \dppljl{} syntax \parencite{tarek2020dynamicppl},
which transforms valid Julia function definitions into a reusable representation (\jlinl{@model} is
a Julia macro; see section~\ref{sec:comp-metapr-julia} for more explanation).  The result is a new
function which produces instances of a structure of type \jlinl{Model}, which in turn will contain
the provided data, some metadata, and a nested function with the slightly changed original model
code. In the concrete case of the model in listing~\ref{lst:normal}, the resulting code would be
approximately equal to the code in listing~\ref{lst:normal-expanded}.  The purpose of this is the
following: the outer function, the \enquote{generator}, constructs an instance of the model for
given parameters~-- usually done once per inference problem, to fix the observations and
hyperparameters.  Subsequently, the \jlinl{sample} function can be applied to this instance with
different values for the sampling algorithm, which in turn will use the \jlinl{evaluator} function
of the instance to run the model with chosen \jlinl{sampler} and \jlinl{context} arguments, that are
passed to the \enquote{tilde functions}, to which the statements of the form \jlinl{expr ~ D} are
converted.

A special distinction is made for the tilde functions of variables that are based on the model's
arguments. \dppljl{} distinguishes between \emph{assumptions}, i.e., latent variables that should be
recovered through posterior inference, and \emph{observations}, that need to be provided when
instantiating the model and are conditioned upon.  The latter by default will only contribute to the
likelihood, instead of being sampled.  But in certain cases, such as in probability evaluation or
when using the complete model in a generative way, this behaviour can be different.  For this
purpose, the tilde functions for the variables \jlinl{x[i]} in listing~\ref{lst:normal-expanded} are
differentiated in a conditional statement.

Inside the tilde functions, the real stochastic work happens.  Depending on the \jlinl{sampler} and
the \jlinl{context}, values may be generated and stored in the \jlinl{varinfo} object, and the joint
log-likelihood incremented, as happens for most MCMC samplers.  In this case, one call to the
\jlinl{evaluator} corresponds to one sampling step.  In other situations, model evaluation serves
the purpose of density evaluation, in which no new values need to be produced; this use case is
needed for probability queries, or density-based algorithms (which might additionally use automatic
differentiation on the density evaluation procedure).  All shared information for external usage is
thereby conventionally stored in the \jlinl{varinfo} object, which resembles a dictionary from
variable names\footnote{These \jlinl{VarName} objects, constructed by the macro \jlinl{@varname},
  simply represent an indexed variable through a symbol and a tuple of integers.}  to values
(internal sampler state can also be stored in the \jlinl{sampler} object).  Through the
\jlinl{sample} interface, the resulting values are then stored in a \jlinl{Chains} object, a data
frame containing a value for each variable at each sampling step.

From the point of view of a sampling algorithm, all that it sees is a sequence of tilde statements,
consisting of a value, a variable name, and a distribution.  \turingjl{}, crucially, does not have a
representation of model structure.  This is sufficient for many kinds of inference algorithms that
it already implements~-- Metropolis-Hastings, several particle methods, HMC and NUTS, and
within-Gibbs combinations of these~-- but does not allow more intelligent usage of the available
information.  For example, to use a true, conditional, Gibbs sampler, the user has to calculate the
conditionals themselves.  Structure-based optimizations such as partial specializtion of a model to
save calculations, automatic conjugacy detection \parencite{hoffman2018autoconj}, or model
transformations such as Rao-Blackwellization \parencite{murray2017delayed} cannot be performed in
this representation.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Compilation and Metaprogramming in Julia}
\label{sec:comp-metapr-julia}

Julia \parencite{bezanson2017julia} is a programming language with a strong, dynamic type system
with nominal, parametric subtyping and elaborate multiple dispatch.  It uses LLVM
\parencite{llvmproject2019llvm} for JIT-compilation and while it is dynamically typed, a combination
of method specialization and type inference allows it to produce very optimized, fast machine code
\parencite{bezanson2018julia}.  The language is syntactically designed to bear a certain resemblance
to Matlab, Python, or Ruby, but contrary to them, it is its own compiler, not primarily the reliance
on third-party numerical libraries (e.g., BLAS and LAPACK), that is enabling C-like speed.

On top of that, the language is built on a very open compilation model.  Underlying the surface
syntax is an abstract syntax tree (AST), that is used internally to the compiler, but also exposed
to the programmer through macros, which allow to transform pieces of code at compile time.  These
macros resemble proper hygienic, LISP-style code transformations, not simple text-substitutions as C
preprocessor macros.  As an example, look at the following function that sums up the \jlinl{sin}
values of a list of numbers:
\begin{lstlisting}
function foo(x)
    y = zero(eltype(x))
    for i in eachindex(x)
        @show y += sin(x[i])
    end
    return y
end
\end{lstlisting}
The invocation of the standard libray macro \jlinl{@show} will be treated by the compiler, during
parsing, as a function call receiving as input the following data structure, representing \jlinl{y
  += sin(x[i])} in S-expression-like form:
\begin{lstlisting}
Expr(:(+=), :y, Expr(:call, :sin, Expr(:ref, :x, :i)))
\end{lstlisting}
In this particular case, the nested structure is not taken advantage of or transformed, but simply
converted to a string used to print the value of the expression, labelled by its form in the code:
\begin{lstlisting}
macro show(ex)
    blk = Expr(:block)
    unquoted = sprint(Base.show_unquoted, ex) * " = "
    assignment = Expr(:call, :repr, Expr(:(=), :value, esc(ex)))
    push!(blk.args, Expr(:call, :println, unquoted, assignment))
    push!(blk.args, :value)
    return blk
end
\end{lstlisting}
The result is then spliced back into the AST, which is compiled further as if it were written as
\begin{lstlisting}
function foo(x)
    y = zero(eltype(x))
    for i = eachindex(x)
        begin
            println("y += sin(x[i]) = ", repr(var"#1#value" = (y += sin(x[i]))))
            var"#1#value"
        end
    end
    return y
end
\end{lstlisting}
(Note the automatic conversion of the symbol \jlinl{:value} to a generated name \jlinl{#1#value}, in
order to not possibly shadow any variables from the calling scope.)


\begin{lstfloat}[t]
\begin{lstlisting}[style=lstfloat]
1: (%1::Core.Compiler.Const(foo, false), %2::Array{Float64,1})
  %3 = eltype(%2)::Compiler.Const(Float64, false)
  %4 = zero(%3)::Float64
  %5 = eachindex(%2)::Base.OneTo{Int64}
  %6 = iterate(%5):Union{Nothing, Tuple{Int64,Int64}}
  %7 = (%6 === nothing)::Bool
  %8 = not_int(%7)::Bool
  br §3 (%4) unless %8
  br §2 (%6, %4)
2: (%9, %10)
  %11 = getfield(%9, 1)::Int64
  %12 = getfield(%9, 2)::Int64
  %13 = getindex(%2, %11)::Float64
  %14 = sin(%13)::Float64
  %15 = (%10 + %14)::Float64
  %16 = repr(%15)::String
  %17 = println("y += sin(x[i]) = ", %16)
  %18 = iterate(%5, %12)::Union{Nothing, Tuple{Int64,Int64}}
  %19 = (%18 === nothing)::Bool
  %20 = not_int(%19)::Bool
  br §3 (%15) unless %20
  br §2 (%18, %15)
3: (%21)
  return %21
\end{lstlisting}
  \caption{SSA-form of the lowered form of the method \protect\jlinl{foo(::Vector\{Int\})} as defined
    defined above, annotated with inferred types (as through
    \protect\jlinl{\@code_warntype}).\label{lst:foo-inferred}}
\end{lstfloat}

After macro expansion, the code of the function is \emph{lowered} into an intermediate
representation consisting of only function calls and branches.  This comprises of sevaral
transformations: for one, certain syntactic constructs are \enquote{desugared} into primitive
function calls.  For example, array accesses, \jlinl{x[i]}, are replaced by calls to the library
function \jlinl{getindex(x, i)}.  The for loop in the example is converted into a while loop using
the \jlinl{iterate} library function:
\begin{lstlisting}
iterable = eachindex(x)
iter_result = iterate(iterable)
while !(iter_result === nothing)
    i, state = iter
    @show y += sin(x[i])
    iter_result = iterate(iterable, state)
end
\end{lstlisting}
Consequently, all nested expressions are split apart, so that only simple, unnested calls remain,
and any subsequent assignments to variables are linearized to a series of definitions, with newly
introduced names of the form \jlinl{\%i}.  The remaining control flow statements (e.g., while loops
and conditionals) are a represented through sequence of labelled code blocks with (possibly
conditional) jumps between them.  The sequence of assignments is further processed into \emph{single
  static assignment (SSA) form} \parencite{singer2018static}, the characteristic property of which
is that every variable is assigned exactly once, thus giving it a unique, position-independent name
to each intermediate value.  The result of the translation of our example into into three SSA blocks
can be found in listing~\ref{lst:foo-inferred}.

There is one complication with the SSA conversion: we need to be able to distinguish between
assignments of variables arising from \enquote{merged} control flow.  Consider the assignment of
\jlinl{y} in the following code example:
\begin{lstlisting}
x = f()
if !g()
    y = x - 1
else
    y = x + 1
end
h(y)
\end{lstlisting}
In the variant of SSA form used in this text and most of Julia, values of variables that are
assigned in multiple parent blocks are passed as \emph{block arguments}, as in
figure~\ref{fig:ssa-phi} on the right, and subsequently in this work.  The traditional, functionally
equivalent alternative is to introduce \emph{\(\phi\)-functions}, which are defined to distinguish
between several values depending on the control path taken before.  This form is shown in the same
figure on the left.

\begin{figure}[t]
  \centering
  \hfill
  \subbottom{\includegraphics[width=0.4\textwidth]{figures/ssa-phi}}
  \hfill
  \subbottom{\includegraphics[width=0.4\textwidth]{figures/ssa-args}}
  \hfill\null
  \caption{Correspondence between SSA representations using \(\phi\)-functions and block arguments.
    The SSA variables \protect\jlinl{\%3} and \protect\jlinl{\%4} correspond to the values of
    \protect\jlinl{y} in the two branches, which are merged in \protect\jlinl{\%5}.}
  \label{fig:ssa-phi}
\end{figure}

Note that until now, the operations involved were purely syntactic in nature, and could be perfomed
by solely taking into account the code of the function \jlinl{foo}.  As soon as \jlinl{foo} is
called on a concrete type, though, the most specific method fitting to the argument types will be
selected, and type inference on its body be applied.  If we go on and call \jlinl{foo([1.0])}, with
\jlinl{Vector\{Int\}} as the sole argument type, the types as annotated in the same listing will be
inferred.

\begin{lstfloat}[p]
  \begin{lstlisting}[style=lstfloat]
1 ── %1  = arraysize(x, 1)::Int64
│    %2  = slt_int(%1, 0)::Bool
│    %3  = ifelse(%2, 0, %1)::Int64
│    %4  = slt_int(%3, 1)::Bool
└───       goto §3 if not %4
2 ──       goto §4
3 ──       goto §4
4 ┄─ %8  = φ (§2 => true, §3 => false)::Bool
│    %9  = φ (§3 => 1)::Int64
│    %10 = φ (§3 => 1)::Int64
│    %11 = not_int(%8)::Bool
└───       goto §22 if not %11
5 ┄─ %13 = φ (§4 => 0.0, §21 => %18)::Float64
│    %14 = φ (§4 => %9, §21 => %42)::Int64
│    %15 = φ (§4 => %10, §21 => %43)::Int64
│    %16 = arrayref(true, x, %14)::Float64
│    %17 = invoke sin(%16::Float64)::Float64
│    %18 = add_float(%13, %17)::Float64
│    %19 = sle_int(1, 1)::Bool
└───       goto §7 if not %19
6 ── %21 = sle_int(1, 0)::Bool
└───       goto §8
7 ──       nothing::Nothing
8 ┄─ %24 = φ (§6 => %21, §7 => false)::Bool
└───       goto §10 if not %24
9 ──       invoke getindex(()::Tuple, 1::Int64)::Union{}
└───       $(Expr(:unreachable))::Union{}
10 ┄       goto §11
11 ─       goto §12
12 ─       goto §13
13 ─       goto §14
14 ─ %32 = invoke :(var"#sprint#339")(
             nothing::Nothing, 0::Int64, sprint::typeof(sprint), 
             show::Function, %18::Float64
           )::String
└───       goto §15
15 ─       goto §16
16 ─       goto §17
17 ─       invoke println("y += sin(x[i]) = "::String, %32::String)::Any
│    %37 = (%15 === %3)::Bool
└───       goto §19 if not %37
18 ─       goto §20
19 ─ %40 = add_int(%15, 1)::Int64
└───       goto §20
20 ┄ %42 = φ (§19 => %40)::Int64
│    %43 = φ (§19 => %40)::Int64
│    %44 = φ (§18 => true, §19 => false)::Bool
│    %45 = not_int(%44)::Bool
└───       goto §22 if not %45
21 ─       goto §5
22 ┄ %48 = φ (§20 => %18, §4 => 0.0)::Float64
└───       return %48
\end{lstlisting}
  \caption{Typed and optimized code of the call \protect\jlinl{foo([1.0])} in SSA form, as obtained
    through \protect\jlinl{@code_typed}.\label{lst:foo-typed}}
\end{lstfloat}

The last step of compilation within Julia consists of inlining and optimizing the typed intermediate
code, resulting in the form shown in listing~\ref{lst:foo-typed}.  There, several called methods
have been inlined, and concrete argument types to invoke been inferred.  This is in true,
traditional SSA form, with all variable slots eliminated, and block arguments converted to
\(\phi\)-functions.  Finally, this representation will be translated and sent to LLVM for
compilation, where further optimization can happen, and machine code will be generated and executed,
as well as stored for later usage as part of the just-in-time compilation mechanism.

% To track dynamic computation graphs, we perform a transformation of the IR implemented as a
% so-called generated function, which is Julia’s mechanism for staged programming (Rompf and Odersky,
% 2010; Bolewski, 2015). Such generated functions, instead of being directly translated into machine
% code, emit new IR to the compiler, which is then compiled


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Computation Graphs and Automatic Differentiation}
\label{sec:cg-ad}

% 1. AD in general
% 2. Zygote principles




%%% Local Variables: 
%%% TeX-master: "main"
%%% End: