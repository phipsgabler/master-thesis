\chapter{Background}
\label{cha:background}

This chapter provides the background for the concepts used later in
chapters~\ref{cha:impl-dynam-graph} and \ref{cha:graph-track-prob}.  Initially, it gives a quick
overview of Baysian inference and probabilistic programming in general, necessary to understand the
requirements and usual approaches of probabilistic programming systems.

Consequently, the machinery and language used to develop the graph tracking system forming the main
part of the work are described.  This consists firstly of a short introduction to graph tracking and
source-to-source automatic differentiation, which contain many ideas and terminology that will be
used later, and often provided inspiration.  Secondly, the basic notions and techniques of the Julia
compilation process as well as the language's metaprogramming capabilities are described, which form
the basis of the implementation.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bayesian Inference and MCMC methods}
\label{sec:bayes-infer}

Generative modelling is an approach for modelling phenomena based on the assumption that
observables can be fully described through some stochastic process.  When we assume this process to
belong to a specified family of processes, the estimation of the \enquote{best} process is a form of
learning: if we have a good description of how obserations are generated, we can make summary
statements about the whole population (descriptive statistics) or predictions about new
observations.  When observations come in pairs of independent and dependent variables, learning the
conditional model of one given the other solves a regression or classification problem.

Within a Baysian statistical framework, we assume that the family of processes used is specified by
random variables related through conditional distributions with densities, which describe how the
observables would be generated: some \emph{unobserved variables} are generated from \emph{prior
  distributions}, and the \emph{observed data} are generated conditionally on the unobserved
variables.  The goal is to learn the \emph{posterior distribution} of the parameters given the
observations, which is a sort of \enquote{inverse} of how the problem is specified.

As an example, consider image classification: if we assume that certain percentages of an image data
set picture cats and dogs, respectively, the distribution of these labels forms the prior.  Given
the information which kind of animal is depicted on it, an image can then be generated as a matrix
of pixels based on a distribution of images conditioned on labels.  The posterior distribution is
then conditional distribution of the label given an image.  When we have this information, we can,
for example, build a Baysian classifier, by returning for a newly observed image that label which
has the highest probability under the posterior.

This kind of learning is called Bayesian inference since, in the form of densities, the form of the
model can be expressed using Bayes' theorem as the conditional distribution with
density\footnote{Note the abuse of notation regarding \(\prob{\cdot}\); see
  page~\pageref{cha:notation} on notation.}
\begin{equation}
  \label{eq:bayes}
  \overbrace{\prob{\theta \given x}}^{\text{posterior}} =
  \frac{\overbrace{\prob{x \given \theta}}^{\text{likelihood}}\;
    \overbrace{\prob{\theta}}^{\text{prior}}}{\prob{x}},
\end{equation}
where \(x\) are the observed data, and \(\theta\) are the unobserved parameters. The posterior
represents the distribution of the unobserved variables as a combination of the prior belief updated
by what has been observed~\parencite{congdon2006bayesian}.  (In practice, not all of the unobserved
variables have to be model parameters we are actually interested in; these can be integrated out).

Going beyond simple applications like the classifier mentioned above, handling the posterior gets
difficult, though.  Simply evaluating the posterior density
\(\theta \mapsto \prob{\theta \given x}\) at single points is not enough in a Baysian setting for
usages such as prediction, parameter estimation, or evaluation of probabilities of continuous
variables.  The problem is that almost all of the relevant quantities depend on some sort of
expectation over the posterior density, an integral of the form
\begin{equation}
  \label{eq:posterior-expectation}
  \Exp{f(\Theta) \given X = x} = \int f(\theta) \prob{\theta \given x} \dif \mu(\theta),
\end{equation}
for some measurable function \(f\) (with the base measure \(\mu\) depending on the type of
\(\Theta\)). This in turn involves calculating the normalizing marginal
\begin{equation}
  \label{eq:normalizing}
  \prob{x} = \int \prob{x, \tilde{\theta}} \dif \mu(\tilde{\theta}).
\end{equation}
in equation~\ref{eq:bayes}, often called the \enquote{evidence}.

When the distributions involved form a sufficiently \enquote{nice} combination, e.g., a conjugate
pair \parencites[see][chapter 2.2.2]{marin2007bayesian}[chapter
9.2.5]{murphy2012machine}, the integration can be performed analytically, since the posterior
density has a closed form for a certain known distribution, or at least is a known integral.  In
general, however, this is not tractable, not even by standard numerical integration methods, and
approximations have to be made.  Even for discrete variables, the applicability of simple summation
is limited by combinatorial explosion.

\newthought{Different techni{q}ues} for posterior approximation are available: among them are
distribution-based approaches for general graphical models, such as variational inference
\parencite[chapter 21 and 22]{murphy2012machine} and other methods generalized under the framework
of message passing \parencite{minka2005divergence}.  The methods described in this thesis, however,
fall into the category of Monte Carlo methods, and are based on sampling \parencites[chapter
23]{murphy2012machine}{vihola2020lectures}.  Their fundamental idea is to derive, for a specified
density of \(\Theta \from \pi\), a sampling procedure with a consistent estimator for expectations:
\begin{equation}
  \label{eq:mc-methods}
  \kth{I}(f) \to \Exp{f(\Theta)}, \quad \text{as} \quad k
  \to \infty
\end{equation}
in some appropriate stochastic convergence (usually convergence in probability is enough).  We leave
out the conditional dependency on \(X\) here for simplicity in notation, and since the data are
usually fixed in inference problems.

Examples of such methods are rejection sampling, importance sampling, and particle filters.  Many
Monte Carlo methods are defined in a form that directly samples a sequence of individual random
variables \(\sequence{\kth{Y}}\), called a \emph{chain}, for which the estimator is given by the
arithmetic mean, such that a law of large numbers (LLN) holds:
\begin{equation}
  \label{eq:mc-lln}
  \kth{I}(f) = \frac{1}{n} \sum_{i=1}^{n} f(\kth{Y}) \to \Exp{f(\Theta)}
\end{equation}
When we can sample \(\kth{Y} \from \pi\) exactly, they are \iid{} and the LLN holds trivially; such
samplers exist, but might also be difficult to derive or not possess good enough convergence
properties (especially in high dimensions).  Another large class of samplers is formed by
\emph{Markov Chain Monte Carlo} (MCMC) methods, which, instead of sampling exactly from the density,
define \(\kth{Y}\) via a Markov chain:
\begin{equation}
  \label{eq:mc-kernel}
  \begin{aligned}
    &\Prob{\kth[k+1]{Y} \in \dif\zeta
      \given \kth{Y} = \kth{y}, \ldots, \kth[1]{Y} = \kth[1]{y}} \\
    &\quad = \Prob{\kth[k+1]{Y} \in \dif\zeta \given \kth{Y} = \kth{y}}  \\
    &\quad = K(\zeta \given \kth{y}) \dif\mu(\zeta)
  \end{aligned}
\end{equation}
for all \(k \ge 1\).  By constructing the \emph{transition kernel}, \(K\), in the right way, the
resulting chain is ergodic with the target density \(\pi\) as the unique stationary distribution,
i.e.,
\begin{equation}
  \label{eq:markov-chain}
  \int \pi(\zeta) K(\theta \given \zeta) \dif\mu(\zeta) = \pi(\theta)
\end{equation}
(which for discrete spaces is usually written in matrix form as a left eigenvalue equation on a
stochastic matrix: \(\pi K = \pi\)).  The advantage of MCMC methods is that they apply equally well
to many structurally complex models, and treat densities in a uniform way, without requiring special
knowledge about the specific distribution in question.  I refer to \textcites[chapter
6]{vihola2020lectures}{robert1999monte}[chapters 24 and following]{murphy2012machine} for an
introductions to MCMC theory and practice.

Frequently, MCMC methods use variations of the \emph{Metropolis-Hastings algorithm} (MH), which
replace the general definition of the transitions kernel by two helper fuctions: a proposal
distribution, given by a conditional density \(q\) that needs to be easy to sample from, and an
acceptance rate \(\alpha\).  Subsequent samples are then produced by proposing values from \(q\)
given the previous element of the cahin, and incorporating them into the chain with a probability
given through \(\alpha\) (see algorithm~\ref{alg:mh}).\todo{nicer algorithm formatting} There exist
many MH-based schemes with different properties and requirements: from the classical random-walk
Metropolis algorithm with Gaussian proposals, over Reversible Jump MCMC for varying dimensions
\parencite{green1995reversible}, to gradient-informed methods like Metropolis Adjusted Langevin and
Hamiltonian Monte Carlo (HMC) \parencite{betancourt2018conceptual,girolami2011riemann}.

\begin{algorithm}
  \begin{myalgorithmic}
  \item Start from an arbitrary \(\kth[1]{Y} = \kth[1]{y}\) with \(\pi(\kth{y}) > 0\).
  \item For each \(k \ge 1\):
    \begin{enumerate}
    \item Sample a proposal \(\kth[k]{\hat{Y}} \from q(\kth[k-1]{Y}, \cdot)\).
    \item With probability \(\alpha(\kth[k]{\hat{Y}}, \kth[k-1]{Y})\), set
      \(\kth[k]{Y} = \kth[k]{\hat{Y}}\); else, keep \(\kth[k]{Y} = \kth[k-1]{Y}\).
    \end{enumerate}
  \end{myalgorithmic}
  \caption{General scheme for the Metropolis-Hastings algorithm.\label{alg:mh}}
\end{algorithm}

Still, when we have a multi-component structure \(\Theta = [\Theta_1, \ldots, \Theta_N]\), a good
transition kernel can be hard to find (especially manually).  One way to break down the problem is
to use a family of componentwise updates, given by conditional kernels \(q_{i}\) operating on only
one component of \(\Theta\), with the others fixed:
\begin{equation}
  \label{eq:conditional-kernels}
  \begin{aligned}
    \kth[k]{\hat{Y}}_{-i} &= \kth[k-1]{Y}_{-i} \\
    \kth[k]{\hat{Y}}_{i} &\from q_{i}(\kth[k-1]{Y}_{i}, \cdot \given \kth[k-1]{Y}_{-i})
  \end{aligned}
\end{equation}
The components can be scalar or multivariate blocks, and the kernel may itself be any valid
transition kernel \parencite[chapter 6.6]{vihola2020lectures}.  This allows one to freely mix
different MCMC methods suitable for each variable in a problem.

This so-called \enquote{within-Gibbs} sampler bears its name because it is a generalization of the
classical \emph{Gibbs sampling} algorithm: often, the simplest available set of transition kernels
is given by the conditional densities \(\Theta_{i} \mapsto p(\Theta_{i} \given \Theta_{-i},
x)\). They can directly be used as component proposals for a within-Gibbs sampler, leading to a
cancelling acceptance rate of \(\alpha \equiv 1\).  This approach has the advantage that it is in
many models it is rather easy to derive, even manually, from a given joint density; examples are
extensively covered in \textcite[chapter 24.2]{murphy2012machine}.

\begin{equation}
  \label{eq:normal-mixture-1}
  \begin{aligned}
    \mu_{k} &\iidfrom \Normal(m, s) \quad\text{for } 1 \le k \le K, \\
    Z_{n} &\iidfrom \distr{Categorical}(K) \quad\text{for } 1 \le n \le N, \\
    X_{n} &\iidfrom \Normal(\mu_{Z_{n}}, \sigma) \quad\text{for } 1 \le n \le N
  \end{aligned}
\end{equation}

% \begin{align*}
%   &q(z_{n} \given z_{-n}, \mu_{1:K}, x_{1:N}) \\
%   &\quad = p(z_{n}) p(x_{n} | \mu_{z_{n}}) / Z \\
%   &\quad = \dfrac{\distr{Categorical}(z_{n}) \, \Normal(x_{n} \given \mu_{z_{n}}, \sigma)}{
%     \sum_{\zeta \in \supp(Z_{n})} \distr{Categorical}(\zeta)\, \Normal(x_{n} \given \mu_{\zeta},
%     \sigma)} \\
%   &\quad = \dfrac{\distr{Categorical}(z_{n}) \, \Normal(x_{n} \given \mu_{z_{n}}, \sigma)}{
%     \sum_{\zeta = 1}^{K} \distr{Categorical}(\zeta)\, \Normal(x_{n} \given \mu_{\zeta}, \sigma)}
% \end{align*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Probabilistic Programming}
\label{sec:prob-prog}

Probabilistic programming is a means of describing generative models through the syntax of a
programming language.  It makes sense to consider probabilistic programs not only as syntactic sugar
for denoting a function that calculates a joint probability density over some set of variables, but
as structured objects in their own right: they open up possibilities that \enquote{black box}
density functions cannot automatically provide. In more concise terms from
\textcite{vandemeent2018introduction}:
\begin{quote}
  Probabilistic programming is largely about designing languages, interpreters, and compilers that
  translate inference problems denoted in programming language syntax into formal mathematical
  objects that allow and accommodate generic probabilistic inference, particularly Bayesian
  inference and conditioning.
\end{quote}

A probabilistic program differs from a regular program with stochastic values through the
possibility of being conditioned on: some of the internal variables can be fixed to observed
values. As such, the program denotes on the one hand a joint distribution, that can be \emph{forward
  sampled} from by simply running the program top to bottom and calling a pseudo-random functions.
But at the same time, it also represents a conditional distribution, given as an unnormalized
conditional density, which together with an inference algorithm can also be sampled from.  Consider
the model~\eqref{eq:normal-mixture-1} from above: its mathematical description might be translated
into a program in \turingjl{} syntax as
\begin{lstlisting}
@model function normal_mixture(x, K, m, s, σ)
    N = length(x)

    μ = Vector{Float64}(undef, K)
    for k = 1:K
        μ[k] ~ Normal(m, s)
    end

    z = Vector{Int}(undef, N)
    for n = 1:N
        z[n] ~ Categorical(K)
    end

    for n = 1:N
        x[n] ~ Normal(μ[z[n]], σ)
    end

    return x
end
\end{lstlisting}
We can then \emph{query} the model in several ways:
\begin{lstlisting}
julia> m = normal_mixture(x_observations, K, m, s, σ);
julia> forward = sample(m, Prior(), 10);
julia> chain = sample(m, MH(), 1000);
\end{lstlisting}
The value of \jlinl{forward} will be an dataframe-like object containing 10 values for each variable
sampled from the forward (i.e., joint) distribution, matching the size of \jlinl{x_observations}.
Similarly, \jlinl{chain} will contain a length 1000 sample from a Markov chain targetting the
posterior, created using the MH algorithm.  If we were to write these two functionalities manually,
in idiomatic Julia, we would end up with at least the following two functions:
\begin{lstlisting}
function normal_mixture_sampler(N, K, m, s, σ)
    μ = rand(Normal(m, s), K)
    z = rand(Categorical(K), N)
    x = rand.(Normal.(μ[z], s))
    return μ, z, x
end

function normal_mixture_logpdf(μ, z, x, K, m, s, σ)
    N = length(x)
    ℓ = 0.0
    ℓ += sum(logpdf(Normal(m, s), μ[k]) for k = 1:K)
    ℓ += sum(logpdf(Categorical(K), z[n]) for n = 1:N)
    ℓ += sum(logpdf(Normal(μ[z[n]]), x[n]) for n = 1:N)
    return ℓ
end
\end{lstlisting}
And still, with these, there would be no flexible interface for sampling algorithms to automatically
detect all latent and observed variables, put them all into a dataframe with their names, etc.

While probabilistic programming languages (PPLs) are often implemented as separate, domain-specific
languages (DSLs), they can also be embedded into \enquote{host} programming languages with
sufficient syntactic flexibility.  The latter is advantageous if one wants to use regular
general-purpose programming constructs or interact with other functionalities of the host language.

There are a variety of further reasons why one would rather describe an inference problem in terms
of a program than in more \enquote{mathematical} form, like a graph or likelihood function.  In a
good DSL, models will read as close to textbook model specifications as possible, while allowing to
use the host language to express, for example:
\begin{itemize}
  \firmlist
\item Recursive relationships
\item Usage of imperative constructs, such as loops, or mutable intermediate computations for
  efficiency
\item Manual manipulations, e.g. for memoization, scaling (-Inf), or preliminary termination
\item Distributions over complex custom data structures, e.g. trees
\item Inference involving complex transformations from other domains, for which implementations
  already exist, e.g. neural networks or differential equation solvers
\item Inference that integrates calls to very complex external systems, e.g. simulators or renderers
\end{itemize}
\todo{references for examples}

In this sense, a probabilistic programming syntax defines a common format for model description,
more general.

See \textcite{vandemeent2018introduction} for a general introduction into the implementation of
PPLs. \Textcite{goodman2014design} gives an in-depth overview of the implementation and usage of one
specific, continuation-based implementation called WebPPL.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Compilation and Metaprogramming in Julia}
\label{sec:comp-metapr-julia}

\textcite{singer2018introduction}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Computation Graphs and Automatic Differentiation}
\label{sec:graph-track-autom}

% 1. AD in general
% 2. Zygote principles



%%% Local Variables: 
%%% TeX-master: "main"
%%% End: