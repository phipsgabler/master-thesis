\chapter{Background}
\label{cha:background}

This section provides the background for the concepts used later in
chapters~\ref{cha:impl-dynam-graph} and \ref{cha:graph-track-prob}.  Initially, it gives a quick
overview of Baysian inference and probabilistic programming in general, necessary to understand the
requirements and usual approaches of probabilitic programming systems.

Consequently, the machinery and language used to develop the graph tracking system forming the main
part of the work are described.  This consists firstly of a short introduction to graph tracking and
source-to-source automatic differentiation, which contain many ideas and terminology that will be
used later, and often provided inspiration.  Secondly, the basic notions and techniques of the Julia
compilation process as well as the language's metaprogramming capabilities are described, which form
the basis of the implementation.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bayesian Inference and Probabilistic Programming}
\label{sec:bayes-infer-prob}

Probabilistic modelling is a technique for modelling phenomena based on the assumption that
observables can be fully described through some stochastic process.  When we assume this process to
belong to a specified family of processes, the estimation of the \enquote{best} process is a form of
learning: if we have a good description of how obserations are generated, we can make summary
statements about the whole population (descriptive statistics) or predictions about new
observations.  When observations come in pairs of independent and dependent variables, learning the
conditional model of one given the other solves a regression or classification problem.

Within a Baysian statistical framework, we assume that the family of processes used is specified by
random variables related through conditional distributions with densities, which describe how the
observables would be generated: some \emph{latent variables} are generated from \emph{prior
  distributions}, and the observed \emph{data} are generated conditionally on the latent variables.
The goal is to learn the \emph{posterior distribution} of the parameters given the observations,
which is the inverse of how the problem is specified.

As an example, consider image classification: if we assume that certain percentages of an image data
set picture cats and dogs, respectively, the distribution of these labels forms the prior.  Given
the information which kind of animal is depicted on it, an image can then be generated as a matrix
of pixels based on a distribution of images conditioned on labels.  The posterior distribution is
then the ``inverse'' distribution of the label given an image.  When we have this information, we
can, for example, build a Baysian classifier, by returning for a newly observed image that label
which has the higher probability under the posterior.

This kind of learning is called Bayesian inference since, in the form of densities, the form of the
model can be expressed using Bayes' theorem:
\begin{equation}
  \label{eq:bayes}
  \prob{\theta \given x} = \frac{\prob{x \given \theta}\; \prob{\theta}}{\prob{x}},
\end{equation}
where \(x\) are the observed data, and \(\theta\) are the parameters.\footnote{Note the abuse of
  notation regarding the name of \(\prob{\cdot}\); see page~\pageref{cha:notation} on notation.}  (In
practice, not all of the latent variables have to parameters we are actually interested in; these
can be integrated out).

Going beyond simple applications like the classifier mentioned above, handling the posterior gets
difficult, though.  Simply evaluating the posterior density
\(\theta \mapsto \prob{\theta \given x}\) at single points is not enough for usages such as
parameter estimation with continuous parameters, or even simple probability evaluation; and even for
discrete variables, combinatorial explosion limits the usability of the simple summation approach to
calculate probabilities.  The reason is that almost all of the relevant quantities depend on some
sort of expectation over the posterior density, which is an integral:
\begin{equation}
  \label{eq:posterior-expectation}
  \Exp{f(\Theta) \given X = x} = \int f(\theta) \prob{\theta \given x} \dif \theta,
\end{equation}
with the base measure depending on the type of \(\Theta\), or equivalently the fact that it is hard
to find the normalizing marginal
\begin{equation}
  \label{eq:normalizing}
  \prob{x} = \int \prob{x, \tilde{\theta}} \dif \tilde{\theta}.
\end{equation}
When the distributions involved form a sufficiently \enquote{nice} combination, e.g., a conjugate
pair \todo{ref}, the integrals can be calculated analytically since the posterior density has a
closed form for a certain known distribution, or at least is a known integral.  In general, however,
this is not possible, and approximations have to be made, for which different approaches are
available: for example, expectation maximization for models with latent variables\todo{ref} or
message passing and variational methods for general graphical models\todo{ref}.

The applications described in this thesis fall into another framework for approximating posterior
information: Monte Carlo methods, which derive from a specified density a consistent estimator for
the expected value:
\begin{equation}
  \label{eq:mc-methods}
  \kth[n]{I}(f) \to \Exp{f(\Theta) \given X = x}, \quad \text{as} \quad n
  \to \infty
\end{equation}
in some appropriate stochastic convergence (usually convergence in probability is enough).  Most of
these methods are defined in a form that samples individual random variables, for which a law of
large numbers (LLN) holds:
\begin{equation}
  \label{eq:mc-lln}
  \kth[n]{I}(f) = \frac{1}{n} \sum_{i=1}^{n} f(\kth[i]{\Theta}) \to \Exp{f(\Theta) \given X = x}
\end{equation}
When we can sample \(\Theta_{i} \sim \prob{\cdot \given x}\) exactly, the \(\Theta_{i}\) are \iid{}
and the LLN holds trivially; such samplers exist, but might also be difficult to derive or not
possess good enough convergence properties.  Another large class of samplers is formed by
\emph{Markov Chain Monte Carlo} (MCMC) methods, which, instead of sampling exactly from the density,
define \(\kth[i]{\Theta}\) via a Markov chain whose stationary distribution is the target density.
By choosing the transition the right way, the resolving Markov chain is ergodic with the target
density as the unique stationary distribution (this is the inverse to the problem of finding a
stationary distribution).  The advantage of MCMC methods is that they mostly treat the density as a
black box, without requiring any more formal manipulations on its structure.

A frequent scheme for MCMC methods is the Metropolis-Hastings algorithm, consisting of the
definition of a transition kernel by means of two helper fuctions: a proposal distribution with
density \(q\), and an acceptance rate \(\alpha\), both depending on the old value in the
chain:\todo{make an algorithm}
\begin{enumerate}
\item Start from an arbitrary \(\kth[1]{\Theta}\).
\item For each \(i \ge 1\):
  \begin{enumerate}
  \item Propose \(\kth[i]{\hat{\Theta}} \sim q(\cdot \given \kth[i-1]{\Theta})\).
  \item With probability \(\alpha(\kth[i]{\hat{\Theta}}, \kth[i-1]{\Theta})\), set
    \(\kth[i]{\Theta} = \kth[i]{\hat{\Theta}}\); else, keep \(\kth[i]{\Theta} = \kth[i-1]{\Theta}\).
  \end{enumerate}
\end{enumerate}

There exist many such schemes with different properties and requirements: the classical Metropolis
algorithm with Gaussian proposals; Reversible Jump MCMC; gradient-informed methods like Metropolis
Adjusted Langevin or Hamiltonian Monte Carlo (HMC); and Gibbs sampling.

% when we assume that \(\Theta = [\Theta_1, \ldots, \Theta_N]\),

The within-Gibbs sampler is a generalization of the classical Gibbs sampling algorithm: the
conditional densities \(\theta_{i} \mapsto p(\theta_{i} \given \theta_{-i}, x)\) can directly be
used as proposals for a within-Gibbs sampler, leading to an acceptance probability of
\(\alpha \equiv 1\).  This sampler has the advantage that it is in many cases rather easy to derive,
even manually, from a given model specification.

\subsection{Probabilistic Programming}
\label{sec:prob-prog}

Probabilistic programming is a means of describing probabilistic models through the syntax of a
programming language. Probabilistic programms distinguish themselves from normal programs by the
possibility of being sampled from conditionally, with some of the internal variables fixed to
observed values.  While probabilistic programming systems are often implemented as separate,
domain-specific languages, they can also be embedded into \enquote{host} programming languages with
sufficient syntactic flexibility.  The latter is advantageous if one wants to use regular
general-purpose programming constructs or interact with other functionalities of the host language.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Computation Graphs and Automatic Differentiation}
\label{sec:graph-track-autom}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Metaprogramming and Compilation in Julia}
\label{sec:metapr-comp-julia}




%%% Local Variables: 
%%% TeX-master: "main"
%%% End: