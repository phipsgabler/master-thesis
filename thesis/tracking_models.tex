\chapter{Graph Tracking in Probabilistic Models}
\label{cha:graph-track-prob}

The system described in chapter~\ref{cha:impl-dynam-graph}, implemented in a Julia package
\irtrackerjl{}, can now be utilized for the analysis of probabilistic models written in \dppljl{},
and for posterior inference in \turingjl{}.  This part of the work is realized in another package,
\autogibbsjl{}, which is available as open-source
code\footnote{\url{https://github.com/phipsgabler/AutoGibbs.jl}}.  There are two applications
provided, built on top of the graph tracking functionality: first, dependency analysis of random
variables in a model can be performed.  This results in the complete graphical model for static
models, and a slice of it for dynamic models.  The resulting graph can be plotted for visualization.
Second, given the dependency graph, the conditional likelihoods of unobserved variables in static
models can be extracted.  With these, analytic Gibbs conditionals can be derived and used in
\turingjl{}'s within-Gibbs sampler.

\section{Dependency Analysis in Dynamic Models}
\label{sec:dependency-analysis}

In order to use \irtrackerjl{} to extract the dependencies in a probabilistic model written in
\dppljl{}, we need to remember the structure of such models, which was introduced in
section~\ref{sec:prob-prog}: there is one evaluator function, into which the original code is
transformed, and which evaluates the model in different modes.  This function has the same structure
as the original code, but adds some more complicated book-keeping logic to it, and transforms the
tilde statements into function calls with some additional metadata.  Furthermore, when calling the
model as a callable object, there are several layers of dispatch (about five layers of nesting,
depending on the arguments), until the real evaluator function is actually hit.  On the other hand,
there is no further nesting involved beyond the evaluator function~-- \turingjl{} simply does not
support nested models, for technical reasons.

Therefore, we at first need to introduce an \irtrackerjl{} context that will record all the internal
function calls down to the evaluator function, and stop there.  Similar to the
\jlinl{DepthLimitContext} demonstrated on page~\pageref{lst:depthlimitcontext}, the main task here
is to overload the \jlinl{canrecur} method to stop at the right call.  This can easily be done by
introducing a helper predicate function \jlinl{ismodelcall} that dispatches on the involved types.
Next, we notice that the resulting computation graph consists of a nested and quite unusable
structure, due to the initial levels of nesting.  To work with the model code, we need to strip the
outer layers off the inner node containing the trace of the evaluator function.  Thirdly, many of
the statements in the trace of the evaluator function do not have relevance for dependency
analysis~-- like those that stem from internal calculations done by the model, or statements that
were written by the user but to not lie on the dependency graph, such as debugging statements or the
lowered code of for loops, in some cases.  These we can strip off in advance, so as to clean the raw
dependency trace.  These three preparation steps are put together in one method:
\begin{lstlisting}
function slicedependencies(model::Model{F}, args...) where {F}
    trace = trackmodel(model, args...)
    strip = strip_model_layers(F, trace)
    slice = strip_dependencies(strip)
    return slice
end
\end{lstlisting}
Here, \jlinl{trackmodel} extracts the computation graph with the context for models tracking,
\jlinl{strip_model_layers} removes the outer method calls, and \jlinl{strip_dependencies} removes
all SSA code that is not on the dependecy graph spanned by the sampling statements.

The final and most intricate step is to add all the remaining SSA statements to a new graph
structure, that describes a more domain-specific representation.  In this \jlinl{Graph} type, only
assumption, observation, call, and constant nodes remain, containing relevant metadata such as their
values, variable names, and distribution objects.  In addition, the object stores intermediate
information used during its construction, such as the mapping between newly generated and original
references.  The graph construction is implemented in a function \jlinl{makegraph}, and we finally
have one exported function
\begin{lstlisting}
function trackdependencies(model, args...)
    slice = slicedependencies(model, args...)
    return makegraph(slice)
end
\end{lstlisting}
There are two complications regarding \jlinl{makegraph}.  For one, model arguments are handled
specially by \dppljl{}~-- there are some internal arguments added, and the original arguments are
inspected to allow to run the same model in generative or posterior mode.  This part needs to be
sorted out, so that the passed argument values are correctly set up as constants in the dependency
graph, but since all information is present, the task is resolved by correctly identifying the
arguments and restructuring their contents into the right form.

The other problem is the handling of mutation, and tracking of modified array elements.  For
example, a hidden Markov model might contain code like this:
\begin{lstlisting}
s = zeros(Int, N)
s[1] ~ Categorical(K)
for i = 2:N
    s[i] ~ Categorical(T[s[i-1]])
end
\end{lstlisting}
In order to express the dependency between successive elements of \jlinl{s}, an empty array is first
set up, and then subsequently populated by the results of the tilde statements describing the Markov
process.  In this form, only the individual variables \jlinl{s[i]} are recognized by the model
language.  Internally, the tilde statements are translated to array assignments of the form
\jlinl{s[i] = tilde_assume(...)}, but with additional lowering of the involved arguments, after
which the corresponding IR will look approximately like this:
\begin{lstlisting}
%9 = %i - 1                 # i - 1
%10 = getindex(%s, %9)      # s[i - 1]
%11 = getindex(%T, %10)     # T[s[i - 1]]
%12 = VarInfo{:s}(((%i,),))
%13 = Categorical(%11)
%14 = tilde_assume(..., %13, ..., %12, ...)
%15 = setindex!(%a, %14)
\end{lstlisting}
(to be understood symbolically, not as real SSA~-- several statements have been collapsed).  We see
that the direct association between the variable \jlinl{s} is not preserved in the line of the tilde
method, but spread over multiple statements. Even worse, since all statements for the different
\jlinl{s[i]} result in mutations of \jlinl{\%a}, the immediate dependency between \jlinl{s[i]} and
\jlinl{s[i-1]} is not available as a data dependency, but must be recovered dynamically.

The \jlinl{makegraph} implementation solves this by successively identifying mutated arrays
representing random variables by inspecting the indexing calls around tilde statements, and storing
the association between the assumption and the array elements.  This part of the procedure is the
most intricate one, and not complete; there may exist cases where mutation is able to
\enquote{circumvent} the dependecy analysis.  Additionally, the matching between indexing arguments
involves some careful treatment of variable names; the existing \dppljl{} API for this functionality
is not very comprehensive.  Due to this, the current implementation of \autogibbsjl{} currently only
supports \enquote{simple} indexing by one tuple of integers.  Other, more general indexing styles
allowed in Julia could be added in future extensions.  Furthermore, broadcasting tilde statements,
that are supported in \dppljl{}, are not supported by \autogibbsjl{} either.

\newthought{As an example} for the resulting graphs, take the two simple models in
listing~\ref{lst:dependency-examples}.  The pretty-printed dependency \jlinl{Graph}s of them are
shown in listing~\ref{lst:trace-examples} below.  We can see that the model arguments for
observations occur as constant values, and all of the intermediate transformation visible in the
original model definitions are observed.  From this structure, \autogibbsjl{} can construct output
in the Dot graph format and vizualized using GraphViz \parencite{gansner2000open}.  The visual
outputs of the example models is shown in figure~\ref{fig:geom-deps}.

\begin{lstfloat}[p]
\begin{lstlisting}[style=lstfloat]
@model function bernoulli_mixture(x)
    w ~ Dirichlet(2, 1/2)
    p ~ DiscreteNonParametric([0.3, 0.7], w)
    x ~ Bernoulli(p)
end

@model function hierarchical_gaussian(x)
    λ ~ Gamma(2.0, inv(3.0))
    m ~ Normal(0, sqrt(1 / λ))
    x ~ Normal(m, sqrt(1 / λ))
end
\end{lstlisting}
  \caption{Two simple example models: a mixture of two Bernoulli random variables with fixed
    probabilities, and a Gaussian model with conjugate prior.  Both models are defined over one
    single observation.}
  \label{lst:dependency-examples}
\end{lstfloat}

\newsavebox{\bernoullitrace}
\begin{lrbox}{\bernoullitrace}
\begin{lstlisting}[style=lstfloat]
⟨2⟩ = false
⟨3⟩ = Dirichlet(2, 0.5) → Dirichlet{Float64}(alpha=[0.5, 0.5])
⟨4⟩ = w ~ ⟨3⟩ → [0.826304431175434, 0.17369556882456608]
⟨5⟩ = DiscreteNonParametric([0.3, 0.7], ⟨4⟩) → DiscreteNonParametric{...}(
        support=[0.3, 0.7], p=[0.826304431175434, 0.17369556882456608])
⟨6⟩ = p ~ ⟨5⟩ → 0.3
⟨7⟩ = Bernoulli(⟨6⟩) → Bernoulli{Float64}(p=0.3)
⟨8⟩ = x ~ ⟨7⟩ ← ⟨2⟩
\end{lstlisting}
\end{lrbox}
\newsavebox{\gaussiantrace}
\begin{lrbox}{\gaussiantrace}
\begin{lstlisting}[style=lstfloat]
⟨2⟩ = 1.4
⟨3⟩ = Gamma(2.0, 0.3333333333333333) → Gamma{Float64}(
        α=2.0, θ=0.3333333333333333)
⟨4⟩ = λ ~ ⟨3⟩ → 0.9257859525673857
⟨5⟩ = /(1, ⟨4⟩) → 1.0801632896100921
⟨6⟩ = sqrt(⟨5⟩) → 1.0393090443222806
⟨7⟩ = Normal(0, ⟨6⟩) → Normal{Float64}(μ=0.0, σ=1.0393090443222806)
⟨8⟩ = m ~ ⟨7⟩ → 1.8505166567138398
⟨9⟩ = /(1, ⟨4⟩) → 1.0801632896100921
⟨10⟩ = sqrt(⟨9⟩) → 1.0393090443222806
⟨11⟩ = Normal(⟨8⟩, ⟨10⟩) → Normal{Float64}(
         μ=1.8505166567138398, σ=1.0393090443222806)
⟨12⟩ = x ~ ⟨11⟩ ← ⟨2⟩
\end{lstlisting}
\end{lrbox}
\begin{lstfloat}[p]
  \loosesubcaptions
  \subbottom[Trace of \texttt{bernoulli\_mixture(false)} (some type parameters not shown).]{%
    \usebox{\bernoullitrace}}
  \subbottom[Trace of \texttt{hierarchical\_gaussian(1.4)}.]{%
    \usebox{\gaussiantrace}}
  \caption{Traced structure of the two example models introduced above.  Values in \(\langle\)angle
    brackets\(\rangle\) denote intermediate values (similar to SSA variables), and right arrows
    denote the resulting values of function calls.  The left arrow indicates the source of the
    observed value.}
  \label{lst:trace-examples}
\end{lstfloat}

\FloatBlock

\begin{figure}[p]
  \centering
  \subbottom[][\texttt{bernoulli\_mixture(false)}]{%
    \includegraphics[width=0.49\textwidth]{figures/bernoulli_dependencies}}
  \subbottom[][\texttt{hierarchical\_gaussian(1.4)}]{%
    \includegraphics[width=0.49\textwidth]{figures/gaussian_dependencies}}
  \caption{Dependency graphs of the models in listing~\ref{lst:dependency-examples}, generated by
    \autogibbsjl{} and rendered by GraphViz.  More information, such as node values, is stored in
    the real model graph, but not printed for better readability.  Circular nodes denote tilde
    statements, while deterministic intermediate values, corresponding to normal SSA statements, are
    written in rectangles.}
  \label{fig:geom-deps}
\end{figure}


\section{Automatic Calculation of Gibbs Conditionals}
\label{sec:automatic-conditionals}

The ultimate contribution of this work is to utilize the dependency extraction system to extend
\turingjl{} with JAGS-style automatic calculation of Gibbs Conditionals.  In JAGS (and its sibling,
BUGS) conditional extraction works over a wide range of variable types \parencite{plummer2003jags}
by symbolic analysis and recognition of several patterns (e.g., conjugate distributions from
exponential families, log-concave or compactly supported distributions; see
\textcite{lunn2000winbugs}.), which is possible since the class of models is constrained by the
modelling language, and available in completely structured form.

In \turingjl{}, models are much less restricted, and the symbolic form has to recovered from
outside, as we have seen.  To focus on the principal ideas and not to extend the scope too much, the
implementation described in this section was restricted to finite, discrete conditionals, which are
trivial to sample from, given the respective log-density.  Since the construction of conditional
log-densities is independent from the normalization step, though, this can serve as a starting point
for further, more general conditional samplers, as those in JAGS and BUGS.  Additionally, and this
is a more fundamental limitation, the models to which the extraction algorithm can be applied must
be static in a specific sense: the whole Markov blanket of the variable in question must be unique,
and reachedable within one run of model tracking.  A large fraction of the models used in practise
do fulfill this condition, though.  As this problem is difficult so solve in general, the same
constraint applies to JAGS and BUGS, which makes \autogibbsjl{} not more limited than these.

The implementation of the conditional extraction system involves three main parts:
\begin{enumerate}
  \firmlist
\item Extracting the symbolic form of the conditional likelihood of Markov blankets in a given
  dependency graph.
\item Constructing closures calculating the normalized discrete distribution from these likelihoods.
\item Providing a Gibbs-component sampler for \turingjl{}, that can utilize the resulting
  conditional distributions.
\end{enumerate}
The third step turned out to be the easiest, since the sampling system of \turingjl{} is designed to
be extensible.  Ideally, a Gibbs-conditional sampler would have first been added to \turingjl{} and
then simply been reused for \autogibbsjl{}; in practice, it worked out the other way round, and the
\autogibbsjl{} sampler has, in generalized form, been added to \turingjl{} afterwards (without the
automatic extraction, only supporting user-provided conditional distributions).

likelihood closures; conditional likelihood extraction\todo{TODO}

\section{Evaluation}
\label{sec:autogibbs-eval}

To begin with a qualitative assessment, it must be admitted that \autogibbsjl{} is, when run
manually and only once, empirically so noticeably slow, that a user may be tempted to dismiss it
outright (concretely speaking, extraction of one conditional for some not-so-large models takes
\(20\) to \(200\) seconds on the author's laptop).  This is a valid point, but two counter-arguments
must be considered.  For one, the implementation is just more conceptual than optimized.  Much of
the slow-down could be mitigated by improving key parts of \autogibbsjl{} and \irtrackerjl{}.  In
addition to that, one must realize where this appearent slowness comes from: namely, from
compilation, and therein primarily type inference, of the functions handling all the strongly typed
expression trees.  Even neglecting optimization possibilities of these, compilation takes place only
once; as soon as a conditional is constructed, it can be reused in arbitrarily many sampling runs of
the same model.  The finished conditionals then do not take so much time anymore, on the contrary:
they are much faster than other within-Gibbs samplers, since they only involve evaluating a fixed
expression, constructing a distribution, and sampling from it once (and even this could be sped up
further).  This makes it possible to sample much longer chains in the same time.

So, overall, while the implementation in current form is not very applicable in practise for all
cases, it is very much so in principle.  Based on the lessons learned through this work, further
contributions to Gibbs sampling in \turingjl{} are already planned (although not necessarily based
on \autogibbsjl{}).

\newthought{For a more {q}uantitative} point of view, let us now turn to some empirical evaluations.
Besides several unit tests for correctness of the derived dependencies and conditionals on a variety
of models chosen to test certain features and corner cases, an experimental comparison of
\autogibbsjl{} and existing \turingjl{} samplers has been conducted.  For this purpose, three
off-the-shelf Bayesian models were chosen: a Gaussian mixture model with known variances and priors
over cluster centers, weights, and assignments \parencite[section 6.2]{marin2007bayesian},
\begin{equation}
  \label{eq:gmm}
  \begin{aligned}
    w &\from \distr{Dirichlet(K)} \\
    z_{n} &\from \distr{Discrete}([1, \ldots, K], w), \quad n = 1, \ldots, N \\
    \mu_{k} &\from \Normal(0, \sigma_{1}), \quad k = 1, \ldots, K \\
    x_{n} &\from \Normal(\mu_{z_{n}}, \sigma_{1}), \quad n = 1, \ldots, N;
  \end{aligned}
\end{equation}
a hidden Markov model with known variances and priors over transition and emission
probabilities \parencite[section 7.3]{marin2007bayesian},
\begin{equation}
  \label{eq:hmm}
  \begin{aligned}
    T_{k} &\from \distr{Dirichlet}(K), \quad k = 1, \ldots, K \\
    m_{k} &\from \Normal(k, \sigma_{1}), \quad k = 1, \ldots, K \\
    s_{1} &\from \distr{Categorical}(K) \\
    s_{k} &\from \distr{Categorical}(T_{s_{k-1}}), \quad k = 2, \ldots, N \\
    x_{k} &\from \Normal(m_{s_{k}}, \sigma_{2}), \quad k = 1, \ldots, N;
  \end{aligned}
\end{equation}
and an infinite mixture model in stick-breaking construction, but otherwise of the same form as the
finite GMM above, to represent a nonparametric example \parencite[section 2.2]{hjort2010bayesian}:
\begin{equation}
  \label{eq:imm}
  \begin{aligned}
    w &\from \distr{TruncatedStickBreakingProcess(\alpha, K)} \\
    z_{n} &\from \distr{Categorical}(w), \quad n = 1, \ldots, N \\
    \mu_{k} &\from \Normal(0, \sigma_{1}), \quad k = 1, \ldots, K \\
    y_{n} &\from \Normal(\mu_{z_{n}}, \sigma_{2}), \quad n = 1, \ldots, N.
  \end{aligned}
\end{equation}
The models are implemented in \dppljl{} as shown in listing~\ref{lst:evaluation-models}.
\begin{lstfloat}[p]
\begin{lstlisting}[style=lstfloat]
@model function gmm(x, K)
    N = length(x)
    w ~ Dirichlet(K, 1/K)  # Cluster association prior
    z ~ filldist(DiscreteNonParametric(1:K, w), N)  # Cluster assignments
    μ ~ filldist(Normal(0.0, s1_gmm), K)  # Cluster centers

    for n = 1:N
        x[n] ~ Normal(μ[z[n]], s2_gmm)  # Observations
    end
end

@model function hmm(x, K, ::Type{T}=Float64) where {T<:Real}
    N = length(x)

    T = Vector{Vector{X}}(undef, K)
    for i = 1:K
        T[i] ~ Dirichlet(K, 1/K)  # Transition probabilities
    end
    
    s = zeros(Int, N)
    s[1] ~ Categorical(K)
    for i = 2:N
        s[i] ~ Categorical(T[s[i-1]])  # State sequence
    end
    
    m = Vector{T}(undef, K)
    for i = 1:K
        m[i] ~ Normal(i, s1_hmm)  # Emission probabilities
    end
    
    x[1] ~ Normal(m[s[1]], s2_hmm)
    for i = 2:N
        x[i] ~ Normal(m[s[i]], s2_hmm)  # Observations
    end
end

@model function imm_stick(y, α, K)
    N = length(y)
    crm = DirichletProcess(α)
    v ~ filldist(StickBreakingProcess(crm), K - 1)
    w = stickbreak(v)  # Cluster weights
    
    z = zeros(Int, N)
    for n = 1:N
        z[n] ~ Categorical(w)  # Cluster assignments
    end

    μ ~ filldist(Normal(0.0, s1_imm), K)  # Cluster centers

    for n = 1:N
        y[n] ~ Normal(μ[z[n]], s2_imm)  # Observations
    end
end
\end{lstlisting}
  \caption{Gaussian mixture model, hidden Markov model, and infinite mixture model using a
    stick-breaking construction.  The two-step calculation of \texttt{w} via \texttt{v} is a
    technicality due to \turingjl{}'s handling of nonparametric models.  The function
    \texttt{stickbreak} normalizes the stick-lengths \texttt{v} into a Dirichlet-like distribution.}
  \label{lst:evaluation-models}
\end{lstfloat}
The three interesting classes of metrics to evaluate are, in the case of this work,
\begin{enumerate}
  \firmlist
\item the \enquote{compilation time} of \autogibbsjl{}, i.e., the time it takes to extract a
  conditional,
\item the sampling speed when used as component of a within-Gibbs sampler, and
\item the quality of the resulting chains, in terms of convergence and variance diagnostics.
  (Although this really measures Gibbs sampling, not the implementation of \autogibbsjl{}, it is a
  relevant comparison for the practitioner.)
\end{enumerate}
Each of the test models involves one discrete and two continuous parameters.  As benchmark for
\autogibbsjl{}'s static conditional (AG), it is compared to \turingjl{}'s Particle Gibbs sampler
(PG), which is also suited to discrete parameters.  Continuous variables are all sampled using
Hamiltonian Monte Carlo (HMC) with hand-tuned parameters per model.  The experiments have been set
up such that each model is evaluated for ten chains of length \(10000\), varying between AG and PG,
and four different values for the number of observations (since these determine the size of the
trace, and thus have an influence on both AG's compile times and the overall sampling time).  PG was
always used with \(100\) particles, since lower values did lead to convergent chains.

Concrete parameters in table~\ref{tab:autogibbs-params}.\todo{more meaningful table}

\begin{table}[t]
  \centering
  \libertineTabular
  \begin{tabularx}{\textwidth}{XXrrr@{\hskip 10mm}rrrr}
    \toprule
     & & \multicolumn{3}{c@{\hskip 10mm}}{\textbf{GMM}} & \multicolumn{3}{c}{\textbf{HMM}} & \multicolumn{1}{c}{\textbf{IMM}} \\
    \midrule
    & HMC Step size & & 0.05 & & & 0.05 & & 0.05 \\
    & HMC Steps & & 10 & & & 10 & & 10 \\
    \midrule
    \textbf{AG + HMC} & Observations & 10 & 25 & 50 & 10 & 25 & 50 & 10 \\
    & Chains & 30 & 30 & 30 & 30 & 30 & 30 & 30 \\
    & Compilations & 3 & 3 & 3 & 3 & 3 & 3 & 3 \\
    \addlinespace
    \textbf{PG + HMC,} & Observations & 10 & 25 & 50 & 10 & 25 & 50 & 10 \\
     & Chains & 10 & 10 & 10 & 10 & 10 & 10 & 10 \\
    \bottomrule
  \end{tabularx}
  \caption{Experimental conditions for evaluating AutoGibbs (AG) agains Particle Gibbs (PG).  Chains
    were always of length \(10000\).  A new static Gibbs conditional was extracted for each block of
    \(10\) chains that was run with the same parameters while Particle Gibbs was varied over the
    three particle sizes.  Particle Gibbs with 50 particles was sometimes killed due to timeouts on
    the server.}
  \label{tab:autogibbs-params}
\end{table}



% extraction times
% Measuring both compilation of the traced code and the conditional calculation.",
% "All 2 or 3 repetitions per data size class are shown."
% Linear fit for time ~ datasize²
% three samples

% sampling times
% subtitle = "Factored by algorithm and number of PG particles"

% diagnostics
% subtitle = "Factored by algorithm and number of particles"

% densities
% subtitle = paste("Factored by number of observations (data size)", "and selected parameters")

% ACFs
% subtitle = "ACF plots for one sample chain per data size"


%%% Local Variables: 
%%% TeX-master: "main"
%%% End: