
@unpublished{abadi2015tensorflow,
  title = {{{TensorFlow}}: {{Large}}-Scale Machine Learning on Heterogeneous Systems},
  author = {Abadi, Martín and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mané, Dandelion and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viégas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  date = {2015},
  url = {https://www.tensorflow.org/},
  urldate = {2020-07-29},
  file = {/home/philipp/Zotero/storage/YG5QTYPF/Abadi et al_2015_TensorFlow.pdf},
  howpublished = {Preliminary White Paper},
  type = {Preliminary White Paper}
}

@article{andrieu2010particlea,
  title = {Particle {{Markov}} Chain {{Monte Carlo}} Methods},
  shorttitle = {Particle {{Markov}} Chain {{Monte Carlo}} Methods},
  author = {Andrieu, Christophe and Doucet, Arnaud and Holenstein, Roman},
  date = {2010-06},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {72},
  pages = {269--342},
  doi = {10.1111/j.1467-9868.2009.00736.x},
  url = {http://doi.wiley.com/10.1111/j.1467-9868.2009.00736.x},
  urldate = {2019-03-22},
  abstract = {Markov chain Monte Carlo and sequential Monte Carlo methods have emerged as the two main tools to sample from high dimensional probability distributions. Although asymptotic convergence of Markov chain Monte Carlo algorithms is ensured under weak assumptions, the performance of these algorithms is unreliable when the proposal distributions that are used to explore the space are poorly chosen and/or if highly correlated variables are updated independently. We show here how it is possible to build efficient high dimensional proposal distributions by using sequential Monte Carlo methods. This allows us not only to improve over standard Markov chain Monte Carlo schemes but also to make Bayesian inference feasible for a large class of statistical models where this was not previously so. We demonstrate these algorithms on a non-linear state space model and a Lévy-driven stochastic volatility model.},
  file = {/home/philipp/Zotero/storage/B4YYESTH/Andrieu et al. - 2010 - Particle Markov chain Monte Carlo methods Particl.pdf},
  langid = {english},
  number = {3}
}

@article{bartholomew-biggs2000automatic,
  title = {Automatic Differentiation of Algorithms},
  author = {Bartholomew-Biggs, Michael and Brown, Steven and Christianson, Bruce and Dixon, Laurence},
  date = {2000-12-01},
  journaltitle = {Journal of Computational and Applied Mathematics},
  shortjournal = {Journal of Computational and Applied Mathematics},
  volume = {124},
  pages = {171--190},
  doi = {10.1016/S0377-0427(00)00422-2},
  url = {http://www.sciencedirect.com/science/article/pii/S0377042700004222},
  urldate = {2019-04-22},
  abstract = {We introduce the basic notions of automatic differentiation, describe some extensions which are of interest in the context of nonlinear optimization and give some illustrative examples.},
  file = {/home/philipp/Zotero/storage/TCZM3IUB/Bartholomew-Biggs et al_2000_Automatic differentiation of algorithms.pdf;/home/philipp/Zotero/storage/T6SMYKUV/S0377042700004222.html},
  number = {1},
  series = {Numerical {{Analysis}} 2000. {{Vol}}. {{IV}}: {{Optimization}} and {{Nonlinear Equations}}}
}

@article{baydin2018automatic,
  ids = {baydinautomatic},
  title = {Automatic Differentiation in Machine Learning: A Survey},
  author = {Baydin, Atılım Güneş and Pearlmutter, Barak A. and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark},
  date = {2018},
  journaltitle = {Journal of Machine Learning Research},
  volume = {18},
  pages = {1--43},
  url = {http://jmlr.org/papers/v18/17-468.html},
  file = {/home/philipp/Zotero/storage/6AB74F72/Baydin et al. - Automatic Diﬀerentiation in Machine Learning a Su.pdf},
  number = {153}
}

@online{baydin2019etalumis,
  title = {Etalumis: {{Bringing Probabilistic Programming}} to {{Scientific Simulators}} at {{Scale}}},
  shorttitle = {Etalumis},
  author = {Baydin, Atılım Güneş and Shao, Lei and Bhimji, Wahid and Heinrich, Lukas and Meadows, Lawrence and Liu, Jialin and Munk, Andreas and Naderiparizi, Saeid and Gram-Hansen, Bradley and Louppe, Gilles and Ma, Mingfei and Zhao, Xiaohui and Torr, Philip and Lee, Victor and Cranmer, Kyle and Prabhat and Wood, Frank},
  date = {2019-08-27},
  url = {http://arxiv.org/abs/1907.03382},
  urldate = {2019-11-17},
  abstract = {Probabilistic programming languages (PPLs) are receiving widespread attention for performing Bayesian inference in complex generative models. However, applications to science remain limited because of the impracticability of rewriting complex scientific simulators in a PPL, the computational cost of inference, and the lack of scalable implementations. To address these, we present a novel PPL framework that couples directly to existing scientific simulators through a cross-platform probabilistic execution protocol and provides Markov chain Monte Carlo (MCMC) and deep-learning-based inference compilation (IC) engines for tractable inference. To guide IC inference, we perform distributed training of a dynamic 3DCNN--LSTM architecture with a PyTorch-MPI-based framework on 1,024 32-core CPU nodes of the Cori supercomputer with a global minibatch size of 128k: achieving a performance of 450 Tflop/s through enhancements to PyTorch. We demonstrate a Large Hadron Collider (LHC) use-case with the C++ Sherpa simulator and achieve the largest-scale posterior inference in a Turing-complete PPL.},
  archivePrefix = {arXiv},
  eprint = {1907.03382},
  eprinttype = {arxiv},
  file = {/home/philipp/Zotero/storage/9BN6PNDS/Baydin et al. - 2019 - Etalumis Bringing Probabilistic Programming to Sc.pdf;/home/philipp/Zotero/storage/559BSTHZ/1907.html},
  primaryClass = {cs, stat}
}

@online{bernstein2020differentiating,
  title = {Differentiating a {{Tensor Language}}},
  author = {Bernstein, Gilbert and Mara, Michael and Li, Tzu-Mao and Maclaurin, Dougal and Ragan-Kelley, Jonathan},
  date = {2020-08-25},
  url = {http://arxiv.org/abs/2008.11256},
  urldate = {2020-09-14},
  abstract = {How does one compile derivatives of tensor programs, such that the resulting code is purely functional (hence easier to optimize and parallelize) and provably efficient relative to the original program? We show that naively differentiating tensor code---as done in popular systems like Tensorflow and PyTorch---can cause asymptotic slowdowns in pathological cases, violating the Cheap Gradients Principle. However, all existing automatic differentiation methods that guarantee this principle (for variable size data) do so by relying on += mutation through aliases/pointers---which complicates downstream optimization. We provide the first purely functional, provably efficient, adjoint/reverse-mode derivatives of array/tensor code by explicitly accounting for sparsity. We do this by focusing on the indicator function from Iverson's APL. We also introduce a new "Tensor SSA" normal form and a new derivation of reverse-mode automatic differentiation based on the universal property of inner-products.},
  archivePrefix = {arXiv},
  eprint = {2008.11256},
  eprinttype = {arxiv},
  file = {/home/philipp/Zotero/storage/29FNMBQS/Bernstein et al_2020_Differentiating a Tensor Language.pdf;/home/philipp/Zotero/storage/U523BZ74/2008.html},
  primaryClass = {cs}
}

@online{betancourt2018conceptual,
  title = {A {{Conceptual Introduction}} to {{Hamiltonian Monte Carlo}}},
  author = {Betancourt, Michael},
  date = {2018-07-15},
  url = {http://arxiv.org/abs/1701.02434},
  urldate = {2020-10-17},
  abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous understanding of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is confined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important. In this review I provide a comprehensive conceptual account of these theoretical foundations, focusing on developing a principled intuition behind the method and its optimal implementations rather of any exhaustive rigor. Whether a practitioner or a statistician, the dedicated reader will acquire a solid grasp of how Hamiltonian Monte Carlo works, when it succeeds, and, perhaps most importantly, when it fails.},
  archivePrefix = {arXiv},
  eprint = {1701.02434},
  eprinttype = {arxiv},
  file = {/home/philipp/Zotero/storage/5N2EZ4J6/Betancourt_2018_A Conceptual Introduction to Hamiltonian Monte Carlo.pdf;/home/philipp/Zotero/storage/YQ23HB6W/1701.html},
  primaryClass = {stat}
}

@article{bezanson2017julia,
  title = {Julia: {{A Fresh Approach}} to {{Numerical Computing}}},
  shorttitle = {Julia},
  author = {Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B.},
  date = {2017-01-01},
  journaltitle = {SIAM Review},
  shortjournal = {SIAM Rev.},
  volume = {59},
  pages = {65--98},
  doi = {10.1137/141000671},
  url = {https://epubs.siam.org/doi/10.1137/141000671},
  urldate = {2019-10-02},
  abstract = {Bridging cultures that have often been distant, Julia combines expertise from the diverse fields of computer science and computational science to create a new approach to numerical  computing. Julia is  designed to be easy and fast and questions notions generally held to be “laws of nature"  by practitioners of numerical computing: \textbackslash beginlist \textbackslash item  High-level dynamic programs have to be slow. \textbackslash item  One must prototype in one language and then rewrite in another language for speed or deployment. \textbackslash item There are parts of a system appropriate for the programmer, and other parts that are best left untouched as they have been built by the experts. \textbackslash endlist We introduce the  Julia programming language and its design---a  dance between specialization and abstraction. Specialization allows for custom treatment. Multiple dispatch,  a  technique from computer science, picks  the right algorithm for the right circumstance. Abstraction, which is what good computation is really about, recognizes what remains the same after differences are stripped away. Abstractions in mathematics are captured as code through another technique from computer science, generic programming. Julia shows that  one can achieve machine performance without sacrificing human convenience.},
  file = {/home/philipp/Zotero/storage/QCSTCADU/Bezanson et al_2017_Julia.pdf;/home/philipp/Zotero/storage/WW3Q6Q7D/141000671.html},
  number = {1}
}

@article{bezanson2018julia,
  title = {Julia: {{Dynamism}} and Performance Reconciled by Design},
  author = {Bezanson, Jeff and Chen, Jiahao and Chung, Benjamin and Karpinski, Stefan and Shah, Viral B. and Vitek, Jan and Zoubritzky, Lionel},
  date = {2018-10},
  journaltitle = {Proc. ACM Program. Lang.},
  volume = {2},
  doi = {10.1145/3276490},
  url = {https://doi.org/10.1145/3276490},
  file = {/home/philipp/Zotero/storage/WLAU4MCM/Bezanson et al. - Julia Dynamism and Performance Reconciled by Desi.pdf},
  issue = {OOPSLA},
  keywords = {dynamic languages,just-in-time compilation,multiple dispatch}
}

@article{bhat2012type,
  title = {A Type Theory for Probability Density Functions},
  author = {Bhat, Sooraj and Agarwal, Ashish and Vuduc, Richard and Gray, Alexander},
  date = {2012},
  journaltitle = {ACM SIGPLAN Notices},
  volume = {47},
  pages = {545--556},
  file = {/home/philipp/Zotero/storage/JIXCSWNI/Bhat et al_2012_A type theory for probability density functions.pdf},
  number = {1}
}

@article{bianucci2000application,
  title = {Application of {{Cascade Correlation Networks}} for {{Structures}} to {{Chemistry}}},
  author = {Bianucci, Anna Maria and Micheli, Alessio and Sperduti, Alessandro and Starita, Antonina},
  date = {2000},
  journaltitle = {Applied Intelligence},
  shortjournal = {Applied Intelligence},
  volume = {12},
  pages = {117--147},
  doi = {10.1023/A:1008368105614},
  abstract = {We present the application of Cascade Correlation for structures to QSPR (quantitative structure-property relationships) and QSAR (quantitative structure-activity relationships) analysis. Cascade Correlation for structures is a neural network model recently proposed for the processing of structured data. This allows the direct treatment of chemical compounds as labeled trees, which constitutes a novel approach to QSPR/QSAR. We report the results obtained for QSPR on Alkanes (predicting the boiling point) and QSAR of a class of Benzodiazepines. Our approach compares favorably versus the traditional QSAR treatment based on equations and it is competitive with ‘ad hoc’ MLPs for the QSPR problem.},
  file = {/home/philipp/Zotero/storage/YVHHWTZW/Bianucci et al_2000_Application of Cascade Correlation Networks for Structures to Chemistry.pdf},
  langid = {english},
  number = {1}
}

@online{bingham2018pyro,
  title = {Pyro: {{Deep Universal Probabilistic Programming}}},
  shorttitle = {Pyro},
  author = {Bingham, Eli and Chen, Jonathan P. and Jankowiak, Martin and Obermeyer, Fritz and Pradhan, Neeraj and Karaletsos, Theofanis and Singh, Rohit and Szerlip, Paul and Horsfall, Paul and Goodman, Noah D.},
  date = {2018-10-18},
  url = {http://arxiv.org/abs/1810.09538},
  urldate = {2019-10-14},
  abstract = {Pyro is a probabilistic programming language built on Python as a platform for developing advanced probabilistic models in AI research. To scale to large datasets and high-dimensional models, Pyro uses stochastic variational inference algorithms and probability distributions built on top of PyTorch, a modern GPU-accelerated deep learning framework. To accommodate complex or model-specific algorithmic behavior, Pyro leverages Poutine, a library of composable building blocks for modifying the behavior of probabilistic programs.},
  archivePrefix = {arXiv},
  eprint = {1810.09538},
  eprinttype = {arxiv},
  file = {/home/philipp/Zotero/storage/J6WRQ2BB/Bingham et al_2018_Pyro.pdf;/home/philipp/Zotero/storage/UHNB9PXN/1810.html},
  primaryClass = {cs, stat}
}

@report{bolewski2015staged,
  title = {Staged Programming in {{Julia}}},
  author = {Bolewski, Jake},
  date = {2015-08-04},
  institution = {{JuliaCon 2015}},
  location = {{Boston}},
  url = {https://www.youtube.com/watch?v=KAN8zbM659o},
  urldate = {2019-10-09},
  langid = {english},
  type = {Presentation}
}

@online{carpenter2015stan,
  title = {The {{Stan Math Library}}: {{Reverse}}-{{Mode Automatic Differentiation}} in {{C}}++},
  shorttitle = {The {{Stan Math Library}}},
  author = {Carpenter, Bob and Hoffman, Matthew D. and Brubaker, Marcus and Lee, Daniel and Li, Peter and Betancourt, Michael},
  date = {2015-09-23},
  url = {http://arxiv.org/abs/1509.07164},
  urldate = {2020-03-26},
  abstract = {As computational challenges in optimization and statistical inference grow ever harder, algorithms that utilize derivatives are becoming increasingly more important. The implementation of the derivatives that make these algorithms so powerful, however, is a substantial user burden and the practicality of these algorithms depends critically on tools like automatic differentiation that remove the implementation burden entirely. The Stan Math Library is a C++, reverse-mode automatic differentiation library designed to be usable, extensive and extensible, efficient, scalable, stable, portable, and redistributable in order to facilitate the construction and utilization of such algorithms. Usability is achieved through a simple direct interface and a cleanly abstracted functional interface. The extensive built-in library includes functions for matrix operations, linear algebra, differential equation solving, and most common probability functions. Extensibility derives from a straightforward object-oriented framework for expressions, allowing users to easily create custom functions. Efficiency is achieved through a combination of custom memory management, subexpression caching, traits-based metaprogramming, and expression templates. Partial derivatives for compound functions are evaluated lazily for improved scalability. Stability is achieved by taking care with arithmetic precision in algebraic expressions and providing stable, compound functions where possible. For portability, the library is standards-compliant C++ (03) and has been tested for all major compilers for Windows, Mac OS X, and Linux.},
  archivePrefix = {arXiv},
  eprint = {1509.07164},
  eprinttype = {arxiv},
  file = {/home/philipp/Zotero/storage/FN47Y5L4/Carpenter et al_2015_The Stan Math Library.pdf;/home/philipp/Zotero/storage/AFZTJE7E/1509.html},
  primaryClass = {cs}
}

@report{cohen2016automatic,
  title = {Automatic {{Reverse}}-{{Mode Differentiation}}: {{Lecture Notes}}},
  author = {Cohen, William W},
  date = {2016},
  pages = {12},
  file = {/home/philipp/Zotero/storage/E94CBFEF/Cohen - Automatic Reverse-Mode Diﬀerentiation Lecture Not.pdf},
  langid = {english}
}

@book{congdon2006bayesian,
  title = {Bayesian Statistical Modelling},
  author = {Congdon, P.},
  date = {2006},
  edition = {2nd ed},
  publisher = {{John Wiley \& Sons}},
  location = {{Chichester, England ; Hoboken, NJ}},
  annotation = {OCLC: ocm70673258},
  file = {/home/philipp/Zotero/storage/ZZQKXVD6/Congdon_2006_Bayesian statistical modelling.pdf},
  isbn = {978-0-470-01875-0},
  pagetotal = {573},
  series = {Wiley Series in Probability and Statistics}
}

@thesis{cusumano-towner2020gen,
  title = {Gen: {{A High}}-{{Level Programming Platform}} for {{Probabilistic Inference}}},
  author = {Cusumano-Towner, Marco Francis},
  date = {2020},
  institution = {{Massachusetts Institute of Technology}},
  location = {{Massachusetts}},
  abstract = {Probabilistic inference provides a powerful theoretical framework for engineering intelligent systems. However, diverse modeling approaches and inference algorithms are needed to navigate engineering tradeoffs between robustness, adaptability, accuracy, safety, interpretability, data efficiency, and computational efficiency. Structured generative models represented as symbolic programs provide interpretability. Structure learning of these models provides data-efficient adaptability. Uncertainty quantification is needed for safety. Bottom-up, discriminative inference provides computational efficiency. Iterative “model-in-the-loop” algorithms can improve accuracy by fine-tuning inferences and improve robustness to outof-distribution data. Recent probabilistic programming systems fully or partially automate inference, but are too restrictive for many applications. Differentiable programming systems are also inadequate: they do not support structure learning of generative models or hybrids of “model-in-the-loop” and discriminative inference. Therefore, probabilistic inference is still often implemented by translating tedious mathematical derivations into low-level numerical programs, which are error-prone and difficult to modify and maintain. This thesis presents the design and implementation of the Gen programming platform for probabilistic inference. Gen automates the low-level implementation of probabilistic inference algorithms while remaining flexible enough to support heterogeneous algorithmic approaches and extensible enough for practical inference engineering. Gen users define their models explicitly using probabilistic programs, but instead of compiling the model directly into an inference algorithm implementation, Gen compiles the model into data types that encapsulate low-level inference operations whose semantics are derived from the model, like sampling, density evaluation, and gradients. Users write their inference application in a general-purpose programming language using Gen’s abstract data types as primitives. This thesis defines Gen’s data types and shows that they can be used to compose a variety of inference techniques including sophisticated Monte Carlo algorithms and hybrids of Monte Carlo, variational, and discriminative techniques. The same data types can be generated from multiple probabilistic programming languages that strike different expressiveness and performance tradeoffs. By decoupling probabilistic programming language implementations from inference algorithm design, Gen enables more flexible specialization of both, leading to performance improvements over existing probabilistic programming systems.},
  file = {/home/philipp/Zotero/storage/2FL2Z6ST/Cusumano-Towner - Gen A High-Level Programming Platform for Probabi.pdf},
  langid = {english},
  type = {PhD Thesis}
}

@online{dahlin2015getting,
  title = {Getting {{Started}} with {{Particle Metropolis}}-{{Hastings}} for {{Inference}} in {{Nonlinear Dynamical Models}}},
  author = {Dahlin, Johan and Schön, Thomas B.},
  date = {2015-11-05},
  url = {http://arxiv.org/abs/1511.01707},
  urldate = {2019-04-06},
  abstract = {This tutorial provides a gentle introduction to the particle Metropolis-Hastings (PMH) algorithm for parameter inference in nonlinear state-space models together with a software implementation in the statistical programming language R. We employ a step-by-step approach to develop an implementation of the PMH algorithm (and the particle filter within) together with the reader. This final implementation is also available as the package pmhtutorial from the Comprehensive R Archive Network (CRAN) repository. Throughout the tutorial, we provide some intuition as to how the algorithm operates and discuss some solutions to problems that might occur in practice. To illustrate the use of PMH, we consider parameter inference in a linear Gaussian state-space model with synthetic data and a nonlinear stochastic volatility model with real-world data.},
  archivePrefix = {arXiv},
  eprint = {1511.01707},
  eprinttype = {arxiv},
  file = {/home/philipp/Zotero/storage/QT6M2F9V/Dahlin and Schön - 2015 - Getting Started with Particle Metropolis-Hastings .pdf},
  langid = {english},
  primaryClass = {q-fin, stat}
}

@inproceedings{dauwels2005steepest,
  title = {Steepest Descent as Message Passing},
  author = {Dauwels, Justin and Korl, Sascha and Loeliger, Hans-Andrea},
  date = {2005},
  location = {{Rotorua}},
  doi = {10.1109/ITW.2005.1531853},
  url = {http://ieeexplore.ieee.org/document/1531853/},
  urldate = {2019-10-03},
  abstract = {It is shown how steepest descent (or steepest ascent) may be viewed as a message passing algorithm with “local” message update rules. For example, the well-known backpropagation algorithm for the training of feed-forward neural networks may be viewed as message passing on a factor graph. The factor graph approach with its emphasis on “local” computations makes it easy to combine steepest descent with other message passing algorithms such as the sum/max-product algorithms, expectation maximization, Kalman filtering/smoothing, and particle filters. As an example, parameter estimation in a state space model is considered. For this example, it is shown how steepest descent can be used for the maximization step in expectation maximization.},
  eventtitle = {{{IEEE Information Theory Workshop}}},
  file = {/home/philipp/Zotero/storage/WTCJYCWC/Dauwels et al. - 2005 - Steepest descent as message passing.pdf},
  langid = {english}
}

@online{elliott2018simple,
  title = {The Simple Essence of Automatic Differentiation},
  author = {Elliott, Conal},
  date = {2018-04-02},
  url = {http://arxiv.org/abs/1804.00746},
  urldate = {2019-03-19},
  abstract = {Automatic differentiation (AD) in reverse mode (RAD) is a central component of deep learning and other uses of large-scale optimization. Commonly used RAD algorithms such as backpropagation, however, are complex and stateful, hindering deep understanding, improvement, and parallel execution. This paper develops a simple, generalized AD algorithm calculated from a simple, natural specification. The general algorithm is then specialized by varying the representation of derivatives. In particular, applying well-known constructions to a naive representation yields two RAD algorithms that are far simpler than previously known. In contrast to commonly used RAD implementations, the algorithms defined here involve no graphs, tapes, variables, partial derivatives, or mutation. They are inherently parallel-friendly, correct by construction, and usable directly from an existing programming language with no need for new data types or programming style, thanks to use of an AD-agnostic compiler plugin.},
  archivePrefix = {arXiv},
  eprint = {1804.00746},
  eprinttype = {arxiv},
  file = {/home/philipp/Zotero/storage/PD3BXF3W/Elliott_2018_The simple essence of automatic differentiation.pdf;/home/philipp/Zotero/storage/CHTARMGZ/1804.html},
  primaryClass = {cs}
}

@article{fritz2020synthetic,
  title = {A Synthetic Approach to {{Markov}} Kernels, Conditional Independence and Theorems on Sufficient Statistics},
  author = {Fritz, Tobias},
  date = {2020-08},
  journaltitle = {Advances in Mathematics},
  shortjournal = {Advances in Mathematics},
  volume = {370},
  pages = {107239},
  issn = {00018708},
  doi = {10.1016/j.aim.2020.107239},
  url = {http://arxiv.org/abs/1908.07021},
  urldate = {2020-09-18},
  abstract = {We develop Markov categories as a framework for synthetic probability and statistics, following work of Golubtsov as well as Cho and Jacobs. This means that we treat the following concepts in purely abstract categorical terms: conditioning and disintegration; various versions of conditional independence and its standard properties; conditional products; almost surely; sufficient statistics; versions of theorems on sufficient statistics due to Fisher--Neyman, Basu, and Bahadur. Besides the conceptual clarity offered by our categorical setup, its main advantage is that it provides a uniform treatment of various types of probability theory, including discrete probability theory, measure-theoretic probability with general measurable spaces, Gaussian probability, stochastic processes of either of these kinds, and many others.},
  archivePrefix = {arXiv},
  eprint = {1908.07021},
  eprinttype = {arxiv},
  file = {/home/philipp/Zotero/storage/F3ZQJNWY/Fritz_2020_A synthetic approach to Markov kernels, conditional independence and theorems.pdf;/home/philipp/Zotero/storage/H9H5BDUZ/1908.html}
}

@inproceedings{gabler2019graph,
  title = {Graph {{Tracking}} in {{Dynamic Probabilistic Programs}} via {{Source Transformations}}},
  author = {Gabler, Philipp and Trapp, Martin and Ge, Hong and Pernkopf, Franz},
  date = {2019-10-16},
  location = {{Vancouver}},
  url = {https://openreview.net/forum?id=r1eAFknEKr},
  urldate = {2020-07-07},
  abstract = {Many modern machine learning algorithms, such as automatic differentiation (AD) and versions of approximate Bayesian inference, can be understood as a particular case of message passing on some...},
  eventtitle = {2nd {{Symposium}} on {{Advances}} in {{Approximate Bayesian Inference}}},
  file = {/home/philipp/Zotero/storage/DLMRL3CG/Gabler et al_2019_Graph Tracking in Dynamic Probabilistic Programs via Source Transformations.pdf;/home/philipp/Zotero/storage/HGJI9LI7/forum.html},
  langid = {english}
}

@inproceedings{ge2018turing,
  title = {Turing: {{A Language}} for {{Flexible Probabilistic Inference}}},
  shorttitle = {Turing},
  booktitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Ge, Hong and Xu, Kai and Ghahramani, Zoubin},
  date = {2018-03-31},
  pages = {1682--1690},
  url = {http://proceedings.mlr.press/v84/ge18b.html},
  urldate = {2019-03-01},
  abstract = {Probabilistic programming promises to simplify and democratize probabilistic machine learning, but successful probabilistic programming systems require flexible, generic and efficient inference eng...},
  eventtitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  file = {/home/philipp/Zotero/storage/TH73FNUA/Ge et al. - 2018 - Turing A Language for Flexible Probabilistic Infe.pdf},
  langid = {english}
}

@article{gebremedhin2020introduction,
  title = {An Introduction to Algorithmic Differentiation},
  author = {Gebremedhin, Assefaw H. and Walther, Andrea},
  date = {2020},
  journaltitle = {WIREs Data Mining and Knowledge Discovery},
  volume = {10},
  pages = {e1334},
  issn = {1942-4795},
  doi = {10.1002/widm.1334},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1334},
  urldate = {2020-07-29},
  abstract = {Algorithmic differentiation (AD), also known as automatic differentiation, is a technology for accurate and efficient evaluation of derivatives of a function given as a computer model. The evaluations of such models are essential building blocks in numerous scientific computing and data analysis applications, including optimization, parameter identification, sensitivity analysis, uncertainty quantification, nonlinear equation solving, and integration of differential equations. We provide an introduction to AD and present its basic ideas and techniques, some of its most important results, the implementation paradigms it relies on, the connection it has to other domains including machine learning and parallel computing, and a few of the major open problems in the area. Topics we discuss include: forward mode and reverse mode of AD, higher-order derivatives, operator overloading and source transformation, sparsity exploitation, checkpointing, cross-country mode, and differentiating iterative processes. This article is categorized under: Algorithmic Development {$>$} Scalable Statistical Methods Technologies {$>$} Data Preprocessing},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1334},
  file = {/home/philipp/Zotero/storage/XAJGF8EA/Gebremedhin_Walther_2020_An introduction to algorithmic differentiation.pdf;/home/philipp/Zotero/storage/G33NZV4X/widm.html},
  langid = {english},
  number = {1}
}

@article{geweke2004getting,
  title = {Getting {{It Right}}: {{Joint Distribution Tests}} of {{Posterior Simulators}}},
  shorttitle = {Getting {{It Right}}},
  author = {Geweke, John},
  date = {2004-09},
  journaltitle = {Journal of the American Statistical Association},
  volume = {99},
  pages = {799--804},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/016214504000001132},
  url = {http://www.tandfonline.com/doi/abs/10.1198/016214504000001132},
  urldate = {2020-02-10},
  abstract = {Analytical or coding errors in posterior simulators can produce reasonable but incorrect approximations of posterior moments. This article develops simple tests of posterior simulators that detect both kinds of errors, and uses them to detect and correct errors in two previously published papers. The tests exploit the fact that a Bayesian model specifies the joint distribution of observables (data) and unobservables (parameters). There are two joint distribution simulators. The marginal-conditional simulator draws unobservables from the prior and then observables conditional on unobservables. The successive-conditional simulator alternates between the posterior simulator and an observables simulator. Formal comparison of moment approximations of the two simulators reveals existing analytical or coding errors in the posterior simulator.},
  file = {/home/philipp/Zotero/storage/272J5VMQ/Geweke - 2004 - Getting It Right Joint Distribution Tests of Post.pdf},
  langid = {english},
  number = {467}
}

@unpublished{ghahramani2005nonparametric,
  title = {Non-Parametric {{Bayesian Methods}}},
  author = {Ghahramani, Zoubin},
  date = {2005-07},
  url = {http://mlg.eng.cam.ac.uk/zoubin/talks/uai05tutorial-b.pdf},
  eventtitle = {Uncertainty in {{Artificial Intelligence Tutorial}}},
  file = {/home/philipp/Zotero/storage/2C2D3MLB/Ghahramani - Non-parametric Bayesian Methods.pdf},
  langid = {english}
}

@article{gilesextended,
  title = {An Extended Collection of Matrix Derivative Results for Forward and Reverse Mode Algorithmic Differentiation},
  author = {Giles, Mike},
  pages = {23},
  file = {/home/philipp/Zotero/storage/PX639V8E/Giles - An extended collection of matrix derivative result.pdf},
  langid = {english}
}

@article{girolami2011riemann,
  title = {Riemann Manifold {{Langevin}} and {{Hamiltonian Monte Carlo}} Methods},
  author = {Girolami, Mark and Calderhead, Ben},
  date = {2011},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {73},
  pages = {123--214},
  issn = {1467-9868},
  doi = {10.1111/j.1467-9868.2010.00765.x},
  url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2010.00765.x},
  urldate = {2020-10-17},
  abstract = {Summary. The paper proposes Metropolis adjusted Langevin and Hamiltonian Monte Carlo sampling methods defined on the Riemann manifold to resolve the shortcomings of existing Monte Carlo algorithms when sampling from target densities that may be high dimensional and exhibit strong correlations. The methods provide fully automated adaptation mechanisms that circumvent the costly pilot runs that are required to tune proposal densities for Metropolis–Hastings or indeed Hamiltonian Monte Carlo and Metropolis adjusted Langevin algorithms. This allows for highly efficient sampling even in very high dimensions where different scalings may be required for the transient and stationary phases of the Markov chain. The methodology proposed exploits the Riemann geometry of the parameter space of statistical models and thus automatically adapts to the local structure when simulating paths across this manifold, providing highly efficient convergence and exploration of the target density. The performance of these Riemann manifold Monte Carlo methods is rigorously assessed by performing inference on logistic regression models, log-Gaussian Cox point processes, stochastic volatility models and Bayesian estimation of dynamic systems described by non-linear differential equations. Substantial improvements in the time-normalized effective sample size are reported when compared with alternative sampling approaches. MATLAB code that is available from http://www.ucl.ac.uk/statistics/research/rmhmc allows replication of all the results reported.},
  annotation = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2010.00765.x},
  file = {/home/philipp/Zotero/storage/VBU87IF4/Girolami_Calderhead_2011_Riemann manifold Langevin and Hamiltonian Monte Carlo methods.pdf;/home/philipp/Zotero/storage/EDMRBNC9/j.1467-9868.2010.00765.html},
  langid = {english},
  number = {2}
}

@online{goodman2012church,
  title = {Church: A Language for Generative Models},
  shorttitle = {Church},
  author = {Goodman, Noah D. and Mansinghka, Vikash and Roy, Daniel M. and Bonawitz, Keith and Tenenbaum, Joshua B.},
  date = {2012-06-13},
  url = {http://arxiv.org/abs/1206.3255},
  urldate = {2019-03-22},
  abstract = {We introduce Church, a universal language for describing stochastic generative processes. Church is based on the Lisp model of lambda calculus, containing a pure Lisp as its deterministic subset. The semantics of Church is defined in terms of evaluation histories and conditional distributions on such histories. Church also includes a novel language construct, the stochastic memoizer, which enables simple description of many complex non-parametric models. We illustrate language features through several examples, including: a generalized Bayes net in which parameters cluster over trials, infinite PCFGs, planning by inference, and various non-parametric clustering models. Finally, we show how to implement query on any Church program, exactly and approximately, using Monte Carlo techniques.},
  archivePrefix = {arXiv},
  eprint = {1206.3255},
  eprinttype = {arxiv},
  file = {/home/philipp/Zotero/storage/DYGF5DNS/Goodman et al_2012_Church.pdf;/home/philipp/Zotero/storage/C92LCFDC/1206.html},
  primaryClass = {cs}
}

@online{goodman2014design,
  title = {The {{Design}} and {{Implementation}} of {{Probabilistic Programming Languages}}},
  author = {Goodman, Noah D. and Stuhlmüller, Andreas},
  date = {2014},
  url = {http://dippl.org},
  urldate = {2019-10-15}
}

@article{gowda2019sparsity,
  title = {Sparsity {{Programming}}: {{Automated Sparsity}}-{{Aware Optimizations}} in {{Differentiable Programming}}},
  shorttitle = {Sparsity {{Programming}}},
  author = {Gowda, Shashi and Ma, Yingbo and Churavy, Valentin and Edelman, Alan and Rackauckas, Christopher},
  date = {2019-09-16},
  url = {https://openreview.net/forum?id=rJlPdcY38B},
  urldate = {2020-07-29},
  abstract = {Previous studies in numerical analysis have shown how the calculation of a Jacobian, Hessian, and their factorizations can be accelerated when their sparsity pattern is known. However, accurate...},
  file = {/home/philipp/Zotero/storage/FTEFLWGT/Gowda et al_2019_Sparsity Programming.pdf;/home/philipp/Zotero/storage/ZDXPHV3A/forum.html}
}

@article{green1995reversible,
  title = {Reversible Jump {{Markov}} Chain {{Monte Carlo}} Computation and {{Bayesian}} Model Determination},
  author = {Green, Peter J.},
  date = {1995-12-01},
  journaltitle = {Biometrika},
  shortjournal = {Biometrika},
  volume = {82},
  pages = {711--732},
  publisher = {{Oxford Academic}},
  issn = {0006-3444},
  doi = {10.1093/biomet/82.4.711},
  url = {https://academic.oup.com/biomet/article/82/4/711/252058},
  urldate = {2020-10-17},
  abstract = {Abstract.  Markov chain Monte Carlo methods for Bayesian computation have until recently been restricted to problems where the joint distribution of all variabl},
  file = {/home/philipp/Zotero/storage/SR4VKZNF/Green_1995_Reversible jump Markov chain Monte Carlo computation and Bayesian model.pdf;/home/philipp/Zotero/storage/IKUAKG7S/252058.html},
  langid = {english},
  number = {4}
}

@book{griewank2008evaluating,
  title = {Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation},
  shorttitle = {Evaluating Derivatives},
  author = {Griewank, Andreas and Walther, Andrea},
  date = {2008},
  edition = {2},
  publisher = {{Society for Industrial and Applied Mathematics}},
  location = {{Philadelphia}},
  abstract = {This title is a comprehensive treatment of algorithmic, or automatic, differentiation. The second edition covers recent developments in applications and theory, including an elegant NP completeness argument and an introduction to scarcity},
  file = {/home/philipp/Zotero/storage/NNVAAQ6S/Griewank and Walther - 2008 - Evaluating derivatives principles and techniques .pdf},
  langid = {english},
  pagetotal = {438}
}

@online{grosse2014testing,
  title = {Testing {{MCMC}} Code},
  author = {Grosse, Roger B. and Duvenaud, David K.},
  date = {2014-12-16},
  url = {http://arxiv.org/abs/1412.5218},
  urldate = {2020-02-14},
  abstract = {Markov Chain Monte Carlo (MCMC) algorithms are a workhorse of probabilistic modeling and inference, but are difficult to debug, and are prone to silent failure if implemented naively. We outline several strategies for testing the correctness of MCMC algorithms. Specifically, we advocate writing code in a modular way, where conditional probability calculations are kept separate from the logic of the sampler. We discuss strategies for both unit testing and integration testing. As a running example, we show how a Python implementation of Gibbs sampling for a mixture of Gaussians model can be tested.},
  archivePrefix = {arXiv},
  eprint = {1412.5218},
  eprinttype = {arxiv},
  file = {/home/philipp/Zotero/storage/I2FMJRWD/Grosse_Duvenaud_2014_Testing MCMC code.pdf;/home/philipp/Zotero/storage/H65LWZWT/1412.html},
  primaryClass = {cs, stat}
}

@online{heunen2017convenient,
  title = {A {{Convenient Category}} for {{Higher}}-{{Order Probability Theory}}},
  author = {Heunen, Chris and Kammar, Ohad and Staton, Sam and Yang, Hongseok},
  date = {2017-01-10},
  url = {http://arxiv.org/abs/1701.02547},
  urldate = {2019-03-14},
  abstract = {Higher-order probabilistic programming languages allow programmers to write sophisticated models in machine learning and statistics in a succinct and structured way, but step outside the standard measure-theoretic formalization of probability theory. Programs may use both higher-order functions and continuous distributions, or even define a probability distribution on functions. But standard probability theory does not handle higher-order functions well: the category of measurable spaces is not cartesian closed.},
  archivePrefix = {arXiv},
  eprint = {1701.02547},
  eprinttype = {arxiv},
  file = {/home/philipp/Zotero/storage/FVKDIYAV/Heunen et al. - 2017 - A Convenient Category for Higher-Order Probability.pdf},
  langid = {english},
  primaryClass = {cs, math}
}

@book{hjort2010bayesian,
  title = {Bayesian Nonparametrics},
  author = {Hjort, Nils Lid and Holmes, Chris and Müller, Peter and Walker, Stephen G.},
  date = {2010},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge}},
  file = {/home/philipp/Zotero/storage/9W33W2YB/Hjort et al_2010_Bayesian nonparametrics.pdf},
  number = {28},
  series = {Cambridge {{Series}} in {{Statistical}} and {{Probabilistic Mathematics}}}
}

@online{hoffman2018autoconj,
  title = {Autoconj: {{Recognizing}} and {{Exploiting Conjugacy Without}} a {{Domain}}-{{Specific Language}}},
  shorttitle = {Autoconj},
  author = {Hoffman, Matthew D. and Johnson, Matthew J. and Tran, Dustin},
  date = {2018-11-28},
  url = {http://arxiv.org/abs/1811.11926},
  urldate = {2019-10-09},
  abstract = {Deriving conditional and marginal distributions using conjugacy relationships can be time consuming and error prone. In this paper, we propose a strategy for automating such derivations. Unlike previous systems which focus on relationships between pairs of random variables, our system (which we call Autoconj) operates directly on Python functions that compute log-joint distribution functions. Autoconj provides support for conjugacy-exploiting algorithms in any Python embedded PPL. This paves the way for accelerating development of novel inference algorithms and structure-exploiting modeling strategies.},
  archivePrefix = {arXiv},
  eprint = {1811.11926},
  eprinttype = {arxiv},
  file = {/home/philipp/Zotero/storage/QGRVEU9T/Hoffman et al_2018_Autoconj.pdf;/home/philipp/Zotero/storage/BB9GHGZF/1811.html},
  primaryClass = {cs, stat}
}

@online{homea,
  title = {Home · {{Zygote}}},
  url = {http://fluxml.ai/Zygote.jl/latest/},
  urldate = {2019-04-30},
  file = {/home/philipp/Zotero/storage/NI88J7MG/latest.html}
}

@unpublished{hong2018graph,
  title = {Graph {{Program Extraction}} and {{Device Partitioning}} in {{Swift}} for {{TensorFLow}}},
  shorttitle = {2018 {{LLVM Developers}}' {{Meeting}}},
  author = {Hong, M. and Lattner, C.},
  date = {2018},
  url = {https://www.youtube.com/watch?v=HSneJdPkaKk},
  urldate = {2019-05-01},
  eventtitle = {2018 {{LLVM Developers}}' {{Meeting}}}
}

@online{innes2018don,
  title = {Don't {{Unroll Adjoint}}: {{Differentiating SSA}}-{{Form Programs}}},
  shorttitle = {Don't {{Unroll Adjoint}}},
  author = {Innes, Michael J.},
  date = {2018-10-18},
  url = {http://arxiv.org/abs/1810.07951},
  urldate = {2019-04-26},
  abstract = {This paper presents reverse-mode algorithmic differentiation (AD) based on source code transformation, in particular of the Static Single Assignment (SSA) form used by modern compilers. The approach can support control flow, nesting, mutation, recursion, data structures, higher-order functions, and other language constructs, and the output is given to an existing compiler to produce highly efficient differentiated code. Our implementation is a new AD tool for the Julia language, called Zygote, which presents high-level dynamic semantics while transparently compiling adjoint code under the hood. We discuss the benefits of this approach to both the usability and performance of AD tools.},
  archivePrefix = {arXiv},
  eprint = {1810.07951},
  eprinttype = {arxiv},
  file = {/home/philipp/Zotero/storage/GSIIGVKA/Innes_2018_Don't Unroll Adjoint.pdf;/home/philipp/Zotero/storage/ACK7LCZH/1810.html},
  primaryClass = {cs}
}

@article{innes2018flux,
  title = {Flux: {{Elegant}} Machine Learning with {{Julia}}},
  author = {Innes, Mike},
  date = {2018},
  journaltitle = {Journal of Open Source Software},
  doi = {10.21105/joss.00602},
  file = {/home/philipp/Zotero/storage/5DMNKATG/Innes_2018_Flux.pdf}
}

@online{innes2019differentiable,
  title = {A {{Differentiable Programming System}} to {{Bridge Machine Learning}} and {{Scientific Computing}}},
  author = {Innes, Mike and Edelman, Alan and Fischer, Keno and Rackauckas, Chris and Saba, Elliot and Shah, Viral B. and Tebbutt, Will},
  date = {2019-07-17},
  url = {http://arxiv.org/abs/1907.07587},
  urldate = {2019-08-09},
  abstract = {Scientific computing is increasingly incorporating the advancements in machine learning and the ability to work with large amounts of data. At the same time, machine learning models are becoming increasingly sophisticated and exhibit many features often seen in scientific computing, stressing the capabilities of machine learning frameworks. Just as the disciplines of scientific computing and machine learning have shared common underlying infrastructure in the form of numerical linear algebra, we now have the opportunity to further share new computational infrastructure, and thus ideas, in the form of Differentiable Programming. We describe Zygote, a Differentiable Programming system that is able to take gradients of general program structures. We implement this system in the Julia programming language. Our system supports almost all language constructs (control flow, recursion, mutation, etc.) and compiles high-performance code without requiring any user intervention or refactoring to stage computations. This enables an expressive programming model for deep learning, but more importantly, it enables us to incorporate a large ecosystem of libraries in our models in a straightforward way. We discuss our approach to automatic differentiation, including its support for advanced techniques such as mixed-mode, complex and checkpointed differentiation, and present several examples of differentiating programs.},
  archivePrefix = {arXiv},
  eprint = {1907.07587},
  eprinttype = {arxiv},
  file = {/home/philipp/Zotero/storage/B49FETJX/Innes et al_2019_A Differentiable Programming System to Bridge Machine Learning and Scientific.pdf;/home/philipp/Zotero/storage/ZBLI6RH8/1907.html},
  primaryClass = {cs}
}

@software{innes2019mikeinnes,
  title = {{{MikeInnes}}/Diff-Zoo},
  author = {Innes, Mike J.},
  date = {2019-11-22T13:20:43Z},
  origdate = {2018-11-01T15:04:59Z},
  url = {https://github.com/MikeInnes/diff-zoo},
  urldate = {2019-11-23},
  abstract = {Differentiation for Hackers. Contribute to MikeInnes/diff-zoo development by creating an account on GitHub.}
}

@software{invenia2019,
  title = {Invenia/{{Nabla}}.Jl},
  date = {2019-08-21T20:24:20Z},
  origdate = {2017-03-23T15:44:42Z},
  url = {https://github.com/invenia/Nabla.jl},
  urldate = {2019-10-11},
  abstract = {Contribute to invenia/Nabla.jl development by creating an account on GitHub.},
  organization = {{Invenia Technical Computing}}
}

@article{jordan1999introduction,
  title = {An Introduction to Variational Methods for Graphical Models},
  author = {Jordan, Michael I. and Ghahramani, Zoubin and Jaakkola, Tommi S. and Saul, Lawrence K.},
  date = {1999},
  journaltitle = {Machine learning},
  volume = {37},
  pages = {183--233},
  doi = {10.1023/A:1007665907178},
  number = {2}
}

@online{juliaproject2019julia,
  title = {Julia 1.2 {{Documentation}}},
  author = {{Julia Project}},
  date = {2019-08-20},
  url = {https://docs.julialang.org/en/v1.2/index.html},
  urldate = {2019-10-09}
}

@article{kelsey1995correspondence,
  title = {A Correspondence between Continuation Passing Style and Static Single Assignment Form},
  author = {Kelsey, Richard A.},
  date = {1995-03-01},
  journaltitle = {ACM SIGPLAN Notices},
  shortjournal = {SIGPLAN Not.},
  volume = {30},
  pages = {13--22},
  issn = {0362-1340},
  doi = {10.1145/202530.202532},
  url = {https://doi.org/10.1145/202530.202532},
  urldate = {2020-07-23},
  abstract = {We define syntactic transformations that convert continuation passing style (CPS) programs into static single assignment form (SSA) and vice versa. Some CPS programs cannot be converted to SSA, but these are not produced by the usual CPS transformation. The CPS→SSA transformation is especially helpful for compiling functional programs. Many optimizations that normally require flow analysis can be performed directly on functional CPS programs by viewing them as SSA programs. We also present a simple program transformation that merges CPS procedures together and by doing so greatly increases the scope of the SSA flow information. This transformation is useful for analyzing loops expressed as recursive procedures.},
  file = {/home/philipp/Zotero/storage/M57YS4MD/Kelsey_1995_A correspondence between continuation passing style and static single.pdf},
  number = {3}
}

@book{koller2009probabilistic,
  title = {Probabilistic Graphical Models: Principles and Techniques},
  shorttitle = {Probabilistic Graphical Models},
  author = {Koller, Daphne and Friedman, Nir},
  date = {2009},
  publisher = {{MIT Press}},
  location = {{Cambridge}},
  file = {/home/philipp/Zotero/storage/CN4X37SD/Koller and Friedman - 2009 - Probabilistic graphical models principles and tec.pdf},
  langid = {english},
  pagetotal = {1231},
  series = {Adaptive Computation and Machine Learning}
}

@inproceedings{kulkarni2015picture,
  title = {Picture: {{A Probabilistic Programming Language}} for {{Scene Perception}}},
  shorttitle = {Picture},
  author = {Kulkarni, Tejas D. and Kohli, Pushmeet and Tenenbaum, Joshua B. and Mansinghka, Vikash},
  date = {2015},
  pages = {4390--4399},
  url = {http://openaccess.thecvf.com/content_cvpr_2015/html/Kulkarni_Picture_A_Probabilistic_2015_CVPR_paper.html},
  urldate = {2019-03-22},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {/home/philipp/Zotero/storage/RZAMR9ES/Kulkarni et al_2015_Picture.pdf;/home/philipp/Zotero/storage/L7BIASBI/Kulkarni_Picture_A_Probabilistic_2015_CVPR_paper.html}
}

@inproceedings{lam2015numba,
  title = {Numba: A {{LLVM}}-Based {{Python JIT}} Compiler},
  shorttitle = {Numba},
  booktitle = {Proceedings of the {{Second Workshop}} on the {{LLVM Compiler Infrastructure}} in {{HPC}}},
  author = {Lam, Siu Kwan and Pitrou, Antoine and Seibert, Stanley},
  date = {2015-11-15},
  pages = {1--6},
  publisher = {{Association for Computing Machinery}},
  location = {{Austin, Texas}},
  doi = {10.1145/2833157.2833162},
  url = {https://doi.org/10.1145/2833157.2833162},
  urldate = {2020-07-28},
  abstract = {Dynamic, interpreted languages, like Python, are attractive for domain-experts and scientists experimenting with new ideas. However, the performance of the interpreter is often a barrier when scaling to larger data sets. This paper presents a just-in-time compiler for Python that focuses in scientific and array-oriented computing. Starting with the simple syntax of Python, Numba compiles a subset of the language into efficient machine code that is comparable in performance to a traditional compiled language. In addition, we share our experience in building a JIT compiler using LLVM[1].},
  file = {/home/philipp/Zotero/storage/BAV8URGH/Lam et al_2015_Numba.pdf},
  isbn = {978-1-4503-4005-2},
  series = {{{LLVM}} '15}
}

@online{laue2019equivalence,
  title = {On the {{Equivalence}} of {{Forward Mode Automatic Differentiation}} and {{Symbolic Differentiation}}},
  author = {Laue, Soeren},
  date = {2019-05-03},
  url = {http://arxiv.org/abs/1904.02990},
  urldate = {2019-10-24},
  abstract = {We show that forward mode automatic differentiation and symbolic differentiation are equivalent in the sense that they both perform the same operations when computing derivatives. This is in stark contrast to the common claim that they are substantially different. The difference is often illustrated by claiming that symbolic differentiation suffers from “expression swell” whereas automatic differentiation does not. Here, we show that this statement is not true. “Expression swell” refers to the phenomenon of a much larger representation of the derivative as opposed to the representation of the original function.},
  archivePrefix = {arXiv},
  eprint = {1904.02990},
  eprinttype = {arxiv},
  file = {/home/philipp/Zotero/storage/8S7QB33X/Laue - 2019 - On the Equivalence of Forward Mode Automatic Diffe.pdf},
  langid = {english},
  primaryClass = {cs}
}

@article{lew2020trace,
  title = {Trace Types and Denotational Semantics for Sound Programmable Inference in Probabilistic Languages},
  author = {Lew, Alexander K. and Cusumano-Towner, Marco F. and Sherman, Benjamin and Carbin, Michael and Mansinghka, Vikash K.},
  date = {2020-01},
  journaltitle = {Proceedings of the ACM on Programming Languages},
  volume = {4},
  pages = {1--32},
  issn = {2475-1421, 2475-1421},
  doi = {10.1145/3371087},
  url = {https://dl.acm.org/doi/10.1145/3371087},
  urldate = {2020-04-21},
  abstract = {CCS Concepts: • Mathematics of computing → Probabilistic inference problems; Variational methods; Metropolis-Hastings algorithm; Sequential Monte Carlo methods; • Theory of computation → Semantics and reasoning; Denotational semantics; • Software and its engineering → Formal language definitions.},
  file = {/home/philipp/Zotero/storage/ML85N6AX/Lew et al. - 2020 - Trace types and denotational semantics for sound p.pdf},
  issue = {POPL},
  langid = {english}
}

@online{llvmproject2019llvm,
  title = {{{LLVM Language Reference Manual}}},
  author = {{LLVM Project}},
  date = {2019-10-09},
  url = {https://llvm.org/docs/LangRef.html}
}

@online{looks2017deep,
  title = {Deep {{Learning}} with {{Dynamic Computation Graphs}}},
  author = {Looks, Moshe and Herreshoff, Marcello and Hutchins, DeLesley and Norvig, Peter},
  date = {2017-02-07},
  url = {http://arxiv.org/abs/1702.02181},
  urldate = {2019-03-18},
  abstract = {Neural networks that compute over graph structures are a natural fit for problems in a variety of domains, including natural language (parse trees) and cheminformatics (molecular graphs). However, since the computation graph has a different shape and size for every input, such networks do not directly support batched training or inference. They are also difficult to implement in popular deep learning libraries, which are based on static data-flow graphs. We introduce a technique called dynamic batching, which not only batches together operations between different input graphs of dissimilar shape, but also between different nodes within a single input graph. The technique allows us to create static graphs, using popular libraries, that emulate dynamic computation graphs of arbitrary shape and size. We further present a high-level library of compositional blocks that simplifies the creation of dynamic graph models. Using the library, we demonstrate concise and batch-wise parallel implementations for a variety of models from the literature.},
  archivePrefix = {arXiv},
  eprint = {1702.02181},
  eprinttype = {arxiv},
  file = {/home/philipp/Zotero/storage/Y6URG2XS/Looks et al_2017_Deep Learning with Dynamic Computation Graphs.pdf;/home/philipp/Zotero/storage/LWVG7WAI/1702.html},
  primaryClass = {cs, stat}
}

@article{lunn2000winbugs,
  title = {{{WinBUGS}} - {{A Bayesian}} Modelling Framework: {{Concepts}}, Structure, and Extensibility},
  author = {Lunn, David J. and Thomas, Andrew and Best, Nicky and Spiegelhalter, David},
  date = {2000},
  journaltitle = {Statistics and Computing},
  volume = {10},
  pages = {325--337},
  doi = {10.1023/A:1008929526011},
  abstract = {WinBUGS is a fully extensible modular framework for constructing and analysing Bayesian full probability models. Models may be specified either textually via the BUGS language or pictorially using a graphical interface called DoodleBUGS. WinBUGS processes the model specification and constructs an object-oriented representation of the model. The software offers a user-interface, based on dialogue boxes and menu commands, through which the model may then be analysed using Markov chain Monte Carlo techniques. In this paper we discuss how and why various modern computing concepts, such as object-orientation and run-time linking, feature in the software’s design. We also discuss how the framework may be extended. It is possible to write specific applications that form an apparently seamless interface with WinBUGS for users with specialized requirements. It is also possible to interface with WinBUGS at a lower level by incorporating new object types that may be used by WinBUGS without knowledge of the modules in which they are implemented. Neither of these types of extension require access to, or even recompilation of, the WinBUGS source-code.},
  file = {/home/philipp/Zotero/storage/77M9WPQA/Lunn et al. - WinBUGS - A Bayesian modelling framework Concepts.pdf},
  langid = {english}
}

@online{luo2018implement,
  title = {Implement {{Your Own Automatic Differentiation}} with {{Julia}} in {{ONE}} Day},
  author = {Luo, Roger},
  date = {2018-10-23},
  journaltitle = {Half Integer},
  url = {http://rogerluo.me/blog/2018/10/23/write-an-ad-in-one-day/index.html},
  urldate = {2019-12-30},
  abstract = {I was playing with AutoGrad.jl and Zygote.jl, they both look awesome, and AutoGrad.jl has already been applied to the machine learning framework in Julia: Knet.jl. When I tried to read the source code},
  file = {/home/philipp/Zotero/storage/QVSF46UN/write-an-ad-in-one-day.html}
}

@online{luo2019implement,
  title = {Implement {{Your Own Source To Source AD}} in {{ONE}} Day!},
  author = {Luo, Roger},
  date = {2019-07-31},
  journaltitle = {Half Integer},
  url = {http://rogerluo.me/blog/2019/07/27/yassad/index.html},
  urldate = {2019-12-30},
  abstract = {I wrote a blog post about how to implement your own (operator overloading based) automatic differentiation (AD) in one day (actually 3 hrs) last year. AD looks like magic sometimes, but I'm going to t},
  file = {/home/philipp/Zotero/storage/INQWEZXX/yassad.html}
}

@online{mansinghka2014venture,
  title = {Venture: A Higher-Order Probabilistic Programming Platform with Programmable Inference},
  shorttitle = {Venture},
  author = {Mansinghka, Vikash and Selsam, Daniel and Perov, Yura},
  date = {2014-03-31},
  url = {http://arxiv.org/abs/1404.0099},
  urldate = {2019-09-09},
  abstract = {We describe Venture, an interactive virtual machine for probabilistic programming that aims to be sufficiently expressive, extensible, and efficient for general-purpose use. Like Church, probabilistic models and inference problems in Venture are specified via a Turing-complete, higher-order probabilistic language descended from Lisp. Unlike Church, Venture also provides a compositional language for custom inference strategies built out of scalable exact and approximate techniques. We also describe four key aspects of Venture's implementation that build on ideas from probabilistic graphical models. First, we describe the stochastic procedure interface (SPI) that specifies and encapsulates primitive random variables. The SPI supports custom control flow, higher-order probabilistic procedures, partially exchangeable sequences and ``likelihood-free'' stochastic simulators. It also supports external models that do inference over latent variables hidden from Venture. Second, we describe probabilistic execution traces (PETs), which represent execution histories of Venture programs. PETs capture conditional dependencies, existential dependencies and exchangeable coupling. Third, we describe partitions of execution histories called scaffolds that factor global inference problems into coherent sub-problems. Finally, we describe a family of stochastic regeneration algorithms for efficiently modifying PET fragments contained within scaffolds. Stochastic regeneration linear runtime scaling in cases where many previous approaches scaled quadratically. We show how to use stochastic regeneration and the SPI to implement general-purpose inference strategies such as Metropolis-Hastings, Gibbs sampling, and blocked proposals based on particle Markov chain Monte Carlo and mean-field variational inference techniques.},
  archivePrefix = {arXiv},
  eprint = {1404.0099},
  eprinttype = {arxiv},
  file = {/home/philipp/Zotero/storage/4HV22SYQ/Mansinghka et al_2014_Venture.pdf;/home/philipp/Zotero/storage/V8U7QF5W/1404.html},
  primaryClass = {cs, stat}
}

@book{marin2007bayesian,
  title = {Bayesian Core: A Practical Approach to Computational {{Bayesian}} Statistics},
  shorttitle = {Bayesian Core},
  author = {Marin, Jean-Michel and Robert, Christian P.},
  date = {2007},
  publisher = {{Springer}},
  location = {{New York}},
  file = {/home/philipp/Zotero/storage/M59PIE92/Marin_Robert_2007_Bayesian core.pdf},
  isbn = {978-0-387-38979-0 978-0-387-38983-7},
  pagetotal = {255},
  series = {Springer Texts in Statistics}
}

@software{mauro2019simple,
  title = {Simple {{Traits}} for {{Julia}}. {{Contribute}} to Mauro3/{{SimpleTraits}}.Jl Development by Creating an Account on {{GitHub}}},
  author = {Mauro},
  date = {2019-05-23T14:55:04Z},
  origdate = {2015-06-12T20:41:40Z},
  url = {https://github.com/mauro3/SimpleTraits.jl},
  urldate = {2019-05-28}
}

@inproceedings{minka2001expectation,
  title = {Expectation {{Propagation}} for {{Approximate Bayesian Inference}}},
  booktitle = {{{UAI}}'01: {{Proceedings}} of the {{Seventeenth}} Conference on {{Uncertainty}} in Artificial Intelligence},
  author = {Minka, Thomas P.},
  date = {2001},
  location = {{Seattle}},
  abstract = {This paper presents a new deterministic approximation technique in Bayesian networks. This method, “Expectation Propagation,” unifies two previous techniques: assumed-density filtering, an extension of the Kalman filter, and loopy belief propagation, an extension of belief propagation in Bayesian networks. Loopy belief propagation, because it propagates exact belief states, is useful for a limited class of belief networks, such as those which are purely discrete. Expectation Propagation approximates the belief states by only retaining expectations, such as mean and variance, and iterates until these expectations are consistent throughout the network. This makes it applicable to hybrid networks with discrete and continuous nodes. Experiments with Gaussian mixture models show Expectation Propagation to be convincingly better than methods with similar computational cost: Laplace’s method, variational Bayes, and Monte Carlo. Expectation Propagation also provides an efficient algorithm for training Bayes point machine classifiers.},
  file = {/home/philipp/Zotero/storage/HH52WAS7/Minka - Expectation Propagation for Approximate Bayesian I.pdf},
  langid = {english}
}

@report{minka2005divergence,
  title = {Divergence {{Measures}} and {{Message Passing}}},
  author = {Minka, Thomas},
  date = {2005},
  institution = {{Microsoft Research}},
  url = {https://www.microsoft.com/en-us/research/publication/divergence-measures-and-message-passing/},
  urldate = {2019-10-09},
  abstract = {This paper presents a unifying view of message-passing algorithms, as methods to approximate a complex Bayesian network by a simpler network with minimum information divergence. In this view, the difference between mean-field methods and belief propagation is not the amount of structure they model, but only the measure of loss they minimize (`exclusive’ versus `inclusive’ …},
  file = {/home/philipp/Zotero/storage/V86L6PK4/Minka - Divergence measures and message passing.pdf},
  langid = {american},
  number = {MSR-TR-2005-173},
  type = {Technical Report}
}

@online{minka2018infer,
  title = {Infer.{{NET}} 0.3},
  author = {Minka, Tom and Winn, John and Guiver, John and Zaykov, Yordan and Fabian, Dany and Bronskill, John},
  date = {2018},
  url = {http://dotnet.github.io/infer},
  urldate = {2019-10-14},
  annotation = {Microsoft Research Cambridge.}
}

@report{minka2019automatic,
  title = {From Automatic Differentiation to Message Passing},
  author = {Minka, Tom},
  date = {2019-06-07},
  location = {{Advances and challenges in Machine Learning Languages workshop}},
  url = {https://www.microsoft.com/en-us/research/video/from-automatic-differentiation-to-message-passing/},
  urldate = {2019-09-03},
  abstract = {Automatic differentiation is an elegant technique for converting a computable function expressed as a program into a derivative-computing program with similar time complexity. It does not execute the original program as a black-box, nor does it expand the program into a mathematical formula, both of which would be counter-productive. By generalizing this technique, you can …},
  file = {/home/philipp/Zotero/storage/FJFTFGF9/From automatic differentiation to message passing.pdf;/home/philipp/Zotero/storage/JMAXU5LR/from-automatic-differentiation-to-message-passing.html},
  langid = {american},
  type = {Presentation}
}

@book{muchnick1997advanced,
  title = {Advanced Compiler Design and Implementation},
  author = {Muchnick, Steven S.},
  date = {1997},
  publisher = {{Morgan Kaufmann}},
  location = {{San Francisco}},
  file = {/home/philipp/Zotero/storage/RHKSTFEJ/Muchnick - 1997 - Advanced compiler design and implementation.pdf},
  langid = {english},
  pagetotal = {856}
}

@book{murphy2012machine,
  title = {Machine Learning: A Probabilistic Perspective},
  shorttitle = {Machine Learning},
  author = {Murphy, Kevin P.},
  date = {2012},
  publisher = {{MIT Press}},
  location = {{Cambridge, MA}},
  file = {/home/philipp/Zotero/storage/88WAUMU6/Murphy - 2012 - Machine learning a probabilistic perspective.pdf},
  isbn = {978-0-262-01802-9},
  pagetotal = {1067},
  series = {Adaptive Computation and Machine Learning Series}
}

@online{murray2017delayed,
  title = {Delayed {{Sampling}} and {{Automatic Rao}}-{{Blackwellization}} of {{Probabilistic Programs}}},
  author = {Murray, Lawrence M. and Lundén, Daniel and Kudlicka, Jan and Broman, David and Schön, Thomas B.},
  date = {2017-08-25},
  url = {http://arxiv.org/abs/1708.07787},
  urldate = {2019-10-09},
  abstract = {We introduce a dynamic mechanism for the solution of analytically-tractable substructure in probabilistic programs, using conjugate priors and affine transformations to reduce variance in Monte Carlo estimators. For inference with Sequential Monte Carlo, this automatically yields improvements such as locally-optimal proposals and Rao-Blackwellization. The mechanism maintains a directed graph alongside the running program that evolves dynamically as operations are triggered upon it. Nodes of the graph represent random variables, edges the analytically-tractable relationships between them. Random variables remain in the graph for as long as possible, to be sampled only when they are used by the program in a way that cannot be resolved analytically. In the meantime, they are conditioned on as many observations as possible. We demonstrate the mechanism with a few pedagogical examples, as well as a linear-nonlinear state-space model with simulated data, and an epidemiological model with real data of a dengue outbreak in Micronesia. In all cases one or more variables are automatically marginalized out to significantly reduce variance in estimates of the marginal likelihood, in the final case facilitating a random-weight or pseudo-marginal-type importance sampler for parameter estimation. We have implemented the approach in Anglican and a new probabilistic programming language called Birch.},
  archivePrefix = {arXiv},
  eprint = {1708.07787},
  eprinttype = {arxiv},
  file = {/home/philipp/Zotero/storage/3V5B92ZF/Murray et al_2017_Delayed Sampling and Automatic Rao-Blackwellization of Probabilistic Programs.pdf;/home/philipp/Zotero/storage/NZ4NCBYD/1708.html},
  primaryClass = {stat}
}

@incollection{narayanan2016probabilistic,
  title = {Probabilistic {{Inference}} by {{Program Transformation}} in {{Hakaru}} ({{System Description}})},
  booktitle = {Functional and {{Logic Programming}}},
  author = {Narayanan, Praveen and Carette, Jacques and Romano, Wren and Shan, Chung-chieh and Zinkov, Robert},
  editor = {Kiselyov, Oleg and King, Andy},
  date = {2016},
  volume = {9613},
  pages = {62--79},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-29604-3_5},
  url = {http://link.springer.com/10.1007/978-3-319-29604-3_5},
  urldate = {2020-10-02},
  abstract = {We present Hakaru, a new probabilistic programming system that allows composable reuse of distributions, queries, and inference algorithms, all expressed in a single language of measures. The system implements two automatic and semantics-preserving program transformations—disintegration, which calculates conditional distributions, and simplification, which subsumes exact inference by computer algebra. We show how these features work together by describing the ideal workflow of a Hakaru user on two small problems. We highlight our composition of transformations and types in design and implementation.},
  file = {/home/philipp/Zotero/storage/YLWJLJYN/Narayanan et al. - 2016 - Probabilistic Inference by Program Transformation .pdf},
  isbn = {978-3-319-29603-6 978-3-319-29604-3},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{narayanan2020symbolic,
  title = {Symbolic Disintegration with a Variety of Base Measures},
  author = {Narayanan, Praveen and Shan, Chung-chieh},
  date = {2020-05},
  journaltitle = {ACM Transactions on Programming Languages and Systems},
  shortjournal = {ACM Trans. Program. Lang. Syst.},
  volume = {42},
  publisher = {{Association for Computing Machinery}},
  address = {New York, NY, USA},
  issn = {0164-0925},
  doi = {10.1145/3374208},
  url = {https://doi.org/10.1145/3374208},
  articleno = {9},
  file = {/home/philipp/Zotero/storage/PBQK6IEQ/Narayanan and Shan - Symbolic disintegration with a variety of base mea.pdf},
  issue_date = {May 2020},
  keywords = {conditional distributions,density functions,measure kernels,Probabilistic programs},
  number = {2},
  pagetotal = {60}
}

@online{neubig2017dynet,
  title = {{{DyNet}}: {{The Dynamic Neural Network Toolkit}}},
  shorttitle = {{{DyNet}}},
  author = {Neubig, Graham and Dyer, Chris and Goldberg, Yoav and Matthews, Austin and Ammar, Waleed and Anastasopoulos, Antonios and Ballesteros, Miguel and Chiang, David and Clothiaux, Daniel and Cohn, Trevor and Duh, Kevin and Faruqui, Manaal and Gan, Cynthia and Garrette, Dan and Ji, Yangfeng and Kong, Lingpeng and Kuncoro, Adhiguna and Kumar, Gaurav and Malaviya, Chaitanya and Michel, Paul and Oda, Yusuke and Richardson, Matthew and Saphra, Naomi and Swayamdipta, Swabha and Yin, Pengcheng},
  date = {2017-01-14},
  url = {http://arxiv.org/abs/1701.03980},
  urldate = {2019-03-08},
  abstract = {We describe DyNet, a toolkit for implementing neural network models based on dynamic declaration of network structure. In the static declaration strategy that is used in toolkits like Theano, CNTK, and TensorFlow, the user first defines a computation graph (a symbolic representation of the computation), and then examples are fed into an engine that executes this computation and computes its derivatives. In DyNet’s dynamic declaration strategy, computation graph construction is mostly transparent, being implicitly constructed by executing procedural code that computes the network outputs, and the user is free to use different network structures for each input. Dynamic declaration thus facilitates the implementation of more complicated network architectures, and DyNet is specifically designed to allow users to implement their models in a way that is idiomatic in their preferred programming language (C++ or Python). One challenge with dynamic declaration is that because the symbolic computation graph is defined anew for every training example, its construction must have low overhead. To achieve this, DyNet has an optimized C++ backend and lightweight graph representation. Experiments show that DyNet’s speeds are faster than or comparable with static declaration toolkits, and significantly faster than Chainer, another dynamic declaration toolkit. DyNet is released opensource under the Apache 2.0 license, and available at http://github.com/clab/dynet.},
  archivePrefix = {arXiv},
  eprint = {1701.03980},
  eprinttype = {arxiv},
  file = {/home/philipp/Zotero/storage/NSGI2F8Y/Neubig et al. - 2017 - DyNet The Dynamic Neural Network Toolkit.pdf},
  langid = {english},
  primaryClass = {cs, stat}
}

@online{openppl,
  title = {{{OpenPPL}}: {{A Proposal}} for {{Probabilistic Programming}} in {{Julia}}},
  url = {https://openpp-proposal.readthedocs.io/en/latest/index.html},
  urldate = {2019-03-01},
  file = {/home/philipp/Zotero/storage/E8ISV6IQ/index.html}
}

@report{orbanz2014lecture,
  title = {Lecture {{Notes}} on {{Bayesian Nonparametrics}}},
  author = {Orbanz, Peter},
  date = {2014},
  pages = {108},
  institution = {{Columbia University}},
  file = {/home/philipp/Zotero/storage/UWFFEIUE/Orbanz - Lecture Notes on Bayesian Nonparametrics.pdf},
  langid = {english}
}

@inproceedings{paszke2017automatic,
  title = {Automatic Differentiation in {{PyTorch}}},
  author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  date = {2017},
  abstract = {In this article, we describe an automatic differentiation module of PyTorch — a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua Torch, Chainer, and HIPS Autograd [4], and provides a high performance environment with easy access to automatic differentiation of models executed on different devices (CPU and GPU). To make prototyping easier, PyTorch does not follow the symbolic approach used in many other deep learning frameworks, but focuses on differentiation of purely imperative programs, with a focus on extensibility and low overhead. Note that this preprint is a draft of certain sections from an upcoming paper covering all PyTorch features.},
  eventtitle = {{{NIPS}} 2017 {{Workshop Autodiff}}},
  file = {/home/philipp/Zotero/storage/RQL9ZJYC/Paszke et al. - Automatic differentiation in PyTorch.pdf},
  langid = {english}
}

@article{pearlmutter2008reversemode,
  ids = {pearlmutterreversemode},
  title = {Reverse-{{Mode AD}} in a {{Functional Framework}}: {{Lambda}} the {{Ultimate Backpropagator}}},
  author = {Pearlmutter, Barak A. and Siskind, Jeffrey Mark},
  date = {2008-03},
  journaltitle = {ACM Trans. Program. Lang. Syst.},
  volume = {30},
  pages = {7:1-7:36},
  doi = {10.1145/1330017.1330018},
  url = {http://doi.acm.org/10.1145/1330017.1330018},
  file = {/home/philipp/Zotero/storage/39L26C2H/Pearlmutter - Reverse-Mode AD in a Functional Framework Lambda .pdf},
  keywords = {Closures,derivatives,forward-mode AD,higher-order AD,higher-order functional languages,Jacobian,program transformation,reflection},
  number = {2}
}

@unpublished{peytonjones2019automatic,
  title = {Automatic Differentiation for Dummies},
  author = {Peyton Jones, Simon},
  date = {2019-01-22},
  url = {https://tube.switch.ch/cast/videos/c56685cb-9eca-4db8-b071-eb9a97ecbc3d},
  urldate = {2019-03-19},
  eventtitle = {{{IC Colloquium}}},
  venue = {{EPFL}}
}

@inproceedings{plummer2003jags,
  title = {{{JAGS}}: {{A Program}} for {{Analysis}} of {{Bayesian Graphical Models Using Gibbs Sampling}}},
  booktitle = {Proceedings of the 3rd {{International Workshop}} on {{Distributed Statistical Computing}} ({{DSC}} 2003)},
  author = {Plummer, Martyn},
  date = {2003},
  location = {{Vienna}},
  abstract = {JAGS is a program for Bayesian Graphical modelling which aims for compatibility with classic BUGS. The program could eventually be developed as an R package. This article explains the motivations for this program, briefly describes the architecture and then discusses some ideas for a vectorized form of the BUGS language.},
  file = {/home/philipp/Zotero/storage/PSRFLNS6/Plummer - 2003 - JAGS A Program for Analysis of Bayesian Graphical.pdf},
  langid = {english}
}

@unpublished{plummer2017jags,
  title = {{{JAGS Version}} 4.3.0 User Manual},
  author = {Plummer, Martyn},
  date = {2017},
  url = {https://sourceforge.net/projects/mcmc-jags/files/Manuals/4.x/jags_user_manual.pdf/download},
  urldate = {2020-07-17},
  file = {/home/philipp/Zotero/storage/XIKA757A/jags_user_manual.pdf}
}

@online{radul2013introduction,
  title = {Introduction to {{Automatic Differentiation}}},
  author = {Radul, Alexey},
  date = {2013},
  journaltitle = {Conversations},
  url = {https://alexey.radul.name/ideas/2013/introduction-to-automatic-differentiation/},
  annotation = {Archive-URL: https://web.archive.org/web/20190405034941/https://alexey.radul.name/ideas/2013/introduction-to-automatic-differentiation/}
}

@article{ramsey2002stochastic,
  title = {Stochastic {{Lambda Calculus}} and {{Monads}} of {{Probability Distributions}}},
  author = {Ramsey, Norman and Pfeffer, Avi},
  date = {2002-01},
  journaltitle = {SIGPLAN Not.},
  volume = {37},
  pages = {154--165},
  issn = {0362-1340},
  doi = {10.1145/565816.503288},
  url = {http://doi.acm.org/10.1145/565816.503288},
  file = {/home/philipp/Zotero/storage/C3QEG8EN/Ramsey-Pfeffer-2002-Stochastic_Lambda_Calculus_and_Monads_of_Probability_Distributions.pdf},
  keywords = {haskell,lambda calculus,monads,probability theory,type theory},
  langid = {english},
  number = {1}
}

@online{revels2018dynamic,
  title = {Dynamic {{Automatic Differentiation}} of {{GPU Broadcast Kernels}}},
  author = {Revels, Jarrett and Besard, Tim and Churavy, Valentin and De Sutter, Bjorn and Vielma, Juan Pablo},
  date = {2018-10-18},
  url = {http://arxiv.org/abs/1810.08297},
  urldate = {2019-05-17},
  abstract = {We show how forward-mode automatic differentiation (AD) can be employed within larger reverse-mode computations to dynamically differentiate broadcast operations in a GPU-friendly manner. Our technique fully exploits the broadcast Jacobian’s inherent sparsity structure, and unlike a pure reverse-mode approach, this “mixed-mode” approach does not require a backwards pass over the broadcasted operation’s subgraph, obviating the need for several reverse-mode-specific programmability restrictions on user-authored broadcast operations. Most notably, this approach allows broadcast fusion in primal code despite the presence of datadependent control flow. We discuss an experiment in which a Julia implementation of our technique outperformed pure reverse-mode TensorFlow and Julia implementations for differentiating through broadcast operations within an HM-LSTM cell update calculation.},
  archivePrefix = {arXiv},
  eprint = {1810.08297},
  eprinttype = {arxiv},
  file = {/home/philipp/Zotero/storage/D8J3RZI4/Revels et al. - 2018 - Dynamic Automatic Differentiation of GPU Broadcast.pdf},
  langid = {english},
  primaryClass = {cs}
}

@book{robert1999monte,
  title = {Monte {{Carlo Statistical Methods}}},
  author = {Robert, Christian P. and Casella, George},
  date = {1999},
  publisher = {{Springer}},
  location = {{New York}},
  file = {/home/philipp/Zotero/storage/S8ECC634/Monte-Carlo-Statistical-Methods.pdf}
}

@article{rompf2010lightweight,
  ids = {rompflightweight},
  title = {Lightweight Modular Staging: {{A}} Pragmatic Approach to Runtime Code Generation and Compiled {{DSLs}}},
  author = {Rompf, Tiark and Odersky, Martin},
  date = {2010-10},
  journaltitle = {ACM SIGPLAN Notices},
  volume = {46},
  doi = {10.1145/1868294.1868314},
  file = {/home/philipp/Zotero/storage/LG2XRVI5/Rompf and Odersky - Lightweight Modular Staging A Pragmatic Approach .pdf}
}

@online{ruffwind2016reversemode,
  title = {Reverse-Mode Automatic Differentiation: A Tutorial},
  author = {Ruffwind, Phil},
  date = {2016-12-30},
  journaltitle = {Rufflewind's Scratchpad},
  url = {https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation},
  urldate = {2019-04-20},
  file = {/home/philipp/Zotero/storage/LNDNUD75/reverse-mode-automatic-differentiation.html}
}

@thesis{ruozzi2011message,
  title = {Message {{Passing Algorithms}} for {{Optimization}}},
  author = {Ruozzi, Nicholas Robert},
  date = {2011},
  institution = {{Yale University}},
  file = {/home/philipp/Zotero/storage/GF9DTUG3/Ruozzi - Message Passing Algorithms for Optimization.pdf},
  langid = {english},
  type = {Dissertation}
}

@online{schulman2016gradient,
  title = {Gradient {{Estimation Using Stochastic Computation Graphs}}},
  author = {Schulman, John and Heess, Nicolas and Weber, Theophane and Abbeel, Pieter},
  date = {2016-01-05},
  url = {http://arxiv.org/abs/1506.05254},
  urldate = {2020-09-20},
  abstract = {In a variety of problems originating in supervised, unsupervised, and reinforcement learning, the loss function is defined by an expectation over a collection of random variables, which might be part of a probabilistic model or the external world. Estimating the gradient of this loss function, using samples, lies at the core of gradient-based learning algorithms for these problems. We introduce the formalism of stochastic computation graphs---directed acyclic graphs that include both deterministic functions and conditional probability distributions---and describe how to easily and automatically derive an unbiased estimator of the loss function's gradient. The resulting algorithm for computing the gradient estimator is a simple modification of the standard backpropagation algorithm. The generic scheme we propose unifies estimators derived in variety of prior work, along with variance-reduction techniques therein. It could assist researchers in developing intricate models involving a combination of stochastic and deterministic operations, enabling, for example, attention, memory, and control actions.},
  archivePrefix = {arXiv},
  eprint = {1506.05254},
  eprinttype = {arxiv},
  file = {/home/philipp/Zotero/storage/8E89B845/Schulman et al_2016_Gradient Estimation Using Stochastic Computation Graphs.pdf;/home/philipp/Zotero/storage/SM6EZRXV/1506.html},
  primaryClass = {cs}
}

@inproceedings{scibior2015practical,
  title = {Practical Probabilistic Programming with Monads},
  booktitle = {Proceedings of the 8th {{ACM SIGPLAN Symposium}} on {{Haskell}} - {{Haskell}} 2015},
  author = {Ścibior, Adam and Ghahramani, Zoubin and Gordon, Andrew D.},
  date = {2015},
  pages = {165--176},
  publisher = {{ACM Press}},
  location = {{Vancouver, BC, Canada}},
  doi = {10.1145/2804302.2804317},
  url = {http://dl.acm.org/citation.cfm?doid=2804302.2804317},
  urldate = {2020-07-23},
  abstract = {The machine learning community has recently shown a lot of interest in practical probabilistic programming systems that target the problem of Bayesian inference. Such systems come in different forms, but they all express probabilistic models as computational processes using syntax resembling programming languages. In the functional programming community monads are known to offer a convenient and elegant abstraction for programming with probability distributions, but their use is often limited to very simple inference problems. We show that it is possible to use the monad abstraction to construct probabilistic models for machine learning, while still offering good performance of inference in challenging models. We use a GADT as an underlying representation of a probability distribution and apply Sequential Monte Carlo-based methods to achieve efficient inference. We define a formal semantics via measure theory. We demonstrate a clean and elegant implementation that achieves performance comparable with Anglican, a stateof-the-art probabilistic programming system.},
  eventtitle = {The 8th {{ACM SIGPLAN Symposium}}},
  file = {/home/philipp/Zotero/storage/3DJ2YMXB/Ścibior et al. - 2015 - Practical probabilistic programming with monads.pdf},
  isbn = {978-1-4503-3808-0},
  langid = {english}
}

@article{scibior2018functional,
  title = {Functional Programming for Modular {{Bayesian}} Inference},
  author = {Ścibior, Adam and Kammar, Ohad and Ghahramani, Zoubin},
  date = {2018-07-30},
  journaltitle = {Proceedings of the ACM on Programming Languages},
  shortjournal = {Proc. ACM Program. Lang.},
  volume = {2},
  pages = {1--29},
  issn = {2475-1421, 2475-1421},
  doi = {10.1145/3236778},
  url = {https://dl.acm.org/doi/10.1145/3236778},
  urldate = {2020-07-23},
  file = {/home/philipp/Zotero/storage/J3UHJCXN/Ścibior et al. - 2018 - Functional programming for modular Bayesian infere.pdf},
  issue = {ICFP},
  langid = {english}
}

@incollection{singer2018introduction,
  title = {Introduction},
  booktitle = {Single {{Static Assignment Book}}},
  author = {Singer, J.},
  date = {2018},
  url = {http://ssabook.gforge.inria.fr/latest/book-full.pdf},
  urldate = {2020-07-30}
}

@inproceedings{socher2011parsing,
  title = {Parsing {{Natural Scenes}} and {{Natural Language}} with {{Recursive Neural Networks}}},
  booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
  author = {Socher, Richard and Lin, Cliff Chiung-Yu and Ng, Andrew Y. and Manning, Christopher D.},
  date = {2011},
  pages = {129--136},
  publisher = {{Omnipress}},
  location = {{USA}},
  url = {http://dl.acm.org/citation.cfm?id=3104482.3104499},
  file = {/home/philipp/Zotero/storage/ILEZFBC9/Socher et al_2011_Parsing natural scenes and natural language with recursive neural networks.pdf},
  series = {{{ICML}}'11}
}

@book{static,
  title = {Static {{Single Assignment Book}}},
  file = {/home/philipp/Zotero/storage/FJ8W43HY/book-full.pdf}
}

@software{swift2019,
  title = {Swift for {{TensorFlow}}},
  date = {2019-04-30T12:52:45Z},
  origdate = {2018-04-24T19:18:14Z},
  url = {https://github.com/tensorflow/swift},
  urldate = {2019-04-30},
  organization = {{tensorflow}}
}

@online{tapenadedevelopers2019tapenade,
  title = {The {{Tapenade A}}.{{D}}. Engine},
  author = {{Tapenade developers}},
  date = {2019-03-05},
  url = {https://www-sop.inria.fr/tropics/tapenade.html},
  urldate = {2019-10-09},
  file = {/home/philipp/Zotero/storage/5EIBY7CL/tapenade.html}
}

@inproceedings{tokui2015chainer,
  title = {Chainer: A {{Next}}-{{Generation Open Source Framework}} for {{Deep Learning}}},
  booktitle = {Proceedings of Workshop on Machine Learning Systems ({{LearningSys}}) in the Twenty-Ninth Annual Conference on Neural Information Processing Systems ({{NIPS}})},
  author = {Tokui, Seiya and Oono, Kenta and Hido, Shohei and Clayton, Justin},
  date = {2015},
  url = {http://learningsys.org/papers/LearningSys_2015_paper_33.pdf},
  file = {/home/philipp/Zotero/storage/R873X5Q6/Tokui et al_2015_Chainer.pdf}
}

@online{tolpin2020stochastically,
  title = {Stochastically {{Differentiable Probabilistic Programs}}},
  author = {Tolpin, David and Zhou, Yuan and Yang, Hongseok},
  date = {2020-03-05},
  url = {http://arxiv.org/abs/2003.00704},
  urldate = {2020-03-09},
  abstract = {Probabilistic programs with mixed support (both continuous and discrete latent random variables) commonly appear in many probabilistic programming systems (PPSs). However, the existence of the discrete random variables prohibits many basic gradient-based inference engines, which makes the inference procedure on such models particularly challenging. Existing PPSs either require the user to manually marginalize out the discrete variables or to perform a composing inference by running inference separately on discrete and continuous variables. The former is infeasible in most cases whereas the latter has some fundamental shortcomings. We present a novel approach to run inference efficiently and robustly in such programs using stochastic gradient Markov Chain Monte Carlo family of algorithms. We compare our stochastic gradient-based inference algorithm against conventional baselines in several important cases of probabilistic programs with mixed support, and demonstrate that it outperforms existing composing inference baselines and works almost as well as inference in marginalized versions of the programs, but with less programming effort and at a lower computation cost.},
  archivePrefix = {arXiv},
  eprint = {2003.00704},
  eprinttype = {arxiv},
  file = {/home/philipp/Zotero/storage/GXCX8DSF/Tolpin et al_2020_Stochastically Differentiable Probabilistic Programs.pdf;/home/philipp/Zotero/storage/I8MRPTRW/2003.html},
  primaryClass = {cs, stat}
}

@online{vandemeent2018introduction,
  title = {An {{Introduction}} to {{Probabilistic Programming}}},
  author = {van de Meent, Jan-Willem and Paige, Brooks and Yang, Hongseok and Wood, Frank},
  date = {2018-09-27},
  url = {http://arxiv.org/abs/1809.10756},
  urldate = {2019-03-08},
  abstract = {This document is designed to be a first-year graduate-level introduction to probabilistic programming. It not only provides a thorough background for anyone wishing to use a probabilistic programming system, but also introduces the techniques needed to design and build these systems. It is aimed at people who have an undergraduate-level understanding of either or, ideally, both probabilistic machine learning and programming languages. We start with a discussion of model-based reasoning and explain why conditioning as a foundational computation is central to the fields of probabilistic machine learning and artificial intelligence. We then introduce a simple first-order probabilistic programming language (PPL) whose programs define static-computation-graph, finite-variable-cardinality models. In the context of this restricted PPL we introduce fundamental inference algorithms and describe how they can be implemented in the context of models denoted by probabilistic programs. In the second part of this document, we introduce a higher-order probabilistic programming language, with a functionality analogous to that of established programming languages. This affords the opportunity to define models with dynamic computation graphs, at the cost of requiring inference methods that generate samples by repeatedly executing the program. Foundational inference algorithms for this kind of probabilistic programming language are explained in the context of an interface between program executions and an inference controller. This document closes with a chapter on advanced topics which we believe to be, at the time of writing, interesting directions for probabilistic programming research; directions that point towards a tight integration with deep neural network research and the development of systems for next-generation artificial intelligence applications.},
  archivePrefix = {arXiv},
  eprint = {1809.10756},
  eprinttype = {arxiv},
  file = {/home/philipp/Zotero/storage/V48VS2MS/van de Meent et al. - 2018 - An Introduction to Probabilistic Programming.pdf},
  langid = {english},
  options = {useprefix=true},
  primaryClass = {cs, stat}
}

@online{vanmerrienboer2018automatic,
  title = {Automatic Differentiation in {{ML}}: {{Where}} We Are and Where We Should Be Going},
  shorttitle = {Automatic Differentiation in {{ML}}},
  author = {van Merriënboer, Bart and Breuleux, Olivier and Bergeron, Arnaud and Lamblin, Pascal},
  date = {2018-10-26},
  url = {http://arxiv.org/abs/1810.11530},
  urldate = {2019-10-10},
  abstract = {We review the current state of automatic differentiation (AD) for array programming in machine learning (ML), including the different approaches such as operator overloading (OO) and source transformation (ST) used for AD, graph-based intermediate representations for programs, and source languages. Based on these insights, we introduce a new graph-based intermediate representation (IR) which specifically aims to efficiently support fully-general AD for array programming. Unlike existing dataflow programming representations in ML frameworks, our IR naturally supports function calls, higher-order functions and recursion, making ML models easier to implement. The ability to represent closures allows us to perform AD using ST without a tape, making the resulting derivative (adjoint) program amenable to ahead-of-time optimization using tools from functional language compilers, and enabling higher-order derivatives. Lastly, we introduce a proof of concept compiler toolchain called Myia which uses a subset of Python as a front end.},
  archivePrefix = {arXiv},
  eprint = {1810.11530},
  eprinttype = {arxiv},
  file = {/home/philipp/Zotero/storage/CUI3CBEP/van Merriënboer et al_2018_Automatic differentiation in ML.pdf;/home/philipp/Zotero/storage/N6SDBST9/1810.html},
  options = {useprefix=true},
  primaryClass = {cs, stat}
}

@report{vihola2020lectures,
  title = {Lectures on Stochastic Simulation},
  author = {Vihola, Matti},
  date = {2020},
  location = {{University of Jyväskylä}},
  url = {http://users.jyu.fi/~mvihola/stochsim/},
  file = {/home/philipp/Zotero/storage/BY8MZ3JJ/Vihola - 2020 - Lectures on stochastic simulation.pdf},
  langid = {english}
}

@article{vytiniotis2019differentiable,
  title = {The {{Differentiable Curry}}},
  author = {Vytiniotis, Dimitrios and Belov, Dan and Wei, Richard and Plotkin, Gordon and Abadi, Martin},
  date = {2019-09-20},
  url = {https://openreview.net/forum?id=ryxuz9SzDB},
  urldate = {2019-11-05},
  abstract = {We revisit the automatic differentiation (AD) of programs that contain higher-order functions, in a statically typed setting. Our presentation builds on a recent formulation of AD based on...},
  file = {/home/philipp/Zotero/storage/IXD9AE2L/Vytiniotis et al. - 2019 - The Differentiable Curry.pdf;/home/philipp/Zotero/storage/3X2UMQUE/forum.html}
}

@article{wainwright2007graphical,
  title = {Graphical {{Models}}, {{Exponential Families}}, and {{Variational Inference}}},
  author = {Wainwright, Martin J. and Jordan, Michael I.},
  date = {2007},
  journaltitle = {Foundations and Trends in Machine Learning},
  volume = {1},
  pages = {1--305},
  doi = {10.1561/2200000001},
  url = {http://www.nowpublishers.com/article/Details/MAL-001},
  urldate = {2019-10-09},
  file = {/home/philipp/Zotero/storage/YHAJAFFR/Wainwright and Jordan - 2007 - Graphical Models, Exponential Families, and Variat.pdf},
  langid = {english},
  number = {1–2}
}

@online{wang2019demystifying,
  title = {Demystifying {{Differentiable Programming}}: {{Shift}}/{{Reset}} the {{Penultimate Backpropagator}}},
  shorttitle = {Demystifying {{Differentiable Programming}}},
  author = {Wang, Fei and Zheng, Daniel and Decker, James and Wu, Xilun and Essertel, Grégory M. and Rompf, Tiark},
  date = {2019-08-28},
  url = {http://arxiv.org/abs/1803.10228},
  urldate = {2020-02-17},
  abstract = {Deep learning has seen tremendous success over the past decade in computer vision, machine translation, and gameplay. This success rests in crucial ways on gradient-descent optimization and the ability to learn parameters of a neural network by backpropagating observed errors. However, neural network architectures are growing increasingly sophisticated and diverse, which motivates an emerging quest for even more general forms of differentiable programming, where arbitrary parameterized computations can be trained by gradient descent. In this paper, we take a fresh look at automatic differentiation (AD) techniques, and especially aim to demystify the reverse-mode form of AD that generalizes backpropagation in neural networks. We uncover a tight connection between reverse-mode AD and delimited continuations, which permits implementing reverse-mode AD purely via operator overloading and without any auxiliary data structures. We further show how this formulation of AD can be fruitfully combined with multi-stage programming (staging), leading to a highly efficient implementation that combines the performance benefits of deep learning frameworks based on explicit reified computation graphs (e.g., TensorFlow) with the expressiveness of pure library approaches (e.g., PyTorch).},
  archivePrefix = {arXiv},
  eprint = {1803.10228},
  eprinttype = {arxiv},
  file = {/home/philipp/Zotero/storage/AVZWXKKQ/Wang et al_2019_Demystifying Differentiable Programming.pdf;/home/philipp/Zotero/storage/V98EANLS/1803.html},
  primaryClass = {cs, stat}
}

@inproceedings{wigren2019parameter,
  title = {Parameter Elimination in Particle {{Gibbs}} Sampling},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wigren, Anna and Risuleo, Riccardo Sven and Murray, Lawrence and Lindsten, Fredrik},
  date = {2019},
  abstract = {Bayesian inference in state-space models is challenging due to high-dimensional state trajectories. A viable approach is particle Markov chain Monte Carlo (PM- CMC), combining MCMC and sequential Monte Carlo to form “exact approx- imations” to otherwise-intractable MCMC methods. The performance of the approximation is limited to that of the exact method. We focus on particle Gibbs (PG) and particle Gibbs with ancestor sampling (PGAS), improving their per- formance beyond that of the ideal Gibbs sampler (which they approximate) by marginalizing out one or more parameters. This is possible when the parameter(s) has a conjugate prior relationship with the complete data likelihood. Marginaliza- tion yields a non-Markov model for inference, but we show that, in contrast to the general case, the methods still scale linearly in time. While marginalization can be cumbersome to implement, recent advances in probabilistic programming have enabled its automation. We demonstrate how the marginalized methods are viable as efficient inference backends in probabilistic programming, and demonstrate with examples in ecology and epidemiology.},
  file = {/home/philipp/Zotero/storage/5JWKEDM8/4792__Submission 4792.pdf}
}

@article{wingatelightweight,
  title = {Lightweight {{Implementations}} of {{Probabilistic Programming Languages Via Transformational Compilation}}},
  author = {Wingate, David and Stuhlmüller, Andreas and Goodman, Noah D},
  pages = {9},
  abstract = {We describe a general method of transforming arbitrary programming languages into probabilistic programming languages with straightforward MCMC inference engines. Random choices in the program are “named” with information about their position in an execution trace; these names are used in conjunction with a database holding values of random variables to implement MCMC inference in the space of execution traces. We encode naming information using lightweight source-to-source compilers. Our method enables us to reuse existing infrastructure (compilers, profilers, etc.) with minimal additional code, implying fast models with low development overhead. We illustrate the technique on two languages, one functional and one imperative: Bher, a compiled version of the Church language which eliminates interpretive overhead of the original MIT-Church implementation, and Stochastic Matlab, a new open-source language.},
  file = {/home/philipp/Zotero/storage/SXWS5YEA/Wingate et al. - Lightweight Implementations of Probabilistic Progr.pdf},
  langid = {english}
}

@article{winn2005variational,
  title = {Variational {{Message Passing}}},
  author = {Winn, John and Bishop, Christopher M.},
  date = {2005},
  journaltitle = {Journal of Machine Learning Research},
  volume = {6},
  pages = {661--694},
  abstract = {Bayesian inference is now widely established as one of the principal foundations for machine learning. In practice, exact inference is rarely possible, and so a variety of approximation techniques have been developed, one of the most widely used being a deterministic framework called variational inference. In this paper we introduce Variational Message Passing (VMP), a general purpose algorithm for applying variational inference to Bayesian Networks. Like belief propagation, VMP proceeds by sending messages between nodes in the network and updating posterior beliefs using local operations at each node. Each such update increases a lower bound on the log evidence (unless already at a local maximum). In contrast to belief propagation, VMP can be applied to a very general class of conjugateexponential models because it uses a factorised variational approximation. Furthermore, by introducing additional variational parameters, VMP can be applied to models containing non-conjugate distributions. The VMP framework also allows the lower bound to be evaluated, and this can be used both for model comparison and for detection of convergence. Variational Message Passing has been implemented in the form of a general purpose inference engine called VIBES (‘Variational Inference for BayEsian networkS’) which allows models to be specified graphically and then solved variationally without recourse to coding.},
  file = {/home/philipp/Zotero/storage/VCT85F72/Winn and Bishop - Variational Message Passing.pdf},
  langid = {english}
}

@online{winn2019modelbased,
  title = {Model-{{Based Machine Learning}} ({{Early Access}})},
  author = {Winn, John and Bishop, Christopher M. and Diethe, Thomas and Guiver, John and Zaykov, Yordan},
  date = {2019},
  url = {http://mbmlbook.com/index.html},
  urldate = {2019-09-03},
  file = {/home/philipp/Zotero/storage/XJFYHQ3H/Winn - Model-Based Machine Learning.pdf;/home/philipp/Zotero/storage/NUVN5MX6/index.html}
}

@online{wood2015new,
  title = {A {{New Approach}} to {{Probabilistic Programming Inference}}},
  author = {Wood, Frank and van de Meent, Jan Willem and Mansinghka, Vikash},
  date = {2015-07-03},
  url = {http://arxiv.org/abs/1507.00996},
  urldate = {2019-10-15},
  abstract = {We introduce and demonstrate a new approach to inference in expressive probabilistic programming languages based on particle Markov chain Monte Carlo. Our approach is simple to implement and easy to parallelize. It applies to Turing-complete probabilistic programming languages and supports accurate inference in models that make use of complex control flow, including stochastic recursion. It also includes primitives from Bayesian nonparametric statistics. Our experiments show that this approach can be more efficient than previously introduced single-site Metropolis-Hastings methods.},
  archivePrefix = {arXiv},
  eprint = {1507.00996},
  eprinttype = {arxiv},
  file = {/home/philipp/Zotero/storage/Z7BBE82D/Wood et al_2015_A New Approach to Probabilistic Programming Inference.pdf;/home/philipp/Zotero/storage/QRMHH3CM/1507.html},
  options = {useprefix=true},
  primaryClass = {cs, stat}
}

@thesis{wright2019algebraic,
  title = {An Algebraic Perspective on Computing with Data},
  author = {Wright, William},
  date = {2019},
  institution = {{Pensylvania State University}},
  file = {/home/philipp/Zotero/storage/PJMCI3JT/Wright - AN ALGEBRAIC PERSPECTIVE ON COMPUTING WITH DATA.pdf},
  langid = {english},
  type = {PhD Thesis}
}

@article{yugibbs,
  title = {Gibbs {{Sampling Methods}} for {{Dirichlet Process Mixture Model}}: {{Technical Details}}},
  author = {Yu, Xiaodong},
  pages = {18},
  file = {/home/philipp/Zotero/storage/HYAVTPKH/Yu - Gibbs Sampling Methods for Dirichlet Process Mixtu.pdf},
  langid = {english}
}

@inproceedings{yuret2016knet,
  title = {Knet: Beginning Deep Learning with 100 Lines of {{Julia}}},
  booktitle = {Machine Learning Systems Workshop at {{NIPS}}},
  author = {Yuret, Deniz},
  date = {2016},
  file = {/home/philipp/Zotero/storage/MJD7K8BR/Yuret_2016_Knet.pdf}
}

@article{zappanardelli2018julia,
  title = {Julia Subtyping: A Rational Reconstruction},
  shorttitle = {Julia Subtyping},
  author = {Zappa Nardelli, Francesco and Belyakova, Julia and Pelenitsyn, Artem and Chung, Benjamin and Bezanson, Jeff and Vitek, Jan},
  date = {2018-10-24},
  journaltitle = {Proceedings of the ACM on Programming Languages},
  shortjournal = {Proc. ACM Program. Lang.},
  volume = {2},
  pages = {113:1--113:27},
  doi = {10.1145/3276483},
  url = {https://doi.org/10.1145/3276483},
  urldate = {2020-07-07},
  abstract = {Programming languages that support multiple dispatch rely on an expressive notion of subtyping to specify method applicability. In these languages, type annotations on method declarations are used to select, out of a potentially large set of methods, the one that is most appropriate for a particular tuple of arguments. Julia is a language for scientific computing built around multiple dispatch and an expressive subtyping relation. This paper provides the first formal definition of Julia's subtype relation and motivates its design. We validate our specification empirically with an implementation of our definition that we compare against the existing Julia implementation on a collection of real-world programs. Our subtype implementation differs on 122 subtype tests out of 6,014,476. The first 120 differences are due to a bug in Julia that was fixed once reported; the remaining 2 are under discussion.},
  file = {/home/philipp/Zotero/storage/SXJY8XZ4/Zappa Nardelli et al_2018_Julia subtyping.pdf},
  issue = {OOPSLA}
}


