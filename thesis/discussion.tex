\chapter{Discussion}
\label{cha:discussion}

The history of this project forms a large arc from a general problem in \turingjl{}, over a
digression into compiler technology, back the the implementation of a proof of concept in the form
of a very specific inference method.  As we have seen, two separate pieces of software have emerged
from it: \irtrackerjl{} and \autogibbsjl{}.  The underlying issue~-- that \turingjl{} lacks a
structural representation of models~-- is not at all resolved by them.

The real difficulty is that dynamic models cannot be satisfactorily handled through static
snapshots. \todo{compare to autograd, venture, church}

(\juliapackage{Cassette.jl} is a package very similar to \juliapackage{IRTools.jl})
\begin{quote}
  Using Cassette on code you wrote is a bit like shooting youself with a experimental mind control
  weapon, to force your hands to move like you knew how to fly a helicopter. Even if it works, you
  still had to learn to fly the helicopter in order to program the mind-control weapon to force
  yourself to act like you knew how to fly a helicopter.\footnote{Lyndon White, private
    communication on \protect\url{https://julialang.slack.com}.}
\end{quote}




\section{Future Work}
\label{sec:future-work}

Many of the following ideas have already been informally described by me
online\footnote{\protect\url{https://github.com/phipsgabler/probability-ir}}.

Currently, Turing models are very primitive in this respect: a data structure called \jlinl{VarInfo}
contains a map from variable names to values, the accumulated log-likelihood, and some other
metadata. During this project, I noticed that retrofitting structure onto this is not ideal, and for
proper analysis, it would be nice to begin with a better representation from the start. The two main
difficulties were matching of variable names (e.g., subsuming \jlinl{x[1:10]} under
\jlinl{x[1:3][2]}), and getting rid of array mutations that shadow actual data dependencies (e.g.,
when one has an array \jlinl{x}, samples \jlinl{x[1]}, writes it to \jlinl{x} with
\jlinl{setindex!}, and then uses \jlinl{getindex(x, i)} somewhere downstream).  A more versatile
dictionary structure for variable name keys could improve this situation, but wouldn't
satisfactorily solve all of the issues.

From these difficulties that became appearent during the implementation of the Gibbs conditional
extraction, together with the knowledge about \dppljl{}'s internals, I developed the following
understanding of what an ideal representation of probabilistic models for the purpuse of analysis
would be for me. Probably the answer to any confusion I have caused is this: I come from a
metaprogramming/analysis perspective, with interest in programming language design. I wanted
variable names and dependencies to behave nicely, and primarily a closed, elegant language. Many PPL
people probably come from an inference perspective, putting the language design problem second to
that. “I want to write all the models” vs. “I want to do all the inference”. But I also try to close
a bridge to the mostly theoretical, FP-based approaches of just formalizing probabilistic programs.

he separation between the “specification abstraction” and “evaluator abstraction”, across multiple
implementations, would be something that I haven’t really seen before – everyone’s always proposing
a complete system, right? The closest thing would be the formalization attempts of probabilistic
models with monads and types, but that is more semantic than syntactic. We do have abstracted “pure
inference” libraries, that really only take a function and do their work, but they aren’t really a
PPL. There’s some “linguae frankae” like the Stan/JAGS syntax, but it’s also somewhat restricted and
not independently maintained – the ones coming later just chose to take over the same kind of input
format for their own implemenation. What I’m thinking of is a model specification form in its own
right, that has more general analysis capabilites, and can then be transformed town to whatever the
evaluator requires – into CPS, as a monad, as a DAG, as a factor graph, you name it.

The advantage of this kind of approach, besides solving "compiler domain" problems like the ones I
mentioned above, is that it provides a different kind of common abstraction for PPLs. Recently,
people have started writing "bridge code" to allow PPL interaction: there is invented a common
interface that multiple PPL systems can be fit under, and then models in each can be used from
within the other at evaluation. This approach is due to the lack of division of a system into an
evalator and a model specification part (DynamicPPL is supposed to be a factored out model system,
but currently way too specialized to Turing): they always go totheger. I believe that starting from
a common model specification language is in many cases more feasible and general than defining a
common interface for evaluators: the latter tends to assume much more about the internals, while
model syntax is essentially fixed: the notation of random variables used in model specification by
hand, extended through general Julia syntax.


% names/traces and tildes must not be separated semantically from the host language!

% cf. oryx: https://www.tensorflow.org/probability/oryx

% cf. soss: already similar approach with regards to symbolic ~> compilation, but expression-based
% with combinators, not statement/IR-based.



%%% Local Variables: 
%%% TeX-master: "main"
%%% End: