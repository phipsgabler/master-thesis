\chapter{Discussion}
\label{cha:discussion}

The history of this project forms a large arc from a general problem in \turingjl{}, over a
digression into compiler technology and automatic differentiation, back the the implementation of a
proof of concept in the form of a very specific inference method.  As we have seen, two separate
pieces of software have emerged from it: \irtrackerjl{} and \autogibbsjl{}.  The underlying issue~--
that \turingjl{} lacks a structural representation of models~-- is not at all resolved by them,
unfortunately.  This makes \autogibbsjl{} not completly satisfactory, since the recursion and branch
tracking features of \irtrackerjl{} cannot be applied in a useful way.

The real difficulty is that dynamic models cannot be satisfactorily handled through static
\enquote{snapshots} in the form of traces.  Systems trying to achieve this either become restrictive
in their expressibility, or limited in their applicability.\todo{compare to autograd, venture,
  church} Furthermore, during implementation, the two main practical difficulties were matching of
variable names (e.g., subsuming \jlinl{x[1:10]} under \jlinl{x[1:3][2]}), and the correct handling
of mutations that shadow actual data dependencies (e.g., when one has an array \jlinl{x}, samples
\jlinl{x[1]}, writes it to \jlinl{x} with \jlinl{setindex!}, and then uses \jlinl{getindex(x, i)}
somewhere downstream).  These cannot be really solved using external tooling, in the way that has
been demonstrated.  A more versatile dictionary structure for variable name keys could improve the
situation for variable names, but wouldn't solve all of the underlying issues.

There is also a fragility problem: Julia IR, while being publicly documented, is a rather internal
feature of the language, and may change between compiler versions.  The \juliapackage{IRTools.jl}
package provides a good mid-layer mitigating this, but still there's lot of reasons why a custom IR
would be nicer.  From the other side, also the structure of \dppljl{}'s model representations is not
stable, and actually an implementation detail that should not be relied on from the outside~--
especially not by an important feature such as dependency extration.  In a certain sense, the whole
approach is misguided: why rely on external tracking for a framework that is really under ones
control, using such heavy machinery as IR transformations?  This is illuminated by following very
telling quote about similar tendencies (\juliapackage{Cassette.jl} is a package very similar to
\juliapackage{IRTools.jl}):
\begin{quote}
  Using Cassette on code you wrote is a bit like shooting youself with a experimental mind control
  weapon, to force your hands to move like you knew how to fly a helicopter.  Even if it works, you
  still had to learn to fly the helicopter in order to program the mind-control weapon to force
  yourself to act like you knew how to fly a helicopter.\footnote{Lyndon White (2020), private
    communication on \protect\url{https://julialang.slack.com}.}
\end{quote}

In conclusion, the dynamic graph tracking system has turned out to be an interesting idea with some
application possibilities.  Unfortunatley, the analysis of dynamic probabilistic models could not be
shown to be among them.  In the course of development, though, various techniques have been tried or
ruled out, challenges identified, and other alternatives explored.  This knowledge has lead me to a
better understanding of the domain and some more advanced ideas for the future, some of which are
layed out in the following section.

\section{Future Work}
\label{sec:future-work}

While \irtrackerjl{} is a quite satisfying and complete system, the approach that \autogibbsjl{}
takes provides only an ad-hoc solution to one shortcoming of \turingjl{}: the lack a structural
model represenation that is open to analysis and transformations.  This has lead me to consider
alternatives, approaching the representation problem for probabilistic programming languages on a
more fundamental level.  Most of the following ideas I have already informally described
online\footnote{\protect\url{https://github.com/phipsgabler/probability-ir}}.

Let us review the important features of a universal, flexible PPL as mentioned in
section~\ref{sec:prob-prog}: its DSL should allow general recursion and nesting, support for all
language constructs and custom types and extensions, and be able to delegate to other samplers or
complex programs.  And the internal representation should be such that multiple forms of analysis,
optimization, non-standard execution, and transformation can be performed.

Currently, \turingjl{} is very primitive in this respect: one data structure (\jlinl{VarInfo})
contains a map from variable names to values, the accumulated log-likelihood, and some other
(stateful!) sampling metadata. \autogibbsjl{}' approach consists of retrofitting some more structure
onto this representation~-- this is not ideal, and for proper analysis, it would be desireable to
begin with a better representation from the start, at least for the reasons mentioned above.

From difficulties described above, that became appearent during the implementation of the Gibbs
conditional extraction, together with the knowledge about \dppljl{}'s internals, I developed an
understanding of what an more advanced representation of probabilistic models, with a focus on
transformation and analysis, could be; coming mostly from a metaprogramming and static analysis
perspective, emplying ideas from functional programming language design.  My abstract, idealized
wish was to for variable names and dependencies to behave nicely as abstract data structures, and to
operate primarily in a closed, elegant, high-level language.  Many successful approaches to PPL
design probably come from the perspective of efficient and general inference algorithm, putting the
language design problem second to such a desire.  But it should be possible to approach the field
from a linguistic perspective as well.  A further goal of mine is the wish to close the gap between
practical inference systems and the mostly theoretical, functional-programming-based approaches of
just formalizing probabilistic programs.

\newthought{Universal PPLs} have as their goal to let the user write down every model the language
allows, and still be able to do inference on it.  Of course, at the boundary of the space of
reasonable programs, tradeoffs need to be made to still be able to do this.  It seem advantageous to
split up this conjuction: by creating a format in which one can denote every possible model of a
very large class, without a priori having to deal with the restrictions of inference.  Then for each
model, suitable transformations and analyses can be performd in a uniform representation, and
specialized backends be chosen from a wide range, which understand precisely the fragment of the
model language used.

What I propose is a \enquote{probabilistic intermediate representation}, that in a way turns around
how things are constructed right now in most of the approaches.  Instead of starting from a
\enquote{sampling function}, which is evaluated to extract graphs or other symbolic representations
from it, one should start from a model representation that already is general, yet richly
structured, and derive evaluators from it.  On the other hand, in contrast to PPLs that are built on
top of a DLS representation, such an IR should be backend-agnostic, and instead allow all kinds of
models to be specified in a uniform syntax, without being constraint by the demands of a specific
sampling algorithm or inference technique.  And if furthermore such a representation takes the form
of SSA-form IR, it doesn't matter whether the model is complicated, nonparametric, dynamic~-- the
object that is worked with is always a fixed, full program in a specified syntax, with an intuitive
denotation

This separation between the a \enquote{specification abstraction} in form of a general
representation and \enquote{evaluator abstractions} provided by interfaces to multiple sampler
implementations seems novel.  The closest correspondence would be the formalization attempts of
probabilistic models through monads and type systems; but that is more semantic than syntactic.
There are also very general \enquote{pure inference} libraries, examples of general evaluators,
which take a function and do their work; but they are not really a PPL.  There are some
domain-specific \enquote{linguae frankae} like the syntax of Stan and JAGS, but they are, too,
somewhat restricted, and not independently maintained~-- the ones coming later just chose to take
over the same kind of input format for their own implemenation.  All these approaches should rather
be abstracted out into a model specification form in its own right, that has more general analysis
capabilites, and can then be transformed town to whatever the evaluator requires.

The advantage of this kind of approach, besides solving "compiler domain" problems like the ones I
mentioned above, is that it provides a different kind of common abstraction for PPLs.  Recently,
people have started writing \enquote{bridge code} to allow PPL interaction: there is invented a
common interface that multiple PPL systems can be fit under, and then models in each can be used
from within the other at evaluation.  This is necessary due to the lack of division of a system into
an evalator and a model specification part: they always go together.  I believe that starting from a
common model specification language is in many cases more feasible and general than defining a
common interface for evaluators.  The latter tends to assume much more about the internals, while
model syntax is essentially fixed: the notation of random variables used in model specification by
hand, extended through general Julia syntax.

As far as I see it, a sort of least upper bound of all the PPL modelling approaches consists of
\begin{itemize}
  \tightlist
\item General Julia code: this can most conveniently be represented like normal Julia IR with SSA
statements, branches, and blocks.
\item \enquote{Sampling statements}: tildes, or assumptions and observations in \turingjl{}
  parlance, which relate names or values to distributions in an immutable way.
\item Variable names: which may be \enquote{complex}, containing indexing, fields, link functions,
  etc., which can be identified and analyzed in a structured way.
\end{itemize}
So my idea was to just combine all that into an extended IR-like syntax.  This amounts to writing
out a directed graphical model with deterministic and stochastic nodes, named random variables, but
generalized to programs~-- i.e., allowing dynamic structure due to branching and recursion.  A model
in this kind of format then defines an abstract and uninterpreted parametrized joint density
function over its trace space (as given through the unified name set of all possible runs),
factorized into primitive statements and blocks.


% TODO: compare to JAX: purely functional intermediate form + transformations, trace-based, so no
% control flow handling

% names/traces and tildes must not be separated semantically from the host language!

% cf. oryx: https://www.tensorflow.org/probability/oryx

% cf. soss: already similar approach with regards to symbolic ~> compilation, but expression-based
% with combinators, not statement/IR-based.



%%% Local Variables: 
%%% TeX-master: "main"
%%% End: